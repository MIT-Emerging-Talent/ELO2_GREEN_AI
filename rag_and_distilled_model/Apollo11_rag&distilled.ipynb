{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "269fd429",
   "metadata": {},
   "source": [
    "# 1. Install required packages\n",
    "\n",
    "We install all the dependencies needed for building a\n",
    "Retrieval-Augmented Generation (RAG) pipeline.\n",
    "These include LangChain components, Hugging Face models,\n",
    "ChromaDB for vector storage, and PyTorch for GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52616a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install somepackage -qq langchain langchain-community langchain-core langchain-text-splitters langchain-huggingface sentence-transformers chromadb transformers torch accelerate unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed814cfe",
   "metadata": {},
   "source": [
    "# 2. Import libraries and set configuration\n",
    "\n",
    "Here we import the necessary modules and define paths, constants,\n",
    "and model settings.\n",
    "We also suppress warnings to keep the notebook output clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e12116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PROMPTS_FILE = \"data/test_data.json\"\n",
    "PERSIST_DIR = \"data/chroma_db\"\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "CHUNK_SIZE = 400\n",
    "CHUNK_OVERLAP = 50\n",
    "TOP_K_RESULTS = 5\n",
    "RELEVANCE_THRESHOLD = 0.3\n",
    "LLM_MODEL = \"MBZUAI/LaMini-Flan-T5-248M\"\n",
    "MAX_NEW_TOKENS = 100\n",
    "LLM_TEMPERATURE = 0.2\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"Answer the question about Apollo 11 based on the context below. If you cannot answer based on the context, say \"I don't have enough information to answer that.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6e30b4",
   "metadata": {},
   "source": [
    "# 3. Initialize embedding model and text splitter\n",
    "\n",
    "The embedding model converts text into numeric vectors, while the text\n",
    "splitter breaks long documents into manageable chunks for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dc94c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d3954a",
   "metadata": {},
   "source": [
    "# 4. Load the local language model\n",
    "\n",
    "We initialize a small, local LLM (LaMini-Flan-T5) that can run on CPU or GPU.\n",
    "This model will later generate answers based on retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c57183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_local_llm():\n",
    "    device = 0 if USE_GPU else -1\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        LLM_MODEL,\n",
    "        torch_dtype=torch.float16 if USE_GPU else torch.float32,\n",
    "        device_map=\"auto\" if USE_GPU else None,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    pipe = pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        temperature=LLM_TEMPERATURE,\n",
    "        repetition_penalty=1.2,\n",
    "        do_sample=False,\n",
    "        top_p=0.95,\n",
    "        device=device,\n",
    "    )\n",
    "    return HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "\n",
    "llm = initialize_local_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59668d86",
   "metadata": {},
   "source": [
    "# Load documents from JSON\n",
    "\n",
    "We read the context and metadata directly from a JSON file.\n",
    "We also clean metadata and split text into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f71f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_from_json(json_path=PROMPTS_FILE):\n",
    "    data_path = Path(json_path)\n",
    "    if not data_path.exists():\n",
    "        print(f\"JSON file not found at: {json_path}\")\n",
    "        return []\n",
    "\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    source_text = data.get(\"source_text\", \"\")\n",
    "    metadata = data.get(\"metadata\", {})\n",
    "\n",
    "    if not source_text.strip():\n",
    "        print(\"No source text found in JSON.\")\n",
    "        return []\n",
    "\n",
    "    for k, v in metadata.items():\n",
    "        if isinstance(v, (list, dict)):\n",
    "            metadata[k] = str(v)\n",
    "\n",
    "    split_docs = splitter.create_documents([source_text])\n",
    "\n",
    "    for doc in split_docs:\n",
    "        doc.metadata = metadata.copy()\n",
    "        doc.metadata[\"topic\"] = \"Apollo 11\"\n",
    "        doc.metadata[\"section\"] = \", \".join(metadata.get(\"sections\", [\"General\"]))\n",
    "\n",
    "    print(f\"Loaded and split {len(split_docs)} chunks from JSON.\")\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854b765d",
   "metadata": {},
   "source": [
    "# 6. Build Chroma vector store\n",
    "\n",
    "Here we embed the document chunks and save them into a local vector database (Chroma).\n",
    "This enables fast similarity-based retrieval of relevant context later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3335a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chroma_store(docs, persist_dir=PERSIST_DIR):\n",
    "    db = Chroma.from_documents(\n",
    "        documents=docs, embedding=embedder, persist_directory=persist_dir\n",
    "    )\n",
    "    db.persist()\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790b3261",
   "metadata": {},
   "source": [
    "# 7. Calling the Load Document Function \n",
    "\n",
    "This cell loads the source document (text and metadata) from the JSON file, and\n",
    "splits it into smaller chunks for embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c349a0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_documents_from_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a12c5f",
   "metadata": {},
   "source": [
    "# 8. Calling the Build Chroma Function\n",
    "\n",
    "This cell builds a Chroma vector database\n",
    "that stores those embeddings for efficient similarity search.\n",
    "\n",
    "Once the database is built, it’s saved to disk,\n",
    "so you only need to run this cell once, unless you change or add new data.\n",
    "\n",
    "Running it again will overwrite the existing database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a744010",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = build_chroma_store(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670d000f",
   "metadata": {},
   "source": [
    "# 9. Define query and response generation\n",
    "\n",
    "These functions retrieve the most relevant text chunks and use the\n",
    "LLM to answer a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab0bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_database(query_text, k=TOP_K_RESULTS, threshold=RELEVANCE_THRESHOLD):\n",
    "    results = db.similarity_search_with_relevance_scores(query_text, k=k)\n",
    "\n",
    "    if len(results) == 0 or results[0][1] < threshold:\n",
    "        return []\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def generate_rag_response(\n",
    "    query_text, k=TOP_K_RESULTS, threshold=RELEVANCE_THRESHOLD, verbose=False\n",
    "):\n",
    "    results = db.similarity_search_with_relevance_scores(query_text, k=k)\n",
    "\n",
    "    if len(results) == 0 or results[0][1] < threshold:\n",
    "        return {\n",
    "            \"answer\": \"No relevant information found.\",\n",
    "            \"sources\": [],\n",
    "            \"context\": \"\",\n",
    "            \"prompt\": \"\",\n",
    "        }\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = PromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "\n",
    "    if llm is None:\n",
    "        return {\n",
    "            \"answer\": \"LLM not initialized.\",\n",
    "            \"sources\": [],\n",
    "            \"context\": context_text,\n",
    "            \"prompt\": prompt,\n",
    "        }\n",
    "\n",
    "    response_text = llm.invoke(prompt)\n",
    "    sources = [doc.metadata.get(\"source\", \"Unknown\") for doc, _score in results]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nQuery: {query_text}\")\n",
    "        print(f\"\\nAnswer: {response_text}\")\n",
    "        print(f\"\\nSources: {', '.join([Path(s).name for s in sources])}\")\n",
    "\n",
    "    return {\n",
    "        \"answer\": response_text,\n",
    "        \"sources\": sources,\n",
    "        \"context\": context_text,\n",
    "        \"prompt\": prompt,\n",
    "        \"scores\": [score for _, score in results],\n",
    "    }\n",
    "\n",
    "\n",
    "def ask(query_text):\n",
    "    result = generate_rag_response(query_text, verbose=True)\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eff73ec",
   "metadata": {},
   "source": [
    "# 10. Load evaluation prompts\n",
    "\n",
    "We load a list of test questions from a JSON file.\n",
    "Each question is labeled with a category (e.g., summarization, reasoning, or RAG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4377d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PROMPTS_FILE, \"r\") as f:\n",
    "    prompts_data = json.load(f)\n",
    "\n",
    "prompts = prompts_data[\"prompts\"]\n",
    "print(f\"Loaded {len(prompts)} evaluation prompts\")\n",
    "print(\"\\nCategories:\")\n",
    "for category in [\"summarization\", \"reasoning\", \"rag\"]:\n",
    "    count = len([p for p in prompts if p[\"category\"] == category])\n",
    "    print(f\"  - {category.title()}: {count} prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1001c44c",
   "metadata": {},
   "source": [
    "# 11. Run automated evaluation\n",
    "\n",
    "For each question, we generate an answer using the RAG system and print\n",
    "both the model’s response and the expected answer (if provided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef553e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for p in prompts:\n",
    "    question = p[\"prompt\"]\n",
    "    expected = p.get(\"expected_answer\", None)\n",
    "    print(f\"\\nTesting Prompt {p['id']}: {question}\")\n",
    "\n",
    "    result = generate_rag_response(question, verbose=False)\n",
    "    answer = result[\"answer\"]\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"id\": p[\"id\"],\n",
    "            \"category\": p[\"category\"],\n",
    "            \"difficulty\": p[\"difficulty\"],\n",
    "            \"prompt\": question,\n",
    "            \"answer\": answer,\n",
    "            \"expected\": expected,\n",
    "            \"context_used\": len(result[\"context\"]),\n",
    "            \"top_sources\": result[\"sources\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\" Model Answer: {answer}\")\n",
    "    if expected:\n",
    "        print(f\" Expected: {expected}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
