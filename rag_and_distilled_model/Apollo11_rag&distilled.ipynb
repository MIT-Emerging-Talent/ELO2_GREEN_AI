{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "269fd429",
   "metadata": {},
   "source": [
    "# 1. Install required packages\n",
    "\n",
    "We install all the dependencies needed for building a\n",
    "Retrieval-Augmented Generation (RAG) pipeline.\n",
    "These include LangChain components, Hugging Face models,\n",
    "ChromaDB for vector storage, and PyTorch for GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b52616a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install somepackage -qq langchain langchain-community langchain-core langchain-text-splitters langchain-huggingface sentence-transformers chromadb transformers torch accelerate unstructured codecarbon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed814cfe",
   "metadata": {},
   "source": [
    "# 2. Imports and Configuration\n",
    "Imports necessary libraries, sets constants for models, embeddings, chunking, and loads prompt templates from a JSON file for various NLP tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e12116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch\n",
    "import warnings\n",
    "from codecarbon import OfflineEmissionsTracker\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PROMPTS_FILE = \"data/test_data.json\"\n",
    "PERSIST_DIR = \"data/chroma_db\"\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "CHUNK_SIZE = 400\n",
    "CHUNK_OVERLAP = 50\n",
    "TOP_K_RESULTS = 5\n",
    "RELEVANCE_THRESHOLD = 0.3\n",
    "LLM_MODEL = \"MBZUAI/LaMini-Flan-T5-248M\"\n",
    "MAX_NEW_TOKENS = 200\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "COUNTRY_ISO_CODE = \"EGY\"\n",
    "ENABLE_RECURSIVE_EDITING = True\n",
    "MAX_EDIT_ITERATIONS = 2\n",
    "\n",
    "with open(PROMPTS_FILE, 'r') as f:\n",
    "    config_data = json.load(f)\n",
    "    MASTER_INSTRUCTION = config_data['metadata']['master_instruction']\n",
    "    TASK_INSTRUCTIONS = config_data['metadata']['task_instructions']\n",
    "\n",
    "def build_prompt_template(task_type):\n",
    "    task_instruction = TASK_INSTRUCTIONS[task_type]\n",
    "    return f\"\"\"{MASTER_INSTRUCTION}\n",
    "\n",
    "{task_instruction}\n",
    "\n",
    "Context:\n",
    "{{context}}\n",
    "\n",
    "Question: {{question}}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "PROMPT_TEMPLATES = {\n",
    "    'summarization': build_prompt_template('summarization'),\n",
    "    'reasoning': build_prompt_template('reasoning'),\n",
    "    'rag': build_prompt_template('rag'),\n",
    "    'paraphrasing': build_prompt_template('paraphrasing'),\n",
    "    'creative_generation': build_prompt_template('creative_generation')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6e30b4",
   "metadata": {},
   "source": [
    "# 3. Initialize embedding model and text splitter\n",
    "\n",
    "The embedding model converts text into numeric vectors, while the text\n",
    "splitter breaks long documents into manageable chunks for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50dc94c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d3954a",
   "metadata": {},
   "source": [
    "# 4. Load the local language model\n",
    "\n",
    "Initializes the HuggingFace Seq2Seq model and tokenizer, wraps it in a pipeline for text generation, and tracks energy usage with `OfflineEmissionsTracker`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c57183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:55:03] offline tracker init\n",
      "[codecarbon WARNING @ 18:55:03] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 18:55:03] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 18:55:03] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 18:55:07] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 18:55:07] CPU Model on constant consumption mode: Intel(R) Core(TM) i5-8350U CPU @ 1.70GHz\n",
      "[codecarbon WARNING @ 18:55:07] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 18:55:07] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 18:55:07] No GPU found.\n",
      "[codecarbon INFO @ 18:55:07] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: global constant\n",
      "                GPU Tracking Method: Unspecified\n",
      "            \n",
      "[codecarbon INFO @ 18:55:07] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 18:55:07]   Platform system: Windows-11-10.0.26200-SP0\n",
      "[codecarbon INFO @ 18:55:07]   Python version: 3.12.10\n",
      "[codecarbon INFO @ 18:55:07]   CodeCarbon version: 3.0.8\n",
      "[codecarbon INFO @ 18:55:07]   Available RAM : 15.843 GB\n",
      "[codecarbon INFO @ 18:55:07]   CPU count: 8 thread(s) in 8 physical CPU(s)\n",
      "[codecarbon INFO @ 18:55:07]   CPU model: Intel(R) Core(TM) i5-8350U CPU @ 1.70GHz\n",
      "[codecarbon INFO @ 18:55:07]   GPU count: None\n",
      "[codecarbon INFO @ 18:55:07]   GPU model: None\n",
      "[codecarbon INFO @ 18:55:07] Emissions data (if any) will be saved to file c:\\Users\\YNA\\ELO2_GREEN_AI\\rag_and_distilled_model\\emissions.csv\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Device set to use cpu\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "[codecarbon INFO @ 18:55:11] Energy consumed for RAM : 0.000012 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 18:55:11] Delta energy consumed for CPU with constant : 0.000070 kWh, power : 60.0 W\n",
      "[codecarbon INFO @ 18:55:11] Energy consumed for All CPU : 0.000070 kWh\n",
      "[codecarbon INFO @ 18:55:11] 0.000082 kWh of electricity and 0.000000 L of water were used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loading emissions: 0.000047 kg CO2\n"
     ]
    }
   ],
   "source": [
    "tracker_loading = OfflineEmissionsTracker(\n",
    "    country_iso_code=COUNTRY_ISO_CODE, project_name=\"model_loading\", log_level=\"error\"\n",
    ")\n",
    "tracker_loading.start()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(LLM_MODEL, low_cpu_mem_usage=True)\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.3,\n",
    "    device=0 if USE_GPU else -1,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "emissions_loading = tracker_loading.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59668d86",
   "metadata": {},
   "source": [
    "# 5. Load documents from JSON\n",
    "\n",
    "We read the context and metadata directly from a JSON file.\n",
    "We also clean metadata and split text into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f71f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_from_json(json_path=PROMPTS_FILE):\n",
    "    data_path = Path(json_path)\n",
    "    if not data_path.exists():\n",
    "        print(f\"JSON file not found at: {json_path}\")\n",
    "        return []\n",
    "\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    source_text = data.get(\"source_text\", \"\")\n",
    "    metadata = data.get(\"metadata\", {})\n",
    "\n",
    "    if not source_text.strip():\n",
    "        print(\"No source text found in JSON.\")\n",
    "        return []\n",
    "\n",
    "    for k, v in metadata.items():\n",
    "        if isinstance(v, (list, dict)):\n",
    "            metadata[k] = str(v)\n",
    "\n",
    "    split_docs = splitter.create_documents([source_text])\n",
    "\n",
    "    for doc in split_docs:\n",
    "        doc.metadata = metadata.copy()\n",
    "        doc.metadata[\"topic\"] = \"Apollo 11\"\n",
    "        doc.metadata[\"section\"] = \", \".join(metadata.get(\"sections\", [\"General\"]))\n",
    "\n",
    "    print(f\"Loaded and split {len(split_docs)} chunks from JSON.\")\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854b765d",
   "metadata": {},
   "source": [
    "# 6. Build Chroma vector store\n",
    "\n",
    "Here we embed the document chunks and save them into a local vector database (Chroma).\n",
    "This enables fast similarity-based retrieval of relevant context later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3335a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chroma_store(docs, persist_dir=PERSIST_DIR):\n",
    "    db = Chroma.from_documents(\n",
    "        documents=docs, embedding=embedder, persist_directory=persist_dir\n",
    "    )\n",
    "    db.persist()\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790b3261",
   "metadata": {},
   "source": [
    "# 7. Calling the Load Document Function \n",
    "\n",
    "This cell loads the source document (text and metadata) from the JSON file, and\n",
    "splits it into smaller chunks for embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c349a0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and split 30 chunks from JSON.\n"
     ]
    }
   ],
   "source": [
    "documents = load_documents_from_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a12c5f",
   "metadata": {},
   "source": [
    "# 8. Calling the Build Chroma Function\n",
    "\n",
    "This cell builds a Chroma vector database\n",
    "that stores those embeddings for efficient similarity search.\n",
    "\n",
    "Once the database is built, it’s saved to disk,\n",
    "so you only need to run this cell once, unless you change or add new data.\n",
    "\n",
    "Running it again will overwrite the existing database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a744010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:55:11] offline tracker init\n",
      "[codecarbon WARNING @ 18:55:11] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 18:55:11] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 18:55:11] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 18:55:14] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 18:55:14] CPU Model on constant consumption mode: Intel(R) Core(TM) i5-8350U CPU @ 1.70GHz\n",
      "[codecarbon WARNING @ 18:55:14] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 18:55:14] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 18:55:14] No GPU found.\n",
      "[codecarbon INFO @ 18:55:14] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: global constant\n",
      "                GPU Tracking Method: Unspecified\n",
      "            \n",
      "[codecarbon INFO @ 18:55:14] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 18:55:14]   Platform system: Windows-11-10.0.26200-SP0\n",
      "[codecarbon INFO @ 18:55:14]   Python version: 3.12.10\n",
      "[codecarbon INFO @ 18:55:14]   CodeCarbon version: 3.0.8\n",
      "[codecarbon INFO @ 18:55:14]   Available RAM : 15.843 GB\n",
      "[codecarbon INFO @ 18:55:14]   CPU count: 8 thread(s) in 8 physical CPU(s)\n",
      "[codecarbon INFO @ 18:55:14]   CPU model: Intel(R) Core(TM) i5-8350U CPU @ 1.70GHz\n",
      "[codecarbon INFO @ 18:55:14]   GPU count: None\n",
      "[codecarbon INFO @ 18:55:14]   GPU model: None\n",
      "[codecarbon INFO @ 18:55:14] Emissions data (if any) will be saved to file c:\\Users\\YNA\\ELO2_GREEN_AI\\rag_and_distilled_model\\emissions.csv\n",
      "[codecarbon INFO @ 18:55:19] Energy consumed for RAM : 0.000014 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 18:55:19] Delta energy consumed for CPU with constant : 0.000083 kWh, power : 60.0 W\n",
      "[codecarbon INFO @ 18:55:19] Energy consumed for All CPU : 0.000083 kWh\n",
      "[codecarbon INFO @ 18:55:19] 0.000096 kWh of electricity and 0.000000 L of water were used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings creation emissions: 0.000055 kg CO2\n"
     ]
    }
   ],
   "source": [
    "tracker_embeddings = OfflineEmissionsTracker(country_iso_code=COUNTRY_ISO_CODE)\n",
    "tracker_embeddings.start()\n",
    "\n",
    "db = build_chroma_store(documents)\n",
    "\n",
    "emissions_embeddings = tracker_embeddings.stop()\n",
    "print(f\"Embeddings creation emissions: {emissions_embeddings:.6f} kg CO2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670d000f",
   "metadata": {},
   "source": [
    "# 9. RAG Response Generation and Answer Refinement\n",
    "\n",
    "Defines functions to generate answers using Retrieval-Augmented Generation (RAG), detect issues in responses, and refine them:\n",
    "\n",
    "- `detect_answer_issues()`: Checks if an answer is incomplete, repetitive, cut off, or disclaimer-only.  \n",
    "- `retry_with_better_retrieval()`: Performs additional retrieval when issues are detected.  \n",
    "- `refine_failed_answer()`: Re-generates answers based on improved context and task-specific instructions.  \n",
    "- `generate_rag_response()`: Combines retrieval, LLM generation, and recursive refinement to produce a final answer.  \n",
    "- `ask()`: Simple wrapper to query the system and print the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab0bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_answer_issues(\n",
    "    answer, question, tokens_generated=None, max_tokens=MAX_NEW_TOKENS\n",
    "):\n",
    "    \n",
    "    answer_lower = answer.lower().strip()\n",
    "\n",
    "    if (\n",
    "        \"no relevant information\" in answer_lower\n",
    "        or \"does not provide information\" in answer_lower\n",
    "    ):\n",
    "        return True, \"no_info\"\n",
    "\n",
    "    sentences = [s.strip() for s in answer.split(\".\") if len(s.strip()) > 10]\n",
    "    if len(sentences) >= 2:\n",
    "        sentence_counts = {}\n",
    "        for sent in sentences:\n",
    "            sent_normalized = sent.lower().strip()\n",
    "            if sent_normalized:\n",
    "                sentence_counts[sent_normalized] = (\n",
    "                    sentence_counts.get(sent_normalized, 0) + 1\n",
    "                )\n",
    "\n",
    "        if any(count > 1 for count in sentence_counts.values()):\n",
    "            return True, \"repetitive\"\n",
    "\n",
    "    if tokens_generated and tokens_generated >= max_tokens - 5:\n",
    "        return True, \"token_limit\"\n",
    "\n",
    "    if answer and len(answer) > 20:\n",
    "        last_char = answer.strip()[-1]\n",
    "        if last_char not in '.!?\":)]}':\n",
    "            return True, \"incomplete\"\n",
    "\n",
    "        if len(sentences) > 1:\n",
    "            last_sentence = sentences[-1].strip()\n",
    "            if len(last_sentence) > 0 and len(last_sentence) < 20:\n",
    "                if last_sentence and last_sentence[0].islower():\n",
    "                    return True, \"cutoff\"\n",
    "\n",
    "    disclaimer_phrases = [\n",
    "        \"i'm sorry\",\n",
    "        \"i cannot\",\n",
    "        \"i don't have\",\n",
    "        \"not possible to determine\",\n",
    "        \"context does not\",\n",
    "    ]\n",
    "\n",
    "    if len(answer) < 150 and any(\n",
    "        phrase in answer_lower for phrase in disclaimer_phrases\n",
    "    ):\n",
    "        substantial_sentences = [\n",
    "            s\n",
    "            for s in sentences\n",
    "            if len(s.strip()) > 30\n",
    "            and not any(phrase in s.lower() for phrase in disclaimer_phrases)\n",
    "        ]\n",
    "        if len(substantial_sentences) == 0:\n",
    "            return True, \"disclaimer_only\"\n",
    "\n",
    "    return False, None\n",
    "\n",
    "\n",
    "def retry_with_better_retrieval(query_text, task_type, original_context, issue_type):\n",
    "    \n",
    "    if issue_type == \"no_info\":\n",
    "        k = 8\n",
    "        threshold = 0.15\n",
    "    elif issue_type in [\"incomplete\", \"cutoff\", \"token_limit\", \"repetitive\"]:\n",
    "        k = 3\n",
    "        threshold = RELEVANCE_THRESHOLD\n",
    "    elif issue_type == \"disclaimer_only\":\n",
    "        k = 7\n",
    "        threshold = 0.2\n",
    "    else:\n",
    "        k = TOP_K_RESULTS\n",
    "        threshold = RELEVANCE_THRESHOLD\n",
    "\n",
    "    results = db.similarity_search_with_relevance_scores(query_text, k=k)\n",
    "\n",
    "    if len(results) == 0 or results[0][1] < threshold:\n",
    "        return None, None\n",
    "\n",
    "    if issue_type in [\"incomplete\", \"cutoff\", \"token_limit\", \"repetitive\"]:\n",
    "        context_parts = [doc.page_content for doc, _ in results[:3]]\n",
    "        context_text = \"\\n\\n\".join(context_parts)[:600]\n",
    "    else:\n",
    "        context_parts = [doc.page_content for doc, _ in results]\n",
    "        context_text = \"\\n\\n\".join(context_parts)[:1200]\n",
    "\n",
    "    return context_text, [score for _, score in results]\n",
    "\n",
    "\n",
    "def refine_failed_answer(original_answer, context, query_text, issue_type):\n",
    "    \n",
    "    if issue_type == \"no_info\":\n",
    "        refine_prompt = f\"\"\"Context: {context[:700]}\n",
    "\n",
    "Question: {query_text}\n",
    "\n",
    "Answer the question using only the information from the context above.\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    elif issue_type == \"repetitive\":\n",
    "        refine_prompt = f\"\"\"Context: {context[:500]}\n",
    "\n",
    "Question: {query_text}\n",
    "\n",
    "Provide a concise answer without repeating information. 2-3 distinct sentences:\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    elif issue_type in [\"incomplete\", \"cutoff\", \"token_limit\"]:\n",
    "        refine_prompt = f\"\"\"Context: {context[:500]}\n",
    "\n",
    "Question: {query_text}\n",
    "\n",
    "Provide a concise, complete answer in 2-3 sentences:\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    elif issue_type == \"disclaimer_only\":\n",
    "        refine_prompt = f\"\"\"Context: {context[:700]}\n",
    "\n",
    "Question: {query_text}\n",
    "\n",
    "Answer the question directly using information from the context. Be specific and factual.\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    else:\n",
    "        return original_answer\n",
    "\n",
    "    tokens = tokenizer.encode(refine_prompt, truncation=False)\n",
    "    if len(tokens) > 450:\n",
    "        refine_prompt = tokenizer.decode(tokens[:450], skip_special_tokens=True)\n",
    "        refine_prompt += f\"\\n\\nQuestion: {query_text}\\n\\nAnswer:\"\n",
    "\n",
    "    refined = llm.invoke(refine_prompt)\n",
    "    return refined.strip()\n",
    "\n",
    "\n",
    "def generate_rag_response(\n",
    "    query_text, task_type, k=TOP_K_RESULTS, threshold=RELEVANCE_THRESHOLD\n",
    "):\n",
    "    results = db.similarity_search_with_relevance_scores(query_text, k=k)\n",
    "\n",
    "    if task_type == \"creative_generation\":\n",
    "        threshold = 0.1\n",
    "\n",
    "    if len(results) == 0 or results[0][1] < threshold:\n",
    "        if task_type == \"creative_generation\" and len(results) > 0:\n",
    "            context_text = results[0][0].page_content\n",
    "        else:\n",
    "            return {\n",
    "                \"answer\": \"No relevant information found.\",\n",
    "                \"sources\": [],\n",
    "                \"task_type\": task_type,\n",
    "                \"scores\": [],\n",
    "                \"iterations\": 0,\n",
    "                \"issue_detected\": \"no_info\",\n",
    "                \"fixed\": False,\n",
    "                \"should_retry\": True,\n",
    "            }\n",
    "    else:\n",
    "        context_text = \"\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "\n",
    "    prompt_template = PromptTemplate.from_template(PROMPT_TEMPLATES[task_type])\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "\n",
    "    tokens = tokenizer.encode(prompt, truncation=False)\n",
    "    token_limit_exceeded = len(tokens) > 400\n",
    "\n",
    "    if token_limit_exceeded:\n",
    "        truncated_tokens = tokens[:450]\n",
    "        truncated_prompt = tokenizer.decode(truncated_tokens, skip_special_tokens=True)\n",
    "        prompt = truncated_prompt + f\"\\n\\nQuestion: {query_text}\\n\\nAnswer:\"\n",
    "        if \"Context:\" in truncated_prompt:\n",
    "            try:\n",
    "                context_text = (\n",
    "                    truncated_prompt.split(\"Context:\")[1].split(\"Question:\")[0].strip()\n",
    "                )\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    answer = llm.invoke(prompt)\n",
    "\n",
    "    answer_tokens = len(tokenizer.encode(answer, truncation=False))\n",
    "\n",
    "    sources = [doc.metadata.get(\"source\", \"Unknown\") for doc, _score in results]\n",
    "\n",
    "    iterations = 0\n",
    "    issue_detected = None\n",
    "    fixed = False\n",
    "\n",
    "    if ENABLE_RECURSIVE_EDITING:\n",
    "        has_issues, issue_type = detect_answer_issues(\n",
    "            answer, query_text, answer_tokens, MAX_NEW_TOKENS\n",
    "        )\n",
    "\n",
    "        if has_issues:\n",
    "            issue_detected = issue_type\n",
    "\n",
    "            for iteration in range(MAX_EDIT_ITERATIONS):\n",
    "                new_context, new_scores = retry_with_better_retrieval(\n",
    "                    query_text, task_type, context_text, issue_type\n",
    "                )\n",
    "\n",
    "                if new_context:\n",
    "                    context_text = new_context\n",
    "                    if new_scores:\n",
    "                        sources = [\n",
    "                            doc.metadata.get(\"source\", \"Unknown\")\n",
    "                            for doc, _ in results[: len(new_scores)]\n",
    "                        ]\n",
    "\n",
    "                iterations += 1\n",
    "                refined_answer = refine_failed_answer(\n",
    "                    answer, context_text, query_text, issue_type\n",
    "                )\n",
    "\n",
    "                refined_tokens = len(tokenizer.encode(refined_answer, truncation=False))\n",
    "                still_has_issues, _ = detect_answer_issues(\n",
    "                    refined_answer, query_text, refined_tokens, MAX_NEW_TOKENS\n",
    "                )\n",
    "\n",
    "                if not still_has_issues and refined_answer != answer:\n",
    "                    answer = refined_answer\n",
    "                    fixed = True\n",
    "                    break\n",
    "                elif refined_answer != answer:\n",
    "                    answer = refined_answer\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"sources\": sources,\n",
    "        \"task_type\": task_type,\n",
    "        \"scores\": [score for _, score in results],\n",
    "        \"iterations\": iterations,\n",
    "        \"issue_detected\": issue_detected,\n",
    "        \"fixed\": fixed,\n",
    "    }\n",
    "\n",
    "\n",
    "def ask(question, task_type=\"rag\"):\n",
    "    result = generate_rag_response(question, task_type)\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"A: {result['answer']}\")\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eff73ec",
   "metadata": {},
   "source": [
    "# 10. Load evaluation prompts\n",
    "\n",
    "We load a list of test questions from a JSON file.\n",
    "Each question is labeled with a category (e.g., summarization, reasoning, or RAG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4377d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15 evaluation prompts\n",
      "\n",
      "Categories:\n",
      "  - Summarization: 5 prompts\n",
      "  - Reasoning: 5 prompts\n",
      "  - Rag: 5 prompts\n"
     ]
    }
   ],
   "source": [
    "with open(PROMPTS_FILE, \"r\") as f:\n",
    "    prompts_data = json.load(f)\n",
    "\n",
    "prompts = prompts_data[\"prompts\"]\n",
    "print(f\"Loaded {len(prompts)} evaluation prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1001c44c",
   "metadata": {},
   "source": [
    "# 11. Run Prompts and Track Metrics\n",
    "\n",
    "Iterates over all prompts, generates RAG responses, retries if necessary, and collects metrics:\n",
    "\n",
    "- Measures **latency** and **energy usage** using `OfflineEmissionsTracker`.  \n",
    "- Retries queries with a lower threshold if no relevant information is found and recursive editing is enabled.  \n",
    "- Tracks **task-level success rates** and response metadata (`iterations`, `issue_detected`, `fixed`).  \n",
    "- Stores all results in a list for later analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef553e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:55:19] offline tracker init\n",
      "[codecarbon WARNING @ 18:55:19] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 18:55:20] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 18:55:20] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 18:55:23] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 18:55:23] CPU Model on constant consumption mode: Intel(R) Core(TM) i5-8350U CPU @ 1.70GHz\n",
      "[codecarbon WARNING @ 18:55:23] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 18:55:23] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 18:55:23] No GPU found.\n",
      "[codecarbon INFO @ 18:55:23] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: global constant\n",
      "                GPU Tracking Method: Unspecified\n",
      "            \n",
      "[codecarbon INFO @ 18:55:23] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 18:55:23]   Platform system: Windows-11-10.0.26200-SP0\n",
      "[codecarbon INFO @ 18:55:23]   Python version: 3.12.10\n",
      "[codecarbon INFO @ 18:55:23]   CodeCarbon version: 3.0.8\n",
      "[codecarbon INFO @ 18:55:23]   Available RAM : 15.843 GB\n",
      "[codecarbon INFO @ 18:55:23]   CPU count: 8 thread(s) in 8 physical CPU(s)\n",
      "[codecarbon INFO @ 18:55:23]   CPU model: Intel(R) Core(TM) i5-8350U CPU @ 1.70GHz\n",
      "[codecarbon INFO @ 18:55:23]   GPU count: None\n",
      "[codecarbon INFO @ 18:55:23]   GPU model: None\n",
      "[codecarbon INFO @ 18:55:23] Emissions data (if any) will be saved to file c:\\Users\\YNA\\ELO2_GREEN_AI\\rag_and_distilled_model\\emissions.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Prompt 1: Summarize the main events during the Apollo 11 lunar landing in 3 sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:55:38] Energy consumed for RAM : 0.000042 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 18:55:38] Delta energy consumed for CPU with constant : 0.000251 kWh, power : 60.0 W\n",
      "[codecarbon INFO @ 18:55:38] Energy consumed for All CPU : 0.000251 kWh\n",
      "[codecarbon INFO @ 18:55:38] 0.000293 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model Answer: The main events during the Apollo 11 lunar landing were: 1. The computer prevented an abort. 2. A complete set of recovery programs was incorporated into the software to eliminate lower priority tasks and re-establish the more important ones. 3. Armstrong collected a contingency soil sample using a sample bag on a stick. 4. Aldrin joined Armstrong on the surface. 5. The surface dust was described as \"very fine-grained\" and \"almost like a\n",
      "\n",
      "Testing Prompt 2: What were the main challenges Armstrong faced while landing the Eagle?\n",
      " Model Answer: Armstrong initially had some difficulties squeezing through the hatch with his portable life support system (PLSS).\n",
      "\n",
      "Testing Prompt 3: Describe the activities the astronauts performed on the lunar surface.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:55:53] Energy consumed for RAM : 0.000083 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 18:55:53] Delta energy consumed for CPU with constant : 0.000250 kWh, power : 60.0 W\n",
      "[codecarbon INFO @ 18:55:53] Energy consumed for All CPU : 0.000501 kWh\n",
      "[codecarbon INFO @ 18:55:53] 0.000585 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 18:56:08] Energy consumed for RAM : 0.000125 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 18:56:08] Delta energy consumed for CPU with constant : 0.000250 kWh, power : 60.0 W\n",
      "[codecarbon INFO @ 18:56:08] Energy consumed for All CPU : 0.000751 kWh\n",
      "[codecarbon INFO @ 18:56:08] 0.000877 kWh of electricity and 0.000000 L of water were used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model Answer: The astronauts planted the Lunar Flag Assembly containing a flag of the United States on the lunar surface, in clear view of the TV camera. They also lifted film and two sample boxes containing 21.55 kilograms (47.5 lb) of lunar surface material to the LM hatch using a flat cable pulley device called the Lunar Equipment Conveyor (LEC).\n",
      "\n",
      "Testing Prompt 4: Explain what scientific equipment the astronauts deployed on the Moon.\n",
      " Model Answer: The astronauts deployed the EASEP, which included a Passive Seismic Experiment Package used to measure moonquakes and a retroreflector array used for the lunar laser ranging experiment.\n",
      "\n",
      "Testing Prompt 5: Compare the planned timeline for the lunar surface operations with what actually happened.\n",
      " Model Answer: No relevant information found.\n",
      "\n",
      "Testing Prompt 6: Why did the computer alarms (1201 and 1202) occur during the descent?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:56:23] Energy consumed for RAM : 0.000167 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 18:56:23] Delta energy consumed for CPU with constant : 0.000250 kWh, power : 60.0 W\n",
      "[codecarbon INFO @ 18:56:23] Energy consumed for All CPU : 0.001001 kWh\n",
      "[codecarbon INFO @ 18:56:23] 0.001168 kWh of electricity and 0.000000 L of water were used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model Answer: The computer alarms (1201 and 1202) occurred during the descent to indicate \"executive overflows\", meaning the guidance computer could not complete all its tasks in real-time and had to postpone some of them.\n",
      "\n",
      "Testing Prompt 7: What would have happened if Armstrong had not taken manual control during the landing?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:56:38] Energy consumed for RAM : 0.000208 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 18:56:38] Delta energy consumed for CPU with constant : 0.000250 kWh, power : 60.0 W\n",
      "[codecarbon INFO @ 18:56:38] Energy consumed for All CPU : 0.001252 kWh\n",
      "[codecarbon INFO @ 18:56:38] 0.001460 kWh of electricity and 0.000000 L of water were used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model Answer: The context does not provide information about what would have happened if Armstrong had not taken manual control during the landing.\n",
      "\n",
      "Testing Prompt 8: Why did Armstrong's famous quote become controversial?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:56:53] Energy consumed for RAM : 0.000250 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 18:56:53] Delta energy consumed for CPU with constant : 0.000250 kWh, power : 60.0 W\n",
      "[codecarbon INFO @ 18:56:53] Energy consumed for All CPU : 0.001501 kWh\n",
      "[codecarbon INFO @ 18:56:53] 0.001751 kWh of electricity and 0.000000 L of water were used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model Answer: Armstrong's famous quote became controversial because the word \"a\" was not audible in the transmission and was not initially reported by most observers of the live broadcast.\n",
      "\n",
      "Testing Prompt 9: Analyze how the fuel situation during landing reflects the risk management challenges of the mission.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:57:08] Energy consumed for RAM : 0.000292 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 18:57:08] Delta energy consumed for CPU with constant : 0.000250 kWh, power : 60.0 W\n",
      "[codecarbon INFO @ 18:57:08] Energy consumed for All CPU : 0.001751 kWh\n",
      "[codecarbon INFO @ 18:57:08] 0.002043 kWh of electricity and 0.000000 L of water were used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model Answer: The fuel situation during landing reflects the risk management challenges of the mission because the LM guidance computer (LGC) distracted the crew with the first of several unexpected 1201 and 1202 program alarms.\n",
      "\n",
      "Testing Prompt 10: Based on the text, what does Margaret Hamilton's statement reveal about the Apollo Guidance Computer's design philosophy?\n",
      " Model Answer: Margaret Hamilton's statement reveals that the Apollo Guidance Computer was programmed to do more than recognize error conditions and incorporated a complete set of recovery programs to eliminate lower priority tasks and re-establish the more important ones.\n",
      "\n",
      "Testing Prompt 11: At what time (UTC) did Eagle land on the Moon?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:57:23] Energy consumed for RAM : 0.000333 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 18:57:23] Delta energy consumed for CPU with constant : 0.000250 kWh, power : 60.0 W\n",
      "[codecarbon INFO @ 18:57:23] Energy consumed for All CPU : 0.002001 kWh\n",
      "[codecarbon INFO @ 18:57:23] 0.002335 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 18:57:23] 0.011083 g.CO2eq/s mean an estimation of 349.514354629282 kg.CO2eq/year\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model Answer: Eagle landed at 20:17:40 UTC on Sunday July 20.\n",
      " Expected: 20:17:40 UTC on July 20\n",
      "\n",
      "Testing Prompt 12: How much lunar material did the astronauts collect?\n",
      " Model Answer: The astronauts collected 21.55 kilograms (47.5 lb) of lunar surface material.\n",
      " Expected: 21.55 kilograms (47.5 lb)\n",
      "\n",
      "Testing Prompt 13: What was Armstrong's famous first words when stepping on the Moon?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:57:38] Energy consumed for RAM : 0.000375 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 18:57:38] Delta energy consumed for CPU with constant : 0.000250 kWh, power : 60.0 W\n",
      "[codecarbon INFO @ 18:57:38] Energy consumed for All CPU : 0.002251 kWh\n",
      "[codecarbon INFO @ 18:57:38] 0.002626 kWh of electricity and 0.000000 L of water were used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model Answer: Armstrong's famous first words when stepping on the Moon were \"Here men from the planet Earth first set foot upon the Moon July 1969, A. D. We came in peace for all mankind.\"\n",
      " Expected: That's one small step for [a] man, one giant leap for mankind\n",
      "\n",
      "Testing Prompt 14: What scientific instruments were included in the EASEP package?\n",
      " Model Answer: No relevant information found.\n",
      " Expected: Passive Seismic Experiment Package and retroreflector array\n",
      "\n",
      "Testing Prompt 15: How much usable fuel remained when Eagle landed, and how many seconds of powered flight did this represent?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:57:53] Energy consumed for RAM : 0.000417 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 18:57:53] Delta energy consumed for CPU with constant : 0.000250 kWh, power : 60.0 W\n",
      "[codecarbon INFO @ 18:57:53] Energy consumed for All CPU : 0.002501 kWh\n",
      "[codecarbon INFO @ 18:57:53] 0.002918 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 18:57:56] Energy consumed for RAM : 0.000425 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 18:57:56] Delta energy consumed for CPU with constant : 0.000053 kWh, power : 60.0 W\n",
      "[codecarbon INFO @ 18:57:56] Energy consumed for All CPU : 0.002554 kWh\n",
      "[codecarbon INFO @ 18:57:56] 0.002980 kWh of electricity and 0.000000 L of water were used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model Answer: The LM had enough fuel for another 25 seconds of powered flight before an abort without touchdown would have become unsafe, but post-mission analysis showed that the real figure was probably closer to 50 seconds.\n",
      " Expected: 216 pounds (98 kg); about 25 seconds according to initial estimates, but post-mission analysis showed closer to 50 seconds\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "task_metrics = {\n",
    "    \"summarization\": {\"total\": 0, \"success\": 0},\n",
    "    \"reasoning\": {\"total\": 0, \"success\": 0},\n",
    "    \"rag\": {\"total\": 0, \"success\": 0},\n",
    "    \"paraphrasing\": {\"total\": 0, \"success\": 0},\n",
    "    \"creative_generation\": {\"total\": 0, \"success\": 0},\n",
    "}\n",
    "\n",
    "total_latency = 0\n",
    "latencies = []\n",
    "\n",
    "tracker_inference = OfflineEmissionsTracker(\n",
    "    country_iso_code=COUNTRY_ISO_CODE, project_name=\"inference\", log_level=\"error\"\n",
    ")\n",
    "tracker_inference.start()\n",
    "\n",
    "for p in prompts:\n",
    "    start_time = time.time()\n",
    "    result = generate_rag_response(p[\"prompt\"], task_type=p[\"category\"])\n",
    "\n",
    "    if (\n",
    "        result[\"answer\"] == \"No relevant information found.\"\n",
    "        and ENABLE_RECURSIVE_EDITING\n",
    "    ):\n",
    "        print(f\"  [Retrying with lower threshold...]\")\n",
    "        retry_results = db.similarity_search_with_relevance_scores(p[\"prompt\"], k=8)\n",
    "\n",
    "        if len(retry_results) > 0 and retry_results[0][1] > 0.1:\n",
    "            retry_context = \"\\n\\n\".join(\n",
    "                [doc.page_content for doc, _ in retry_results[:5]]\n",
    "            )[:800]\n",
    "\n",
    "            retry_prompt = f\"\"\"Context: {retry_context}\n",
    "\n",
    "Question: {p[\"prompt\"]}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "            retry_tokens = tokenizer.encode(retry_prompt, truncation=False)\n",
    "            if len(retry_tokens) > 450:\n",
    "                retry_prompt = tokenizer.decode(\n",
    "                    retry_tokens[:450], skip_special_tokens=True\n",
    "                )\n",
    "                retry_prompt += f\"\\n\\nQuestion: {p['prompt']}\\n\\nAnswer:\"\n",
    "\n",
    "            retry_answer = llm.invoke(retry_prompt)\n",
    "\n",
    "            if retry_answer and retry_answer != \"No relevant information found.\":\n",
    "                result[\"answer\"] = retry_answer\n",
    "                result[\"fixed\"] = True\n",
    "                result[\"iterations\"] = 1\n",
    "                result[\"issue_detected\"] = \"no_info\"\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    latency = end_time - start_time\n",
    "    total_latency += latency\n",
    "    latencies.append(latency)\n",
    "\n",
    "    edit_info = \"\"\n",
    "    if result.get(\"iterations\", 0) > 0:\n",
    "        status = \"Fixed\" if result.get(\"fixed\") else \"Attempted\"\n",
    "        issue = result.get(\"issue_detected\", \"unknown\")\n",
    "        edit_info = f\" [{status}: {issue}, {result['iterations']}x]\"\n",
    "\n",
    "    print(f\"\\nPrompt {p['id']}: {p['prompt']}\")\n",
    "    print(\n",
    "        f\"Answer: {result['answer'][:150]}{'...' if len(result['answer']) > 150 else ''}{edit_info}\"\n",
    "    )\n",
    "    print(f\"Latency: {latency:.3f}s\")\n",
    "\n",
    "    task_metrics[p[\"category\"]][\"total\"] += 1\n",
    "    if result[\"answer\"] != \"No relevant information found.\":\n",
    "        task_metrics[p[\"category\"]][\"success\"] += 1\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"id\": p[\"id\"],\n",
    "            \"category\": p[\"category\"],\n",
    "            \"difficulty\": p[\"difficulty\"],\n",
    "            \"prompt\": p[\"prompt\"],\n",
    "            \"answer\": result[\"answer\"],\n",
    "            \"expected\": p.get(\"expected_answer\"),\n",
    "            \"scores\": result[\"scores\"],\n",
    "            \"iterations\": result.get(\"iterations\", 0),\n",
    "            \"issue_detected\": result.get(\"issue_detected\"),\n",
    "            \"fixed\": result.get(\"fixed\", False),\n",
    "            \"latency\": latency,\n",
    "        }\n",
    "    )\n",
    "\n",
    "emissions_inference = tracker_inference.stop()\n",
    "energy_inference = (\n",
    "    tracker_inference._total_energy.kWh\n",
    "    if hasattr(tracker_inference._total_energy, \"kWh\")\n",
    "    else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9fb6bf",
   "metadata": {},
   "source": [
    "# 12. Performance and Carbon Emissions\n",
    "\n",
    "Calculates and prints performance metrics for all prompts:\n",
    "\n",
    "- **Latency:** total, average, minimum, and maximum per query.  \n",
    "- **Carbon emissions and energy consumption:** for model loading, embeddings, and inference.  \n",
    "- Computes total and per-query values for both CO2 emissions and energy usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bae5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Loading:        0.000047 kg CO2\n",
      "Embeddings Creation:  0.000055 kg CO2\n",
      "15 queries: 0.001699 kg CO2\n",
      "TOTAL EMISSIONS:      0.001801 kg CO2\n",
      "Equivalent to:        1.80 g CO2\n",
      "  - Per query average: 0.000113 kg CO2\n"
     ]
    }
   ],
   "source": [
    "print(\"PERFORMANCE METRICS\")\n",
    "print(\"─\" * 80)\n",
    "\n",
    "avg_latency = total_latency / len(prompts)\n",
    "min_latency = min(latencies)\n",
    "max_latency = max(latencies)\n",
    "\n",
    "print(f\"Total latency:       {total_latency:.3f}s\")\n",
    "print(f\"Average per query:   {avg_latency:.3f}s\")\n",
    "print(f\"Min latency:         {min_latency:.3f}s\")\n",
    "print(f\"Max latency:         {max_latency:.3f}s\")\n",
    "\n",
    "print(\"\\n\" + \"─\" * 80)\n",
    "print(\"CARBON EMISSIONS & ENERGY\")\n",
    "print(\"─\" * 80)\n",
    "\n",
    "total_emissions = emissions_loading + emissions_embeddings + emissions_inference\n",
    "\n",
    "try:\n",
    "    energy_loading = tracker_loading._total_energy.kWh\n",
    "    energy_embeddings = tracker_embeddings._total_energy.kWh\n",
    "    energy_inference_val = energy_inference\n",
    "    total_energy = energy_loading + energy_embeddings + energy_inference_val\n",
    "\n",
    "    print(\n",
    "        f\"Model Loading:       {emissions_loading:.6f} kg CO2  |  {energy_loading:.6f} kWh\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Embeddings:          {emissions_embeddings:.6f} kg CO2  |  {energy_embeddings:.6f} kWh\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Inference:           {emissions_inference:.6f} kg CO2  |  {energy_inference_val:.6f} kWh\"\n",
    "    )\n",
    "    print(f\"{'─' * 80}\")\n",
    "    print(\n",
    "        f\"TOTAL:               {total_emissions:.6f} kg CO2  |  {total_energy:.6f} kWh\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Per query:           {emissions_inference / len(prompts):.6f} kg CO2  |  {energy_inference_val / len(prompts):.6f} kWh\"\n",
    "    )\n",
    "except:\n",
    "    print(f\"Model Loading:       {emissions_loading:.6f} kg CO2\")\n",
    "    print(f\"Embeddings:          {emissions_embeddings:.6f} kg CO2\")\n",
    "    print(f\"Inference:           {emissions_inference:.6f} kg CO2\")\n",
    "    print(f\"{'─' * 80}\")\n",
    "    print(f\"TOTAL:               {total_emissions:.6f} kg CO2\")\n",
    "    print(f\"Per query:           {emissions_inference / len(prompts):.6f} kg CO2\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
