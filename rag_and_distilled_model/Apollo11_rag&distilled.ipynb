{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "269fd429",
   "metadata": {},
   "source": [
    "# 1. Install required packages\n",
    "\n",
    "We install all the dependencies needed for building a\n",
    "Retrieval-Augmented Generation (RAG) pipeline.\n",
    "These include LangChain components, Hugging Face models,\n",
    "ChromaDB for vector storage, and PyTorch for GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b52616a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install somepackage -qq langchain langchain-community langchain-core langchain-text-splitters langchain-huggingface sentence-transformers chromadb transformers torch accelerate unstructured codecarbon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed814cfe",
   "metadata": {},
   "source": [
    "# 2. Import libraries and set configuration\n",
    "\n",
    "Here we import the necessary modules and define paths, constants,\n",
    "and model settings.\n",
    "We also suppress warnings to keep the notebook output clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03e12116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from codecarbon import OfflineEmissionsTracker\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PROMPTS_FILE = \"data/test_data.json\"\n",
    "PERSIST_DIR = \"data/chroma_db\"\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "CHUNK_SIZE = 400\n",
    "CHUNK_OVERLAP = 50\n",
    "TOP_K_RESULTS = 5\n",
    "RELEVANCE_THRESHOLD = 0.3\n",
    "LLM_MODEL = \"MBZUAI/LaMini-Flan-T5-248M\"\n",
    "MAX_NEW_TOKENS = 100\n",
    "LLM_TEMPERATURE = 0.2\n",
    "COUNTRY_ISO_CODE = \"EGY\"\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"Answer the question about Apollo 11 based on the context below. If you cannot answer based on the context, say \"I don't have enough information to answer that.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6e30b4",
   "metadata": {},
   "source": [
    "# 3. Initialize embedding model and text splitter\n",
    "\n",
    "The embedding model converts text into numeric vectors, while the text\n",
    "splitter breaks long documents into manageable chunks for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50dc94c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d3954a",
   "metadata": {},
   "source": [
    "# 4. Load the local language model\n",
    "\n",
    "We initialize a small, local LLM (LaMini-Flan-T5) that can run on CPU or GPU.\n",
    "This model will later generate answers based on retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86c57183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:55:03] offline tracker init\n",
      "[codecarbon WARNING @ 18:55:03] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 18:55:03] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 18:55:03] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 18:55:07] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 18:55:07] CPU Model on constant consumption mode: Intel(R) Core(TM) i5-8350U CPU @ 1.70GHz\n",
      "[codecarbon WARNING @ 18:55:07] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 18:55:07] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 18:55:07] No GPU found.\n",
      "[codecarbon INFO @ 18:55:07] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: global constant\n",
      "                GPU Tracking Method: Unspecified\n",
      "            \n",
      "[codecarbon INFO @ 18:55:07] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 18:55:07]   Platform system: Windows-11-10.0.26200-SP0\n",
      "[codecarbon INFO @ 18:55:07]   Python version: 3.12.10\n",
      "[codecarbon INFO @ 18:55:07]   CodeCarbon version: 3.0.8\n",
      "[codecarbon INFO @ 18:55:07]   Available RAM : 15.843 GB\n",
      "[codecarbon INFO @ 18:55:07]   CPU count: 8 thread(s) in 8 physical CPU(s)\n",
      "[codecarbon INFO @ 18:55:07]   CPU model: Intel(R) Core(TM) i5-8350U CPU @ 1.70GHz\n",
      "[codecarbon INFO @ 18:55:07]   GPU count: None\n",
      "[codecarbon INFO @ 18:55:07]   GPU model: None\n",
      "[codecarbon INFO @ 18:55:07] Emissions data (if any) will be saved to file c:\\Users\\YNA\\ELO2_GREEN_AI\\rag_and_distilled_model\\emissions.csv\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Device set to use cpu\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "[codecarbon INFO @ 18:55:11] Energy consumed for RAM : 0.000012 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 18:55:11] Delta energy consumed for CPU with constant : 0.000070 kWh, power : 60.0 W\n",
      "[codecarbon INFO @ 18:55:11] Energy consumed for All CPU : 0.000070 kWh\n",
      "[codecarbon INFO @ 18:55:11] 0.000082 kWh of electricity and 0.000000 L of water were used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loading emissions: 0.000047 kg CO2\n"
     ]
    }
   ],
   "source": [
    "def initialize_local_llm():\n",
    "    tracker = OfflineEmissionsTracker(country_iso_code=COUNTRY_ISO_CODE)\n",
    "    tracker.start()\n",
    "    \n",
    "    device = 0 if USE_GPU else -1\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        LLM_MODEL,\n",
    "        torch_dtype=torch.float16 if USE_GPU else torch.float32,\n",
    "        device_map=\"auto\" if USE_GPU else None,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    pipe = pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        temperature=LLM_TEMPERATURE,\n",
    "        repetition_penalty=1.2,\n",
    "        do_sample=False,\n",
    "        top_p=0.95,\n",
    "        device=device,\n",
    "    )\n",
    "    \n",
    "    emissions = tracker.stop()\n",
    "    print(f\"Model loading emissions: {emissions:.6f} kg CO2\")\n",
    "    \n",
    "    return HuggingFacePipeline(pipeline=pipe), emissions\n",
    "\n",
    "llm, emissions_loading = initialize_local_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59668d86",
   "metadata": {},
   "source": [
    "# 5. Load documents from JSON\n",
    "\n",
    "We read the context and metadata directly from a JSON file.\n",
    "We also clean metadata and split text into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85f71f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_from_json(json_path=PROMPTS_FILE):\n",
    "    data_path = Path(json_path)\n",
    "    if not data_path.exists():\n",
    "        print(f\"JSON file not found at: {json_path}\")\n",
    "        return []\n",
    "\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    source_text = data.get(\"source_text\", \"\")\n",
    "    metadata = data.get(\"metadata\", {})\n",
    "\n",
    "    if not source_text.strip():\n",
    "        print(\"No source text found in JSON.\")\n",
    "        return []\n",
    "\n",
    "    for k, v in metadata.items():\n",
    "        if isinstance(v, (list, dict)):\n",
    "            metadata[k] = str(v)\n",
    "\n",
    "    split_docs = splitter.create_documents([source_text])\n",
    "\n",
    "    for doc in split_docs:\n",
    "        doc.metadata = metadata.copy()\n",
    "        doc.metadata[\"topic\"] = \"Apollo 11\"\n",
    "        doc.metadata[\"section\"] = \", \".join(metadata.get(\"sections\", [\"General\"]))\n",
    "\n",
    "    print(f\"Loaded and split {len(split_docs)} chunks from JSON.\")\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854b765d",
   "metadata": {},
   "source": [
    "# 6. Build Chroma vector store\n",
    "\n",
    "Here we embed the document chunks and save them into a local vector database (Chroma).\n",
    "This enables fast similarity-based retrieval of relevant context later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3335a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chroma_store(docs, persist_dir=PERSIST_DIR):\n",
    "    db = Chroma.from_documents(\n",
    "        documents=docs, embedding=embedder, persist_directory=persist_dir\n",
    "    )\n",
    "    db.persist()\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790b3261",
   "metadata": {},
   "source": [
    "# 7. Calling the Load Document Function \n",
    "\n",
    "This cell loads the source document (text and metadata) from the JSON file, and\n",
    "splits it into smaller chunks for embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c349a0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and split 30 chunks from JSON.\n"
     ]
    }
   ],
   "source": [
    "documents = load_documents_from_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a12c5f",
   "metadata": {},
   "source": [
    "# 8. Calling the Build Chroma Function\n",
    "\n",
    "This cell builds a Chroma vector database\n",
    "that stores those embeddings for efficient similarity search.\n",
    "\n",
    "Once the database is built, it’s saved to disk,\n",
    "so you only need to run this cell once, unless you change or add new data.\n",
    "\n",
    "Running it again will overwrite the existing database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a744010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:55:11] offline tracker init\n",
      "[codecarbon WARNING @ 18:55:11] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 18:55:11] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 18:55:11] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 18:55:14] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 18:55:14] CPU Model on constant consumption mode: Intel(R) Core(TM) i5-8350U CPU @ 1.70GHz\n",
      "[codecarbon WARNING @ 18:55:14] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 18:55:14] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 18:55:14] No GPU found.\n",
      "[codecarbon INFO @ 18:55:14] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: global constant\n",
      "                GPU Tracking Method: Unspecified\n",
      "            \n",
      "[codecarbon INFO @ 18:55:14] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 18:55:14]   Platform system: Windows-11-10.0.26200-SP0\n",
      "[codecarbon INFO @ 18:55:14]   Python version: 3.12.10\n",
      "[codecarbon INFO @ 18:55:14]   CodeCarbon version: 3.0.8\n",
      "[codecarbon INFO @ 18:55:14]   Available RAM : 15.843 GB\n",
      "[codecarbon INFO @ 18:55:14]   CPU count: 8 thread(s) in 8 physical CPU(s)\n",
      "[codecarbon INFO @ 18:55:14]   CPU model: Intel(R) Core(TM) i5-8350U CPU @ 1.70GHz\n",
      "[codecarbon INFO @ 18:55:14]   GPU count: None\n",
      "[codecarbon INFO @ 18:55:14]   GPU model: None\n",
      "[codecarbon INFO @ 18:55:14] Emissions data (if any) will be saved to file c:\\Users\\YNA\\ELO2_GREEN_AI\\rag_and_distilled_model\\emissions.csv\n",
      "[codecarbon INFO @ 18:55:19] Energy consumed for RAM : 0.000014 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 18:55:19] Delta energy consumed for CPU with constant : 0.000083 kWh, power : 60.0 W\n",
      "[codecarbon INFO @ 18:55:19] Energy consumed for All CPU : 0.000083 kWh\n",
      "[codecarbon INFO @ 18:55:19] 0.000096 kWh of electricity and 0.000000 L of water were used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings creation emissions: 0.000055 kg CO2\n"
     ]
    }
   ],
   "source": [
    "tracker_embeddings = OfflineEmissionsTracker(country_iso_code=COUNTRY_ISO_CODE)\n",
    "tracker_embeddings.start()\n",
    "\n",
    "db = build_chroma_store(documents)\n",
    "\n",
    "emissions_embeddings = tracker_embeddings.stop()\n",
    "print(f\"Embeddings creation emissions: {emissions_embeddings:.6f} kg CO2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670d000f",
   "metadata": {},
   "source": [
    "# 9. Define query and response generation\n",
    "\n",
    "These functions retrieve the most relevant text chunks and use the\n",
    "LLM to answer a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ab0bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_database(query_text, k=TOP_K_RESULTS, threshold=RELEVANCE_THRESHOLD):\n",
    "    results = db.similarity_search_with_relevance_scores(query_text, k=k)\n",
    "\n",
    "    if len(results) == 0 or results[0][1] < threshold:\n",
    "        return []\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def generate_rag_response(\n",
    "    query_text, k=TOP_K_RESULTS, threshold=RELEVANCE_THRESHOLD, verbose=False\n",
    "):\n",
    "    results = db.similarity_search_with_relevance_scores(query_text, k=k)\n",
    "\n",
    "    if len(results) == 0 or results[0][1] < threshold:\n",
    "        return {\n",
    "            \"answer\": \"No relevant information found.\",\n",
    "            \"sources\": [],\n",
    "            \"context\": \"\",\n",
    "            \"prompt\": \"\",\n",
    "        }\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = PromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "\n",
    "    if llm is None:\n",
    "        return {\n",
    "            \"answer\": \"LLM not initialized.\",\n",
    "            \"sources\": [],\n",
    "            \"context\": context_text,\n",
    "            \"prompt\": prompt,\n",
    "        }\n",
    "\n",
    "    response_text = llm.invoke(prompt)\n",
    "    sources = [doc.metadata.get(\"source\", \"Unknown\") for doc, _score in results]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nQuery: {query_text}\")\n",
    "        print(f\"\\nAnswer: {response_text}\")\n",
    "        print(f\"\\nSources: {', '.join([Path(s).name for s in sources])}\")\n",
    "\n",
    "    return {\n",
    "        \"answer\": response_text,\n",
    "        \"sources\": sources,\n",
    "        \"context\": context_text,\n",
    "        \"prompt\": prompt,\n",
    "        \"scores\": [score for _, score in results],\n",
    "    }\n",
    "\n",
    "\n",
    "def ask(query_text):\n",
    "    result = generate_rag_response(query_text, verbose=True)\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eff73ec",
   "metadata": {},
   "source": [
    "# 10. Load evaluation prompts\n",
    "\n",
    "We load a list of test questions from a JSON file.\n",
    "Each question is labeled with a category (e.g., summarization, reasoning, or RAG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4377d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15 evaluation prompts\n",
      "\n",
      "Categories:\n",
      "  - Summarization: 5 prompts\n",
      "  - Reasoning: 5 prompts\n",
      "  - Rag: 5 prompts\n"
     ]
    }
   ],
   "source": [
    "with open(PROMPTS_FILE, \"r\") as f:\n",
    "    prompts_data = json.load(f)\n",
    "\n",
    "prompts = prompts_data[\"prompts\"]\n",
    "print(f\"Loaded {len(prompts)} evaluation prompts\")\n",
    "print(\"\\nCategories:\")\n",
    "for category in [\"summarization\", \"reasoning\", \"rag\"]:\n",
    "    count = len([p for p in prompts if p[\"category\"] == category])\n",
    "    print(f\"  - {category.title()}: {count} prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1001c44c",
   "metadata": {},
   "source": [
    "# 11. Run automated evaluation\n",
    "\n",
    "For each question, we generate an answer using the RAG system and print\n",
    "both the model’s response and the expected answer (if provided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fef553e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:55:19] offline tracker init\n",
      "[codecarbon WARNING @ 18:55:19] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 18:55:20] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 18:55:20] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 18:55:23] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 18:55:23] CPU Model on constant consumption mode: Intel(R) Core(TM) i5-8350U CPU @ 1.70GHz\n",
      "[codecarbon WARNING @ 18:55:23] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 18:55:23] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 18:55:23] No GPU found.\n",
      "[codecarbon INFO @ 18:55:23] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: global constant\n",
      "                GPU Tracking Method: Unspecified\n",
      "            \n",
      "[codecarbon INFO @ 18:55:23] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 18:55:23]   Platform system: Windows-11-10.0.26200-SP0\n",
      "[codecarbon INFO @ 18:55:23]   Python version: 3.12.10\n",
      "[codecarbon INFO @ 18:55:23]   CodeCarbon version: 3.0.8\n",
      "[codecarbon INFO @ 18:55:23]   Available RAM : 15.843 GB\n",
      "[codecarbon INFO @ 18:55:23]   CPU count: 8 thread(s) in 8 physical CPU(s)\n",
      "[codecarbon INFO @ 18:55:23]   CPU model: Intel(R) Core(TM) i5-8350U CPU @ 1.70GHz\n",
      "[codecarbon INFO @ 18:55:23]   GPU count: None\n",
      "[codecarbon INFO @ 18:55:23]   GPU model: None\n",
      "[codecarbon INFO @ 18:55:23] Emissions data (if any) will be saved to file c:\\Users\\YNA\\ELO2_GREEN_AI\\rag_and_distilled_model\\emissions.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Prompt 1: Summarize the main events during the Apollo 11 lunar landing in 3 sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:55:38] Energy consumed for RAM : 0.000042 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 18:55:38] Delta energy consumed for CPU with constant : 0.000251 kWh, power : 60.0 W\n",
      "[codecarbon INFO @ 18:55:38] Energy consumed for All CPU : 0.000251 kWh\n",
      "[codecarbon INFO @ 18:55:38] 0.000293 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model Answer: The main events during the Apollo 11 lunar landing were: 1. The computer prevented an abort. 2. A complete set of recovery programs was incorporated into the software to eliminate lower priority tasks and re-establish the more important ones. 3. Armstrong collected a contingency soil sample using a sample bag on a stick. 4. Aldrin joined Armstrong on the surface. 5. The surface dust was described as \"very fine-grained\" and \"almost like a\n",
      "\n",
      "Testing Prompt 2: What were the main challenges Armstrong faced while landing the Eagle?\n",
      " Model Answer: Armstrong initially had some difficulties squeezing through the hatch with his portable life support system (PLSS).\n",
      "\n",
      "Testing Prompt 3: Describe the activities the astronauts performed on the lunar surface.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:55:53] Energy consumed for RAM : 0.000083 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 18:55:53] Delta energy consumed for CPU with constant : 0.000250 kWh, power : 60.0 W\n",
      "[codecarbon INFO @ 18:55:53] Energy consumed for All CPU : 0.000501 kWh\n",
      "[codecarbon INFO @ 18:55:53] 0.000585 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 18:56:08] Energy consumed for RAM : 0.000125 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 18:56:08] Delta energy consumed for CPU with constant : 0.000250 kWh, power : 60.0 W\n",
      "[codecarbon INFO @ 18:56:08] Energy consumed for All CPU : 0.000751 kWh\n",
      "[codecarbon INFO @ 18:56:08] 0.000877 kWh of electricity and 0.000000 L of water were used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model Answer: The astronauts planted the Lunar Flag Assembly containing a flag of the United States on the lunar surface, in clear view of the TV camera. They also lifted film and two sample boxes containing 21.55 kilograms (47.5 lb) of lunar surface material to the LM hatch using a flat cable pulley device called the Lunar Equipment Conveyor (LEC).\n",
      "\n",
      "Testing Prompt 4: Explain what scientific equipment the astronauts deployed on the Moon.\n",
      " Model Answer: The astronauts deployed the EASEP, which included a Passive Seismic Experiment Package used to measure moonquakes and a retroreflector array used for the lunar laser ranging experiment.\n",
      "\n",
      "Testing Prompt 5: Compare the planned timeline for the lunar surface operations with what actually happened.\n",
      " Model Answer: No relevant information found.\n",
      "\n",
      "Testing Prompt 6: Why did the computer alarms (1201 and 1202) occur during the descent?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:56:23] Energy consumed for RAM : 0.000167 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 18:56:23] Delta energy consumed for CPU with constant : 0.000250 kWh, power : 60.0 W\n",
      "[codecarbon INFO @ 18:56:23] Energy consumed for All CPU : 0.001001 kWh\n",
      "[codecarbon INFO @ 18:56:23] 0.001168 kWh of electricity and 0.000000 L of water were used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model Answer: The computer alarms (1201 and 1202) occurred during the descent to indicate \"executive overflows\", meaning the guidance computer could not complete all its tasks in real-time and had to postpone some of them.\n",
      "\n",
      "Testing Prompt 7: What would have happened if Armstrong had not taken manual control during the landing?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:56:38] Energy consumed for RAM : 0.000208 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 18:56:38] Delta energy consumed for CPU with constant : 0.000250 kWh, power : 60.0 W\n",
      "[codecarbon INFO @ 18:56:38] Energy consumed for All CPU : 0.001252 kWh\n",
      "[codecarbon INFO @ 18:56:38] 0.001460 kWh of electricity and 0.000000 L of water were used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model Answer: The context does not provide information about what would have happened if Armstrong had not taken manual control during the landing.\n",
      "\n",
      "Testing Prompt 8: Why did Armstrong's famous quote become controversial?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:56:53] Energy consumed for RAM : 0.000250 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 18:56:53] Delta energy consumed for CPU with constant : 0.000250 kWh, power : 60.0 W\n",
      "[codecarbon INFO @ 18:56:53] Energy consumed for All CPU : 0.001501 kWh\n",
      "[codecarbon INFO @ 18:56:53] 0.001751 kWh of electricity and 0.000000 L of water were used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model Answer: Armstrong's famous quote became controversial because the word \"a\" was not audible in the transmission and was not initially reported by most observers of the live broadcast.\n",
      "\n",
      "Testing Prompt 9: Analyze how the fuel situation during landing reflects the risk management challenges of the mission.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:57:08] Energy consumed for RAM : 0.000292 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 18:57:08] Delta energy consumed for CPU with constant : 0.000250 kWh, power : 60.0 W\n",
      "[codecarbon INFO @ 18:57:08] Energy consumed for All CPU : 0.001751 kWh\n",
      "[codecarbon INFO @ 18:57:08] 0.002043 kWh of electricity and 0.000000 L of water were used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model Answer: The fuel situation during landing reflects the risk management challenges of the mission because the LM guidance computer (LGC) distracted the crew with the first of several unexpected 1201 and 1202 program alarms.\n",
      "\n",
      "Testing Prompt 10: Based on the text, what does Margaret Hamilton's statement reveal about the Apollo Guidance Computer's design philosophy?\n",
      " Model Answer: Margaret Hamilton's statement reveals that the Apollo Guidance Computer was programmed to do more than recognize error conditions and incorporated a complete set of recovery programs to eliminate lower priority tasks and re-establish the more important ones.\n",
      "\n",
      "Testing Prompt 11: At what time (UTC) did Eagle land on the Moon?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:57:23] Energy consumed for RAM : 0.000333 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 18:57:23] Delta energy consumed for CPU with constant : 0.000250 kWh, power : 60.0 W\n",
      "[codecarbon INFO @ 18:57:23] Energy consumed for All CPU : 0.002001 kWh\n",
      "[codecarbon INFO @ 18:57:23] 0.002335 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 18:57:23] 0.011083 g.CO2eq/s mean an estimation of 349.514354629282 kg.CO2eq/year\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model Answer: Eagle landed at 20:17:40 UTC on Sunday July 20.\n",
      " Expected: 20:17:40 UTC on July 20\n",
      "\n",
      "Testing Prompt 12: How much lunar material did the astronauts collect?\n",
      " Model Answer: The astronauts collected 21.55 kilograms (47.5 lb) of lunar surface material.\n",
      " Expected: 21.55 kilograms (47.5 lb)\n",
      "\n",
      "Testing Prompt 13: What was Armstrong's famous first words when stepping on the Moon?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:57:38] Energy consumed for RAM : 0.000375 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 18:57:38] Delta energy consumed for CPU with constant : 0.000250 kWh, power : 60.0 W\n",
      "[codecarbon INFO @ 18:57:38] Energy consumed for All CPU : 0.002251 kWh\n",
      "[codecarbon INFO @ 18:57:38] 0.002626 kWh of electricity and 0.000000 L of water were used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model Answer: Armstrong's famous first words when stepping on the Moon were \"Here men from the planet Earth first set foot upon the Moon July 1969, A. D. We came in peace for all mankind.\"\n",
      " Expected: That's one small step for [a] man, one giant leap for mankind\n",
      "\n",
      "Testing Prompt 14: What scientific instruments were included in the EASEP package?\n",
      " Model Answer: No relevant information found.\n",
      " Expected: Passive Seismic Experiment Package and retroreflector array\n",
      "\n",
      "Testing Prompt 15: How much usable fuel remained when Eagle landed, and how many seconds of powered flight did this represent?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:57:53] Energy consumed for RAM : 0.000417 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 18:57:53] Delta energy consumed for CPU with constant : 0.000250 kWh, power : 60.0 W\n",
      "[codecarbon INFO @ 18:57:53] Energy consumed for All CPU : 0.002501 kWh\n",
      "[codecarbon INFO @ 18:57:53] 0.002918 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 18:57:56] Energy consumed for RAM : 0.000425 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 18:57:56] Delta energy consumed for CPU with constant : 0.000053 kWh, power : 60.0 W\n",
      "[codecarbon INFO @ 18:57:56] Energy consumed for All CPU : 0.002554 kWh\n",
      "[codecarbon INFO @ 18:57:56] 0.002980 kWh of electricity and 0.000000 L of water were used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model Answer: The LM had enough fuel for another 25 seconds of powered flight before an abort without touchdown would have become unsafe, but post-mission analysis showed that the real figure was probably closer to 50 seconds.\n",
      " Expected: 216 pounds (98 kg); about 25 seconds according to initial estimates, but post-mission analysis showed closer to 50 seconds\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "tracker_inference = OfflineEmissionsTracker(country_iso_code=COUNTRY_ISO_CODE)\n",
    "tracker_inference.start()\n",
    "\n",
    "for p in prompts:\n",
    "    question = p[\"prompt\"]\n",
    "    expected = p.get(\"expected_answer\", None)\n",
    "    print(f\"\\nTesting Prompt {p['id']}: {question}\")\n",
    "\n",
    "    result = generate_rag_response(question, verbose=False)\n",
    "    answer = result[\"answer\"]\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"id\": p[\"id\"],\n",
    "            \"category\": p[\"category\"],\n",
    "            \"difficulty\": p[\"difficulty\"],\n",
    "            \"prompt\": question,\n",
    "            \"answer\": answer,\n",
    "            \"expected\": expected,\n",
    "            \"context_used\": len(result[\"context\"]),\n",
    "            \"top_sources\": result[\"sources\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\" Model Answer: {answer}\")\n",
    "    if expected:\n",
    "        print(f\" Expected: {expected}\")\n",
    "\n",
    "emissions_inference = tracker_inference.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9fb6bf",
   "metadata": {},
   "source": [
    "# 12. Calculate Total Emissions\n",
    "\n",
    "We calculate the total emissions from our model including, model loading, embedding creation and query emissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09bae5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Loading:        0.000047 kg CO2\n",
      "Embeddings Creation:  0.000055 kg CO2\n",
      "15 queries: 0.001699 kg CO2\n",
      "TOTAL EMISSIONS:      0.001801 kg CO2\n",
      "Equivalent to:        1.80 g CO2\n",
      "  - Per query average: 0.000113 kg CO2\n"
     ]
    }
   ],
   "source": [
    "total_emissions = emissions_loading + emissions_embeddings + emissions_inference\n",
    "\n",
    "print(f\"\\nModel Loading:        {emissions_loading:.6f} kg CO2\")\n",
    "print(f\"Embeddings Creation:  {emissions_embeddings:.6f} kg CO2\")\n",
    "print(f\"15 queries: {emissions_inference:.6f} kg CO2\")\n",
    "print(f\"TOTAL EMISSIONS:      {total_emissions:.6f} kg CO2\")\n",
    "print(f\"Equivalent to:        {total_emissions * 1000:.2f} g CO2\")\n",
    "print(f\"  - Per query average: {emissions_inference/len(prompts):.6f} kg CO2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
