{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "269fd429",
   "metadata": {},
   "source": [
    "# 1. Install required packages\n",
    "\n",
    "We install all the dependencies needed for building a\n",
    "Retrieval-Augmented Generation (RAG) pipeline.\n",
    "These include LangChain components, Hugging Face models,\n",
    "ChromaDB for vector storage, and PyTorch for GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52616a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install somepackage -qq langchain langchain-community langchain-core langchain-text-splitters langchain-huggingface sentence-transformers chromadb transformers torch accelerate unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed814cfe",
   "metadata": {},
   "source": [
    "# 2. Import libraries and set configuration\n",
    "\n",
    "Here we import the necessary modules and define paths, constants,\n",
    "and model settings.\n",
    "We also suppress warnings to keep the notebook output clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e12116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PROMPTS_FILE = \"data/test_data.json\"\n",
    "PERSIST_DIR = \"data/chroma_db\"\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "CHUNK_SIZE = 400\n",
    "CHUNK_OVERLAP = 50\n",
    "TOP_K_RESULTS = 5\n",
    "RELEVANCE_THRESHOLD = 0.3\n",
    "LLM_MODEL = \"MBZUAI/LaMini-Flan-T5-248M\"\n",
    "MAX_NEW_TOKENS = 100\n",
    "LLM_TEMPERATURE = 0.2\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"Answer the question about Apollo 11 based on the context below. If you cannot answer based on the context, say \"I don't have enough information to answer that.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6e30b4",
   "metadata": {},
   "source": [
    "# 3. Initialize embedding model and text splitter\n",
    "\n",
    "The embedding model converts text into numeric vectors, while the text\n",
    "splitter breaks long documents into manageable chunks for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dc94c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d3954a",
   "metadata": {},
   "source": [
    "# 4. Load the local language model\n",
    "\n",
    "We initialize a small, local LLM (LaMini-Flan-T5) that can run on CPU or GPU.\n",
    "This model will later generate answers based on retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c57183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_local_llm():\n",
    "    device = 0 if USE_GPU else -1\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        LLM_MODEL,\n",
    "        torch_dtype=torch.float16 if USE_GPU else torch.float32,\n",
    "        device_map=\"auto\" if USE_GPU else None,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    pipe = pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        temperature=LLM_TEMPERATURE,\n",
    "        repetition_penalty=1.2,\n",
    "        do_sample=False,\n",
    "        top_p=0.95,\n",
    "        device=device,\n",
    "    )\n",
    "    return HuggingFacePipeline(pipeline=pipe)\n",
    "llm = initialize_local_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59668d86",
   "metadata": {},
   "source": [
    "# Load documents from JSON\n",
    "\n",
    "We read the context and metadata directly from a JSON file.\n",
    "We also clean metadata and split text into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f71f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_from_json(json_path=PROMPTS_FILE):\n",
    "    data_path = Path(json_path)\n",
    "    if not data_path.exists():\n",
    "        print(f\"JSON file not found at: {json_path}\")\n",
    "        return []\n",
    "\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    source_text = data.get(\"source_text\", \"\")\n",
    "    metadata = data.get(\"metadata\", {})\n",
    "\n",
    "    if not source_text.strip():\n",
    "        print(\"No source text found in JSON.\")\n",
    "        return []\n",
    "\n",
    "    for k, v in metadata.items():\n",
    "        if isinstance(v, (list, dict)):\n",
    "            metadata[k] = str(v)\n",
    "\n",
    "    split_docs = splitter.create_documents([source_text])\n",
    "\n",
    "    for doc in split_docs:\n",
    "        doc.metadata = metadata.copy()\n",
    "        doc.metadata[\"topic\"] = \"Apollo 11\"\n",
    "        doc.metadata[\"section\"] = \", \".join(metadata.get(\"sections\", [\"General\"]))\n",
    "\n",
    "    print(f\"Loaded and split {len(split_docs)} chunks from JSON.\")\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854b765d",
   "metadata": {},
   "source": [
    "# 6. Build Chroma vector store\n",
    "\n",
    "Here we embed the document chunks and save them into a local vector database (Chroma).\n",
    "This enables fast similarity-based retrieval of relevant context later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3335a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chroma_store(docs, persist_dir=PERSIST_DIR):\n",
    "    db = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embedder,\n",
    "        persist_directory=persist_dir\n",
    "    )\n",
    "    db.persist()\n",
    "    return db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790b3261",
   "metadata": {},
   "source": [
    "# 7. Calling the Load Document Function \n",
    "\n",
    "This cell loads the source document (text and metadata) from the JSON file, and\n",
    "splits it into smaller chunks for embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c349a0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_documents_from_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a12c5f",
   "metadata": {},
   "source": [
    "# 8. Calling the Build Chroma Function\n",
    "\n",
    "This cell builds a Chroma vector database\n",
    "that stores those embeddings for efficient similarity search.\n",
    "\n",
    "Once the database is built, itâ€™s saved to disk,\n",
    "so you only need to run this cell once, unless you change or add new data.\n",
    "\n",
    "Running it again will overwrite the existing database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a744010",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = build_chroma_store(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670d000f",
   "metadata": {},
   "source": [
    "# 9. Define query and response generation\n",
    "\n",
    "These functions retrieve the most relevant text chunks and use the\n",
    "LLM to answer a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab0bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_database(query_text, k=TOP_K_RESULTS, threshold=RELEVANCE_THRESHOLD):\n",
    "    results = db.similarity_search_with_relevance_scores(query_text, k=k)\n",
    "    \n",
    "    if len(results) == 0 or results[0][1] < threshold:\n",
    "        return []\n",
    "    \n",
    "    return results\n",
    "\n",
    "def generate_rag_response(\n",
    "    query_text, k=TOP_K_RESULTS, threshold=RELEVANCE_THRESHOLD, verbose=False\n",
    "):\n",
    "    results = db.similarity_search_with_relevance_scores(query_text, k=k)\n",
    "\n",
    "    if len(results) == 0 or results[0][1] < threshold:\n",
    "        return {\n",
    "            \"answer\": \"No relevant information found.\",\n",
    "            \"sources\": [],\n",
    "            \"context\": \"\",\n",
    "            \"prompt\": \"\",\n",
    "        }\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = PromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "\n",
    "    if llm is None:\n",
    "        return {\n",
    "            \"answer\": \"LLM not initialized.\",\n",
    "            \"sources\": [],\n",
    "            \"context\": context_text,\n",
    "            \"prompt\": prompt,\n",
    "        }\n",
    "\n",
    "    response_text = llm.invoke(prompt)\n",
    "    sources = [doc.metadata.get(\"source\", \"Unknown\") for doc, _score in results]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nQuery: {query_text}\")\n",
    "        print(f\"\\nAnswer: {response_text}\")\n",
    "        print(f\"\\nSources: {', '.join([Path(s).name for s in sources])}\")\n",
    "\n",
    "    return {\n",
    "        \"answer\": response_text,\n",
    "        \"sources\": sources,\n",
    "        \"context\": context_text,\n",
    "        \"prompt\": prompt,\n",
    "        \"scores\": [score for _, score in results],\n",
    "    }\n",
    "    \n",
    "def ask(query_text):\n",
    "    result = generate_rag_response(query_text, verbose=True)\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eff73ec",
   "metadata": {},
   "source": [
    "# 10. Load evaluation prompts\n",
    "\n",
    "We load a list of test questions from a JSON file.\n",
    "Each question is labeled with a category (e.g., summarization, reasoning, or RAG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4377d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PROMPTS_FILE, \"r\") as f:\n",
    "    prompts_data = json.load(f)\n",
    "\n",
    "prompts = prompts_data[\"prompts\"]\n",
    "print(f\"Loaded {len(prompts)} evaluation prompts\")\n",
    "print(\"\\nCategories:\")\n",
    "for category in [\"summarization\", \"reasoning\", \"rag\"]:\n",
    "    count = len([p for p in prompts if p[\"category\"] == category])\n",
    "    print(f\"  - {category.title()}: {count} prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1001c44c",
   "metadata": {},
   "source": [
    "# 11. Run automated evaluation\n",
    "\n",
    "For each question, we generate an answer using the RAG system and print\n",
    "both the modelâ€™s response and the expected answer (if provided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef553e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for p in prompts:\n",
    "    question = p[\"prompt\"]\n",
    "    expected = p.get(\"expected_answer\", None)\n",
    "    print(f\"\\nTesting Prompt {p['id']}: {question}\")\n",
    "\n",
    "    result = generate_rag_response(question, verbose=False)\n",
    "    answer = result[\"answer\"]\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"id\": p[\"id\"],\n",
    "            \"category\": p[\"category\"],\n",
    "            \"difficulty\": p[\"difficulty\"],\n",
    "            \"prompt\": question,\n",
    "            \"answer\": answer,\n",
    "            \"expected\": expected,\n",
    "            \"context_used\": len(result[\"context\"]),\n",
    "            \"top_sources\": result[\"sources\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\" Model Answer: {answer}\")\n",
    "    if expected:\n",
    "        print(f\" Expected: {expected}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
