{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "269fd429",
   "metadata": {},
   "source": [
    "# 1. Install required packages\n",
    "\n",
    "We install all the dependencies needed for building a\n",
    "Retrieval-Augmented Generation (RAG) pipeline.\n",
    "These include LangChain components, Hugging Face models,\n",
    "ChromaDB for vector storage, and PyTorch for GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52616a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install somepackage -qq langchain langchain-community langchain-core langchain-text-splitters langchain-huggingface sentence-transformers chromadb transformers torch accelerate unstructured codecarbon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed814cfe",
   "metadata": {},
   "source": [
    "# 2. Imports and Configuration\n",
    "Imports necessary libraries, sets constants for models, embeddings, chunking, and loads prompt templates from a JSON file for various NLP tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e12116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch\n",
    "import warnings\n",
    "from codecarbon import OfflineEmissionsTracker\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PROMPTS_FILE = \"data/test_data.json\"\n",
    "PERSIST_DIR = \"data/chroma_db\"\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "CHUNK_SIZE = 400\n",
    "CHUNK_OVERLAP = 50\n",
    "TOP_K_RESULTS = 5\n",
    "RELEVANCE_THRESHOLD = 0.3\n",
    "LLM_MODEL = \"MBZUAI/LaMini-Flan-T5-248M\"\n",
    "MAX_NEW_TOKENS = 200\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "COUNTRY_ISO_CODE = \"EGY\"\n",
    "ENABLE_RECURSIVE_EDITING = True\n",
    "MAX_EDIT_ITERATIONS = 2\n",
    "\n",
    "with open(PROMPTS_FILE, \"r\") as f:\n",
    "    config_data = json.load(f)\n",
    "    MASTER_INSTRUCTION = config_data[\"metadata\"][\"master_instruction\"]\n",
    "    TASK_INSTRUCTIONS = config_data[\"metadata\"][\"task_instructions\"]\n",
    "\n",
    "\n",
    "def build_prompt_template(task_type):\n",
    "    task_instruction = TASK_INSTRUCTIONS[task_type]\n",
    "    return f\"\"\"{MASTER_INSTRUCTION}\n",
    "\n",
    "{task_instruction}\n",
    "\n",
    "Context:\n",
    "{{context}}\n",
    "\n",
    "Question: {{question}}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATES = {\n",
    "    \"summarization\": build_prompt_template(\"summarization\"),\n",
    "    \"reasoning\": build_prompt_template(\"reasoning\"),\n",
    "    \"rag\": build_prompt_template(\"rag\"),\n",
    "    \"paraphrasing\": build_prompt_template(\"paraphrasing\"),\n",
    "    \"creative_generation\": build_prompt_template(\"creative_generation\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6e30b4",
   "metadata": {},
   "source": [
    "# 3. Initialize embedding model and text splitter\n",
    "\n",
    "The embedding model converts text into numeric vectors, while the text\n",
    "splitter breaks long documents into manageable chunks for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dc94c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d3954a",
   "metadata": {},
   "source": [
    "# 4. Load the local language model\n",
    "\n",
    "Initializes the HuggingFace Seq2Seq model and tokenizer, wraps it in a pipeline for text generation, and tracks energy usage with `OfflineEmissionsTracker`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c57183",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker_loading = OfflineEmissionsTracker(\n",
    "    country_iso_code=COUNTRY_ISO_CODE, project_name=\"model_loading\", log_level=\"error\"\n",
    ")\n",
    "tracker_loading.start()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(LLM_MODEL, low_cpu_mem_usage=True)\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.3,\n",
    "    device=0 if USE_GPU else -1,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "emissions_loading = tracker_loading.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59668d86",
   "metadata": {},
   "source": [
    "# 5. Load documents from JSON\n",
    "\n",
    "We read the context and metadata directly from a JSON file.\n",
    "We also clean metadata and split text into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f71f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_from_json(json_path=PROMPTS_FILE):\n",
    "    data_path = Path(json_path)\n",
    "    if not data_path.exists():\n",
    "        print(f\"JSON file not found at: {json_path}\")\n",
    "        return []\n",
    "\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    source_text = data.get(\"source_text\", \"\")\n",
    "    metadata = data.get(\"metadata\", {})\n",
    "\n",
    "    if not source_text.strip():\n",
    "        print(\"No source text found in JSON.\")\n",
    "        return []\n",
    "\n",
    "    for k, v in metadata.items():\n",
    "        if isinstance(v, (list, dict)):\n",
    "            metadata[k] = str(v)\n",
    "\n",
    "    split_docs = splitter.create_documents([source_text])\n",
    "\n",
    "    for doc in split_docs:\n",
    "        doc.metadata = metadata.copy()\n",
    "        doc.metadata[\"topic\"] = \"Apollo 11\"\n",
    "        doc.metadata[\"section\"] = \", \".join(metadata.get(\"sections\", [\"General\"]))\n",
    "\n",
    "    print(f\"Loaded and split {len(split_docs)} chunks from JSON.\")\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854b765d",
   "metadata": {},
   "source": [
    "# 6. Build Chroma vector store\n",
    "\n",
    "Here we embed the document chunks and save them into a local vector database (Chroma).\n",
    "This enables fast similarity-based retrieval of relevant context later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3335a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chroma_store(docs, persist_dir=PERSIST_DIR):\n",
    "    db = Chroma.from_documents(\n",
    "        documents=docs, embedding=embedder, persist_directory=persist_dir\n",
    "    )\n",
    "    db.persist()\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790b3261",
   "metadata": {},
   "source": [
    "# 7. Calling the Load Document Function \n",
    "\n",
    "This cell loads the source document (text and metadata) from the JSON file, and\n",
    "splits it into smaller chunks for embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c349a0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_documents_from_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a12c5f",
   "metadata": {},
   "source": [
    "# 8. Calling the Build Chroma Function\n",
    "\n",
    "This cell builds a Chroma vector database\n",
    "that stores those embeddings for efficient similarity search.\n",
    "\n",
    "Once the database is built, itâ€™s saved to disk,\n",
    "so you only need to run this cell once, unless you change or add new data.\n",
    "\n",
    "Running it again will overwrite the existing database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a744010",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker_embeddings = OfflineEmissionsTracker(country_iso_code=COUNTRY_ISO_CODE)\n",
    "tracker_embeddings.start()\n",
    "\n",
    "db = build_chroma_store(documents)\n",
    "\n",
    "emissions_embeddings = tracker_embeddings.stop()\n",
    "print(f\"Embeddings creation emissions: {emissions_embeddings:.6f} kg CO2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670d000f",
   "metadata": {},
   "source": [
    "# 9. RAG Response Generation and Answer Refinement\n",
    "\n",
    "Defines functions to generate answers using Retrieval-Augmented Generation (RAG), detect issues in responses, and refine them:\n",
    "\n",
    "- `detect_answer_issues()`: Checks if an answer is incomplete, repetitive, cut off, or disclaimer-only.  \n",
    "- `retry_with_better_retrieval()`: Performs additional retrieval when issues are detected.  \n",
    "- `refine_failed_answer()`: Re-generates answers based on improved context and task-specific instructions.  \n",
    "- `generate_rag_response()`: Combines retrieval, LLM generation, and recursive refinement to produce a final answer.  \n",
    "- `ask()`: Simple wrapper to query the system and print the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab0bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_answer_issues(\n",
    "    answer, question, tokens_generated=None, max_tokens=MAX_NEW_TOKENS\n",
    "):\n",
    "    answer_lower = answer.lower().strip()\n",
    "\n",
    "    if (\n",
    "        \"no relevant information\" in answer_lower\n",
    "        or \"does not provide information\" in answer_lower\n",
    "    ):\n",
    "        return True, \"no_info\"\n",
    "\n",
    "    sentences = [s.strip() for s in answer.split(\".\") if len(s.strip()) > 10]\n",
    "    if len(sentences) >= 2:\n",
    "        sentence_counts = {}\n",
    "        for sent in sentences:\n",
    "            sent_normalized = sent.lower().strip()\n",
    "            if sent_normalized:\n",
    "                sentence_counts[sent_normalized] = (\n",
    "                    sentence_counts.get(sent_normalized, 0) + 1\n",
    "                )\n",
    "\n",
    "        if any(count > 1 for count in sentence_counts.values()):\n",
    "            return True, \"repetitive\"\n",
    "\n",
    "    if tokens_generated and tokens_generated >= max_tokens - 5:\n",
    "        return True, \"token_limit\"\n",
    "\n",
    "    if answer and len(answer) > 20:\n",
    "        last_char = answer.strip()[-1]\n",
    "        if last_char not in '.!?\":)]}':\n",
    "            return True, \"incomplete\"\n",
    "\n",
    "        if len(sentences) > 1:\n",
    "            last_sentence = sentences[-1].strip()\n",
    "            if len(last_sentence) > 0 and len(last_sentence) < 20:\n",
    "                if last_sentence and last_sentence[0].islower():\n",
    "                    return True, \"cutoff\"\n",
    "\n",
    "    disclaimer_phrases = [\n",
    "        \"i'm sorry\",\n",
    "        \"i cannot\",\n",
    "        \"i don't have\",\n",
    "        \"not possible to determine\",\n",
    "        \"context does not\",\n",
    "    ]\n",
    "\n",
    "    if len(answer) < 150 and any(\n",
    "        phrase in answer_lower for phrase in disclaimer_phrases\n",
    "    ):\n",
    "        substantial_sentences = [\n",
    "            s\n",
    "            for s in sentences\n",
    "            if len(s.strip()) > 30\n",
    "            and not any(phrase in s.lower() for phrase in disclaimer_phrases)\n",
    "        ]\n",
    "        if len(substantial_sentences) == 0:\n",
    "            return True, \"disclaimer_only\"\n",
    "\n",
    "    return False, None\n",
    "\n",
    "\n",
    "def retry_with_better_retrieval(query_text, task_type, original_context, issue_type):\n",
    "    if issue_type == \"no_info\":\n",
    "        k = 8\n",
    "        threshold = 0.15\n",
    "    elif issue_type in [\"incomplete\", \"cutoff\", \"token_limit\", \"repetitive\"]:\n",
    "        k = 3\n",
    "        threshold = RELEVANCE_THRESHOLD\n",
    "    elif issue_type == \"disclaimer_only\":\n",
    "        k = 7\n",
    "        threshold = 0.2\n",
    "    else:\n",
    "        k = TOP_K_RESULTS\n",
    "        threshold = RELEVANCE_THRESHOLD\n",
    "\n",
    "    results = db.similarity_search_with_relevance_scores(query_text, k=k)\n",
    "\n",
    "    if len(results) == 0 or results[0][1] < threshold:\n",
    "        return None, None\n",
    "\n",
    "    if issue_type in [\"incomplete\", \"cutoff\", \"token_limit\", \"repetitive\"]:\n",
    "        context_parts = [doc.page_content for doc, _ in results[:3]]\n",
    "        context_text = \"\\n\\n\".join(context_parts)[:600]\n",
    "    else:\n",
    "        context_parts = [doc.page_content for doc, _ in results]\n",
    "        context_text = \"\\n\\n\".join(context_parts)[:1200]\n",
    "\n",
    "    return context_text, [score for _, score in results]\n",
    "\n",
    "\n",
    "def refine_failed_answer(original_answer, context, query_text, issue_type):\n",
    "    if issue_type == \"no_info\":\n",
    "        refine_prompt = f\"\"\"Context: {context[:700]}\n",
    "\n",
    "Question: {query_text}\n",
    "\n",
    "Answer the question using only the information from the context above.\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    elif issue_type == \"repetitive\":\n",
    "        refine_prompt = f\"\"\"Context: {context[:500]}\n",
    "\n",
    "Question: {query_text}\n",
    "\n",
    "Provide a concise answer without repeating information. 2-3 distinct sentences:\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    elif issue_type in [\"incomplete\", \"cutoff\", \"token_limit\"]:\n",
    "        refine_prompt = f\"\"\"Context: {context[:500]}\n",
    "\n",
    "Question: {query_text}\n",
    "\n",
    "Provide a concise, complete answer in 2-3 sentences:\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    elif issue_type == \"disclaimer_only\":\n",
    "        refine_prompt = f\"\"\"Context: {context[:700]}\n",
    "\n",
    "Question: {query_text}\n",
    "\n",
    "Answer the question directly using information from the context. Be specific and factual.\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    else:\n",
    "        return original_answer\n",
    "\n",
    "    tokens = tokenizer.encode(refine_prompt, truncation=False)\n",
    "    if len(tokens) > 450:\n",
    "        refine_prompt = tokenizer.decode(tokens[:450], skip_special_tokens=True)\n",
    "        refine_prompt += f\"\\n\\nQuestion: {query_text}\\n\\nAnswer:\"\n",
    "\n",
    "    refined = llm.invoke(refine_prompt)\n",
    "    return refined.strip()\n",
    "\n",
    "\n",
    "def generate_rag_response(\n",
    "    query_text, task_type, k=TOP_K_RESULTS, threshold=RELEVANCE_THRESHOLD\n",
    "):\n",
    "    results = db.similarity_search_with_relevance_scores(query_text, k=k)\n",
    "\n",
    "    if task_type == \"creative_generation\":\n",
    "        threshold = 0.1\n",
    "\n",
    "    if len(results) == 0 or results[0][1] < threshold:\n",
    "        if task_type == \"creative_generation\" and len(results) > 0:\n",
    "            context_text = results[0][0].page_content\n",
    "        else:\n",
    "            return {\n",
    "                \"answer\": \"No relevant information found.\",\n",
    "                \"sources\": [],\n",
    "                \"task_type\": task_type,\n",
    "                \"scores\": [],\n",
    "                \"iterations\": 0,\n",
    "                \"issue_detected\": \"no_info\",\n",
    "                \"fixed\": False,\n",
    "                \"should_retry\": True,\n",
    "            }\n",
    "    else:\n",
    "        context_text = \"\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "\n",
    "    prompt_template = PromptTemplate.from_template(PROMPT_TEMPLATES[task_type])\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "\n",
    "    tokens = tokenizer.encode(prompt, truncation=False)\n",
    "    token_limit_exceeded = len(tokens) > 400\n",
    "\n",
    "    if token_limit_exceeded:\n",
    "        truncated_tokens = tokens[:450]\n",
    "        truncated_prompt = tokenizer.decode(truncated_tokens, skip_special_tokens=True)\n",
    "        prompt = truncated_prompt + f\"\\n\\nQuestion: {query_text}\\n\\nAnswer:\"\n",
    "        if \"Context:\" in truncated_prompt:\n",
    "            try:\n",
    "                context_text = (\n",
    "                    truncated_prompt.split(\"Context:\")[1].split(\"Question:\")[0].strip()\n",
    "                )\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    answer = llm.invoke(prompt)\n",
    "\n",
    "    answer_tokens = len(tokenizer.encode(answer, truncation=False))\n",
    "\n",
    "    sources = [doc.metadata.get(\"source\", \"Unknown\") for doc, _score in results]\n",
    "\n",
    "    iterations = 0\n",
    "    issue_detected = None\n",
    "    fixed = False\n",
    "\n",
    "    if ENABLE_RECURSIVE_EDITING:\n",
    "        has_issues, issue_type = detect_answer_issues(\n",
    "            answer, query_text, answer_tokens, MAX_NEW_TOKENS\n",
    "        )\n",
    "\n",
    "        if has_issues:\n",
    "            issue_detected = issue_type\n",
    "\n",
    "            for iteration in range(MAX_EDIT_ITERATIONS):\n",
    "                new_context, new_scores = retry_with_better_retrieval(\n",
    "                    query_text, task_type, context_text, issue_type\n",
    "                )\n",
    "\n",
    "                if new_context:\n",
    "                    context_text = new_context\n",
    "                    if new_scores:\n",
    "                        sources = [\n",
    "                            doc.metadata.get(\"source\", \"Unknown\")\n",
    "                            for doc, _ in results[: len(new_scores)]\n",
    "                        ]\n",
    "\n",
    "                iterations += 1\n",
    "                refined_answer = refine_failed_answer(\n",
    "                    answer, context_text, query_text, issue_type\n",
    "                )\n",
    "\n",
    "                refined_tokens = len(tokenizer.encode(refined_answer, truncation=False))\n",
    "                still_has_issues, _ = detect_answer_issues(\n",
    "                    refined_answer, query_text, refined_tokens, MAX_NEW_TOKENS\n",
    "                )\n",
    "\n",
    "                if not still_has_issues and refined_answer != answer:\n",
    "                    answer = refined_answer\n",
    "                    fixed = True\n",
    "                    break\n",
    "                elif refined_answer != answer:\n",
    "                    answer = refined_answer\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"sources\": sources,\n",
    "        \"task_type\": task_type,\n",
    "        \"scores\": [score for _, score in results],\n",
    "        \"iterations\": iterations,\n",
    "        \"issue_detected\": issue_detected,\n",
    "        \"fixed\": fixed,\n",
    "    }\n",
    "\n",
    "\n",
    "def ask(question, task_type=\"rag\"):\n",
    "    result = generate_rag_response(question, task_type)\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"A: {result['answer']}\")\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eff73ec",
   "metadata": {},
   "source": [
    "# 10. Load evaluation prompts\n",
    "\n",
    "We load a list of test questions from a JSON file.\n",
    "Each question is labeled with a category (e.g., summarization, reasoning, or RAG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4377d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PROMPTS_FILE, \"r\") as f:\n",
    "    prompts_data = json.load(f)\n",
    "\n",
    "prompts = prompts_data[\"prompts\"]\n",
    "print(f\"Loaded {len(prompts)} evaluation prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1001c44c",
   "metadata": {},
   "source": [
    "# 11. Run Prompts and Track Metrics\n",
    "\n",
    "Iterates over all prompts, generates RAG responses, retries if necessary, and collects metrics:\n",
    "\n",
    "- Measures **latency** and **energy usage** using `OfflineEmissionsTracker`.  \n",
    "- Retries queries with a lower threshold if no relevant information is found and recursive editing is enabled.  \n",
    "- Tracks **task-level success rates** and response metadata (`iterations`, `issue_detected`, `fixed`).  \n",
    "- Stores all results in a list for later analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef553e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "task_metrics = {\n",
    "    \"summarization\": {\"total\": 0, \"success\": 0},\n",
    "    \"reasoning\": {\"total\": 0, \"success\": 0},\n",
    "    \"rag\": {\"total\": 0, \"success\": 0},\n",
    "    \"paraphrasing\": {\"total\": 0, \"success\": 0},\n",
    "    \"creative_generation\": {\"total\": 0, \"success\": 0},\n",
    "}\n",
    "\n",
    "total_latency = 0\n",
    "latencies = []\n",
    "\n",
    "tracker_inference = OfflineEmissionsTracker(\n",
    "    country_iso_code=COUNTRY_ISO_CODE, project_name=\"inference\", log_level=\"error\"\n",
    ")\n",
    "tracker_inference.start()\n",
    "\n",
    "for p in prompts:\n",
    "    start_time = time.time()\n",
    "    result = generate_rag_response(p[\"prompt\"], task_type=p[\"category\"])\n",
    "\n",
    "    if (\n",
    "        result[\"answer\"] == \"No relevant information found.\"\n",
    "        and ENABLE_RECURSIVE_EDITING\n",
    "    ):\n",
    "        print(\"  [Retrying with lower threshold.]\")\n",
    "        retry_results = db.similarity_search_with_relevance_scores(p[\"prompt\"], k=8)\n",
    "\n",
    "        if len(retry_results) > 0 and retry_results[0][1] > 0.1:\n",
    "            retry_context = \"\\n\\n\".join(\n",
    "                [doc.page_content for doc, _ in retry_results[:5]]\n",
    "            )[:800]\n",
    "\n",
    "            retry_prompt = f\"\"\"Context: {retry_context}\n",
    "\n",
    "Question: {p[\"prompt\"]}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "            retry_tokens = tokenizer.encode(retry_prompt, truncation=False)\n",
    "            if len(retry_tokens) > 450:\n",
    "                retry_prompt = tokenizer.decode(\n",
    "                    retry_tokens[:450], skip_special_tokens=True\n",
    "                )\n",
    "                retry_prompt += f\"\\n\\nQuestion: {p['prompt']}\\n\\nAnswer:\"\n",
    "\n",
    "            retry_answer = llm.invoke(retry_prompt)\n",
    "\n",
    "            if retry_answer and retry_answer != \"No relevant information found.\":\n",
    "                result[\"answer\"] = retry_answer\n",
    "                result[\"fixed\"] = True\n",
    "                result[\"iterations\"] = 1\n",
    "                result[\"issue_detected\"] = \"no_info\"\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    latency = end_time - start_time\n",
    "    total_latency += latency\n",
    "    latencies.append(latency)\n",
    "\n",
    "    edit_info = \"\"\n",
    "    if result.get(\"iterations\", 0) > 0:\n",
    "        status = \"Fixed\" if result.get(\"fixed\") else \"Attempted\"\n",
    "        issue = result.get(\"issue_detected\", \"unknown\")\n",
    "        edit_info = f\" [{status}: {issue}, {result['iterations']}x]\"\n",
    "\n",
    "    print(f\"\\nPrompt {p['id']}: {p['prompt']}\")\n",
    "    print(\n",
    "        f\"Answer: {result['answer'][:150]}{'...' if len(result['answer']) > 150 else ''}{edit_info}\"\n",
    "    )\n",
    "    print(f\"Latency: {latency:.3f}s\")\n",
    "\n",
    "    task_metrics[p[\"category\"]][\"total\"] += 1\n",
    "    if result[\"answer\"] != \"No relevant information found.\":\n",
    "        task_metrics[p[\"category\"]][\"success\"] += 1\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"id\": p[\"id\"],\n",
    "            \"category\": p[\"category\"],\n",
    "            \"difficulty\": p[\"difficulty\"],\n",
    "            \"prompt\": p[\"prompt\"],\n",
    "            \"answer\": result[\"answer\"],\n",
    "            \"expected\": p.get(\"expected_answer\"),\n",
    "            \"scores\": result[\"scores\"],\n",
    "            \"iterations\": result.get(\"iterations\", 0),\n",
    "            \"issue_detected\": result.get(\"issue_detected\"),\n",
    "            \"fixed\": result.get(\"fixed\", False),\n",
    "            \"latency\": latency,\n",
    "        }\n",
    "    )\n",
    "\n",
    "emissions_inference = tracker_inference.stop()\n",
    "energy_inference = (\n",
    "    tracker_inference._total_energy.kWh\n",
    "    if hasattr(tracker_inference._total_energy, \"kWh\")\n",
    "    else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9fb6bf",
   "metadata": {},
   "source": [
    "# 12. Performance and Carbon Emissions\n",
    "\n",
    "Calculates and prints performance metrics for all prompts:\n",
    "\n",
    "- **Latency:** total, average, minimum, and maximum per query.  \n",
    "- **Carbon emissions and energy consumption:** for model loading, embeddings, and inference.  \n",
    "- Computes total and per-query values for both CO2 emissions and energy usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bae5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PERFORMANCE METRICS\")\n",
    "\n",
    "avg_latency = total_latency / len(prompts)\n",
    "min_latency = min(latencies)\n",
    "max_latency = max(latencies)\n",
    "\n",
    "print(f\"Total latency:       {total_latency:.3f}s\")\n",
    "print(f\"Average per query:   {avg_latency:.3f}s\")\n",
    "print(f\"Min latency:         {min_latency:.3f}s\")\n",
    "print(f\"Max latency:         {max_latency:.3f}s\")\n",
    "\n",
    "print(\"\\n CARBON EMISSIONS & ENERGY\")\n",
    "total_emissions = emissions_loading + emissions_embeddings + emissions_inference\n",
    "\n",
    "try:\n",
    "    energy_loading = tracker_loading._total_energy.kWh\n",
    "    energy_embeddings = tracker_embeddings._total_energy.kWh\n",
    "    energy_inference_val = energy_inference\n",
    "    total_energy = energy_loading + energy_embeddings + energy_inference_val\n",
    "\n",
    "    print(f\"Model Loading: {emissions_loading:.6f} kg CO2  |  {energy_loading:.6f} kWh\")\n",
    "    print(\n",
    "        f\"Embeddings:    {emissions_embeddings:.6f} kg CO2  |  {energy_embeddings:.6f} kWh\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Inference:     {emissions_inference:.6f} kg CO2  |  {energy_inference_val:.6f} kWh\"\n",
    "    )\n",
    "    print(f\"TOTAL:         {total_emissions:.6f} kg CO2  |  {total_energy:.6f} kWh\")\n",
    "    print(\n",
    "        f\"Per query:     {emissions_inference / len(prompts):.6f} kg CO2  |  {energy_inference_val / len(prompts):.6f} kWh\"\n",
    "    )\n",
    "except:\n",
    "    print(f\"Model Loading: {emissions_loading:.6f} kg CO2\")\n",
    "    print(f\"Embeddings:    {emissions_embeddings:.6f} kg CO2\")\n",
    "    print(f\"Inference:     {emissions_inference:.6f} kg CO2\")\n",
    "    print(f\"TOTAL:         {total_emissions:.6f} kg CO2\")\n",
    "    print(f\"Per query:     {emissions_inference / len(prompts):.6f} kg CO2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
