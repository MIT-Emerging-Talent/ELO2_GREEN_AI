{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local RAG with Mistral 7B & CodeCarbon Evaluation\n",
    "\n",
    "This notebook runs a complete Retrieval-Augmented Generation (RAG) pipeline locally on your machine. It uses:\n",
    "\n",
    "- **`llama-index`**: To build the RAG pipeline.\n",
    "- **`llama-cpp-python`**: To run the quantized Mistral 7B GGUF model.\n",
    "- **GPU Acceleration**: The model is configured to run on your NVIDIA GPU (`n_gpu_layers=-1`).\n",
    "- **`codecarbon`**: To measure the energy consumption and CO2 emissions for each query you make in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installations\n",
    "\n",
    "This cell installs all the required Python libraries. \n",
    "\n",
    "**Note:** This assumes you have already installed `llama-cpp-python` with the correct CUDA (GPU) support. If not, you may need to run this command in your terminal first:\n",
    "\n",
    "`$env:CMAKE_ARGS = \"-DGGML_CUDA=on\"`\n",
    "`pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: codecarbon in c:\\users\\engam\\anaconda3\\lib\\site-packages (3.0.6)\n",
      "Requirement already satisfied: arrow in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (1.2.3)\n",
      "Requirement already satisfied: click in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (8.2.1)\n",
      "Requirement already satisfied: fief-client[cli] in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (0.20.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (2.2.2)\n",
      "Requirement already satisfied: prometheus_client in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (0.14.1)\n",
      "Requirement already satisfied: psutil>=6.0.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (7.1.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (9.0.0)\n",
      "Requirement already satisfied: pydantic in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (2.12.3)\n",
      "Requirement already satisfied: nvidia-ml-py in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (13.580.82)\n",
      "Requirement already satisfied: rapidfuzz in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (3.14.1)\n",
      "Requirement already satisfied: requests in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (2.32.5)\n",
      "Requirement already satisfied: questionary in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (2.1.1)\n",
      "Requirement already satisfied: rich in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (13.7.1)\n",
      "Requirement already satisfied: typer in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from arrow->codecarbon) (2.9.0.post0)\n",
      "Requirement already satisfied: colorama in c:\\users\\engam\\anaconda3\\lib\\site-packages (from click->codecarbon) (0.4.6)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.21.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from fief-client[cli]->codecarbon) (0.27.0)\n",
      "Requirement already satisfied: jwcrypto<2.0.0,>=1.4 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from fief-client[cli]->codecarbon) (1.5.6)\n",
      "Requirement already satisfied: yaspin in c:\\users\\engam\\anaconda3\\lib\\site-packages (from fief-client[cli]->codecarbon) (3.2.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pandas->codecarbon) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pandas->codecarbon) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pandas->codecarbon) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pydantic->codecarbon) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pydantic->codecarbon) (2.41.4)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pydantic->codecarbon) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pydantic->codecarbon) (0.4.2)\n",
      "Requirement already satisfied: prompt_toolkit<4.0,>=2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from questionary->codecarbon) (3.0.43)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests->codecarbon) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests->codecarbon) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests->codecarbon) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests->codecarbon) (2025.4.26)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from rich->codecarbon) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from rich->codecarbon) (2.15.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from typer->codecarbon) (1.5.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (0.14.0)\n",
      "Requirement already satisfied: cryptography>=3.4 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (43.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->codecarbon) (0.1.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\engam\\anaconda3\\lib\\site-packages (from prompt_toolkit<4.0,>=2.0->questionary->codecarbon) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.16.0)\n",
      "Requirement already satisfied: termcolor<4.0,>=3.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from yaspin->fief-client[cli]->codecarbon) (3.1.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\engam\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index\n",
    "!pip install llama-index-llms-llama-cpp\n",
    "!pip install llama-index-embeddings-huggingface\n",
    "!pip install sentence-transformers\n",
    "!pip install pypdf\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install codecarbon\n",
    "!pip install langchain-community # Dependency for Ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Configuration\n",
    "\n",
    "Here we import all necessary modules and set up the file paths for your model and data. \n",
    "\n",
    "**Please double-check that `MODEL_PATH` and `DATA_PATH` are correct for your system.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\engam\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'validate_default' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'validate_default' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\engam\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Configuration loaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from codecarbon import OfflineEmissionsTracker\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "\n",
    "# Set the path to your downloaded GGUF model\n",
    "# IMPORTANT: Use a raw string (r\"...\") for Windows paths\n",
    "MODEL_PATH = r\"D:\\Mistral7B\\mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
    "\n",
    "# Set the path to your data (PDFs, .txt, etc.)\n",
    "DATA_PATH = r\"D:\\Mistral7B\\data\"\n",
    "\n",
    "# Set your country's 3-letter ISO code for CodeCarbon\n",
    "# Find your code: https://en.wikipedia.org/wiki/List_of_ISO_3166-1_alpha-3_codes\n",
    "YOUR_COUNTRY_ISO_CODE = \"EGY\"\n",
    "\n",
    "print(\"Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Models and Index\n",
    "\n",
    "This cell loads the Mistral 7B model into your GPU VRAM, loads the embedding model, and then scans your `DATA_PATH` to build the searchable RAG index. This step may take a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3050 Laptop GPU, compute capability 8.6, VMM: yes\n",
      "llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3050 Laptop GPU) - 3302 MiB free\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from D:\\Mistral7B\\mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: control token:      2 '</s>' is not marked as EOG\n",
      "llm_load_vocab: control token:      1 '<s>' is not marked as EOG\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading output layer to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CUDA0 model buffer size =  4095.05 MiB\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 3904\n",
      "llama_new_context_with_model: n_ctx_per_seq = 3904\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 1000000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (3904) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   488.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  488.00 MiB, K (f16):  244.00 MiB, V (f16):  244.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   283.63 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    15.63 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | FORCE_MMQ = 1 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading and indexing documents...\n",
      "Loaded 1 document(s).\n",
      "Indexing complete.\n",
      "Query engine is ready (with custom anti-leak prompt).\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing models...\")\n",
    "\n",
    "# Load the local LLM (Mistral 7B) with GPU offloading\n",
    "llm = LlamaCPP(\n",
    "    model_path=MODEL_PATH,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=512,\n",
    "    context_window=3900,\n",
    "    generate_kwargs={},\n",
    "    # Set n_gpu_layers to -1 to offload all layers to GPU\n",
    "    model_kwargs={\"n_gpu_layers\": -1},\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Load the local Embedding Model\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# Set up LlamaIndex global settings to use our local models\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "print(\"\\nLoading and indexing documents...\")\n",
    "documents = SimpleDirectoryReader(DATA_PATH).load_data()\n",
    "print(f\"Loaded {len(documents)} document(s).\")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "print(\"Indexing complete.\")\n",
    "\n",
    "\n",
    "# --- ADD THIS SECTION ---\n",
    "# Define the new, strict prompt template\n",
    "qa_template_str = (\n",
    "    \"You are an expert assistant. Answer the user's question based *only* on the \"\n",
    "    \"provided context.\\n\\n\"\n",
    "    \"Strict Rules:\\n\"\n",
    "    \"1. Do not mention the context, the source document, or 'the text'.\\n\"\n",
    "    \"2. Answer the question directly, as if you knew the information yourself.\\n\"\n",
    "    \"3. If the answer is not in the context, state that you do not have enough \"\n",
    "    \"information to answer.\\n\\n\"\n",
    "    \"--- Context ---\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"--- Question ---\\n\"\n",
    "    \"{query_str}\\n\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "qa_template = PromptTemplate(qa_template_str)\n",
    "# --- END SECTION ---\n",
    "\n",
    "\n",
    "# --- MODIFY THIS LINE ---\n",
    "# Create the query engine, passing in the new template\n",
    "query_engine = index.as_query_engine(\n",
    "    streaming=True,\n",
    "    text_qa_template=qa_template,  # <-- Pass the template here\n",
    ")\n",
    "# --- END MODIFICATION ---\n",
    "\n",
    "print(\"Query engine is ready (with custom anti-leak prompt).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Start Interactive RAG + Carbon Tracking\n",
    "\n",
    "Run this cell to start the interactive chat. You can ask questions about your documents.\n",
    "\n",
    "- Type your question and press Enter.\n",
    "- The model will stream its answer.\n",
    "- After the answer, `codecarbon` will print the latency and environmental cost for that specific query.\n",
    "- Type `exit` to stop the loop and see the total emissions for the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing CodeCarbon tracker for country: EGY\n",
      "\n",
      "--- Query Engine Ready (Tracking Emissions) ---\n",
      "Type 'exit' to quit.\n",
      "\n",
      "Assistant: "
     ]
    }
   ],
   "source": [
    "print(f\"\\nInitializing CodeCarbon tracker for country: {YOUR_COUNTRY_ISO_CODE}\")\n",
    "tracker = OfflineEmissionsTracker(country_iso_code=YOUR_COUNTRY_ISO_CODE)\n",
    "tracker.start()\n",
    "\n",
    "print(\"\\n--- Query Engine Ready (Tracking Emissions) ---\")\n",
    "print(\"Type 'exit' to quit.\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        query = input(\"Ask a question about your documents: \")\n",
    "        if query.lower() == \"exit\":\n",
    "            break\n",
    "\n",
    "        # --- Start tracking just for the query ---\n",
    "        tracker.start_task(\"RAG Query\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        response_stream = query_engine.query(query)\n",
    "\n",
    "        print(\"\\nAssistant: \", end=\"\")\n",
    "        # Stream the response to the console\n",
    "        response_stream.print_response_stream()\n",
    "\n",
    "        # --- Stop tracking and get emissions for this single query ---\n",
    "        end_time = time.time()\n",
    "        emissions_data = tracker.stop_task()\n",
    "\n",
    "        print(\"\\n\\n--- Query Metrics ---\")\n",
    "        print(f\"Latency: {end_time - start_time:.2f} seconds\")\n",
    "        print(f\"Emissions: {emissions_data.emissions * 1000:.6f} gCO2eq\")\n",
    "        print(f\"Energy: {emissions_data.energy_consumed * 1000:.6f} Wh\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "finally:\n",
    "    # This stops the main tracker and saves the total emissions.csv file\n",
    "    total_emissions_kg = tracker.stop()\n",
    "    print(\"\\n\\n--- Total Emissions Summary (Session) ---\")\n",
    "    # Access total energy from the tracker object itself\n",
    "    if tracker.final_emissions_data:\n",
    "        print(\n",
    "            f\"Total Energy Consumed: {tracker.final_emissions_data.energy_consumed * 1000:.4f} Wh\"\n",
    "        )\n",
    "    print(f\"Total CO2 Emitted: {total_emissions_kg * 1000:.4f} gCO2eq\")\n",
    "    print(\"Full report saved to 'emissions.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
