{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local RAG with Mistral 7B & CodeCarbon Evaluation\n",
    "\n",
    "This notebook runs a complete Retrieval-Augmented Generation (RAG) pipeline locally on your machine. It uses:\n",
    "\n",
    "- **`llama-index`**: To build the RAG pipeline.\n",
    "- **`llama-cpp-python`**: To run the quantized Mistral 7B GGUF model.\n",
    "- **GPU Acceleration**: The model is configured to run on your NVIDIA GPU (`n_gpu_layers=-1`).\n",
    "- **`codecarbon`**: To measure the energy consumption and CO2 emissions for each query you make in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installations\n",
    "\n",
    "This cell installs all the required Python libraries. \n",
    "\n",
    "**Note:** This assumes you have already installed `llama-cpp-python` with the correct CUDA (GPU) support. If not, you may need to run this command in your terminal first:\n",
    "\n",
    "`$env:CMAKE_ARGS = \"-DGGML_CUDA=on\"`\n",
    "`pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in c:\\users\\engam\\anaconda3\\lib\\site-packages (0.14.5)\n",
      "Requirement already satisfied: llama-index-cli<0.6,>=0.5.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index) (0.5.3)\n",
      "Requirement already satisfied: llama-index-core<0.15.0,>=0.14.5 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index) (0.14.5)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.6,>=0.5.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index) (0.5.1)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index) (0.9.4)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.7,>=0.6.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index) (0.6.5)\n",
      "Requirement already satisfied: llama-index-readers-file<0.6,>=0.5.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index) (0.5.4)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index) (0.5.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (3.10.5)\n",
      "Requirement already satisfied: aiosqlite in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (2.2.0)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (2024.6.1)\n",
      "Requirement already satisfied: httpx in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (0.27.0)\n",
      "Requirement already satisfied: llama-index-workflows<3,>=2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (2.8.3)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (10.4.0)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (4.5.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (2.12.3)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (2.32.5)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.5->llama-index) (2.0.34)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (0.12.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (4.15.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (1.14.1)\n",
      "Requirement already satisfied: openai>=1.1.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.109.1)\n",
      "Requirement already satisfied: llama-cloud==0.1.35 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.35)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-cloud==0.1.35->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.4.26)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (4.12.3)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.7.1)\n",
      "Requirement already satisfied: pandas<2.3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.2.2)\n",
      "Requirement already satisfied: pypdf<7,>=5.1.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (6.1.2)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.54)\n",
      "Requirement already satisfied: click in c:\\users\\engam\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\engam\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index) (2024.9.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.5->llama-index) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.5->llama-index) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.5->llama-index) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.5->llama-index) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.5->llama-index) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.5->llama-index) (1.11.0)\n",
      "Requirement already satisfied: griffe in c:\\users\\engam\\anaconda3\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.5->llama-index) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.5->llama-index) (3.1.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.5)\n",
      "Requirement already satisfied: anyio in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.15.0,>=0.14.5->llama-index) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.15.0,>=0.14.5->llama-index) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.15.0,>=0.14.5->llama-index) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.15.0,>=0.14.5->llama-index) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.15.0,>=0.14.5->llama-index) (0.14.0)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-workflows<3,>=2->llama-index-core<0.15.0,>=0.14.5->llama-index) (0.4.2)\n",
      "Requirement already satisfied: llama-cloud-services>=0.6.54 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.54)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.5->llama-index) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.5->llama-index) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.5->llama-index) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.15.0,>=0.14.5->llama-index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.15.0,>=0.14.5->llama-index) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.5->llama-index) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\engam\\anaconda3\\lib\\site-packages (from tqdm<5,>=4.66.1->llama-index-core<0.15.0,>=0.14.5->llama-index) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.15.0,>=0.14.5->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from dataclasses-json->llama-index-core<0.15.0,>=0.14.5->llama-index) (3.26.1)\n",
      "Requirement already satisfied: python-dotenv<2,>=1.0.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-cloud-services>=0.6.54->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.1.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.15.0,>=0.14.5->llama-index) (24.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.5->llama-index) (2.1.3)\n",
      "Requirement already satisfied: llama-index-llms-llama-cpp in c:\\users\\engam\\anaconda3\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: llama-cpp-python<0.4,>=0.3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-llms-llama-cpp) (0.3.4)\n",
      "Requirement already satisfied: llama-index-core<0.15,>=0.13.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-llms-llama-cpp) (0.14.5)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-cpp-python<0.4,>=0.3.0->llama-index-llms-llama-cpp) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-cpp-python<0.4,>=0.3.0->llama-index-llms-llama-cpp) (1.26.4)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-cpp-python<0.4,>=0.3.0->llama-index-llms-llama-cpp) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-cpp-python<0.4,>=0.3.0->llama-index-llms-llama-cpp) (3.1.4)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.10.5)\n",
      "Requirement already satisfied: aiosqlite in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.2.0)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2024.6.1)\n",
      "Requirement already satisfied: httpx in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.27.0)\n",
      "Requirement already satisfied: llama-index-workflows<3,>=2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.8.3)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.3)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.9.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (10.4.0)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (4.5.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.12.3)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.32.5)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.0.34)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.12.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (4.66.5)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.14.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.11.0)\n",
      "Requirement already satisfied: griffe in c:\\users\\engam\\anaconda3\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python<0.4,>=0.3.0->llama-index-llms-llama-cpp) (2.1.3)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-workflows<3,>=2->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.4.2)\n",
      "Requirement already satisfied: click in c:\\users\\engam\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\engam\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2024.9.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2025.4.26)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\engam\\anaconda3\\lib\\site-packages (from tqdm<5,>=4.66.1->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.26.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.14.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (24.1)\n",
      "Requirement already satisfied: llama-index-embeddings-huggingface in c:\\users\\engam\\anaconda3\\lib\\site-packages (0.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.33.0)\n",
      "Requirement already satisfied: llama-index-core<0.15,>=0.13.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-embeddings-huggingface) (0.14.5)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-embeddings-huggingface) (4.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\engam\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\engam\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.15.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\engam\\anaconda3\\lib\\site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.10.5)\n",
      "Requirement already satisfied: aiosqlite in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.2.0)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.2.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.27.0)\n",
      "Requirement already satisfied: llama-index-workflows<3,>=2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.8.3)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.3)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (10.4.0)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (4.5.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.12.3)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.0.34)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.12.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.14.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (4.52.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.7.1+cu118)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.13.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.11.0)\n",
      "Requirement already satisfied: griffe in c:\\users\\engam\\anaconda3\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.1.4)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-workflows<3,>=2->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.4.2)\n",
      "Requirement already satisfied: click in c:\\users\\engam\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\engam\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2024.9.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2025.4.26)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.0.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\engam\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.4.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.5.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.26.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.14.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.1.3)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\engam\\anaconda3\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.52.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.7.1+cu118)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.33.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\engam\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\engam\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\engam\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\engam\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\engam\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n",
      "Requirement already satisfied: pypdf in c:\\users\\engam\\anaconda3\\lib\\site-packages (6.1.2)\n",
      "Requirement already satisfied: torch in c:\\users\\engam\\anaconda3\\lib\\site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\engam\\anaconda3\\lib\\site-packages (0.22.1+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\engam\\anaconda3\\lib\\site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\engam\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\engam\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\engam\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\engam\\anaconda3\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\engam\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: codecarbon in c:\\users\\engam\\anaconda3\\lib\\site-packages (3.0.6)\n",
      "Requirement already satisfied: arrow in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (1.2.3)\n",
      "Requirement already satisfied: click in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (8.2.1)\n",
      "Requirement already satisfied: fief-client[cli] in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (0.20.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (2.2.2)\n",
      "Requirement already satisfied: prometheus_client in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (0.14.1)\n",
      "Requirement already satisfied: psutil>=6.0.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (7.1.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (9.0.0)\n",
      "Requirement already satisfied: pydantic in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (2.12.3)\n",
      "Requirement already satisfied: nvidia-ml-py in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (13.580.82)\n",
      "Requirement already satisfied: rapidfuzz in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (3.14.1)\n",
      "Requirement already satisfied: requests in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (2.32.5)\n",
      "Requirement already satisfied: questionary in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (2.1.1)\n",
      "Requirement already satisfied: rich in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (13.7.1)\n",
      "Requirement already satisfied: typer in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from arrow->codecarbon) (2.9.0.post0)\n",
      "Requirement already satisfied: colorama in c:\\users\\engam\\anaconda3\\lib\\site-packages (from click->codecarbon) (0.4.6)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.21.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from fief-client[cli]->codecarbon) (0.27.0)\n",
      "Requirement already satisfied: jwcrypto<2.0.0,>=1.4 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from fief-client[cli]->codecarbon) (1.5.6)\n",
      "Requirement already satisfied: yaspin in c:\\users\\engam\\anaconda3\\lib\\site-packages (from fief-client[cli]->codecarbon) (3.2.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pandas->codecarbon) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pandas->codecarbon) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pandas->codecarbon) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pydantic->codecarbon) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pydantic->codecarbon) (2.41.4)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pydantic->codecarbon) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pydantic->codecarbon) (0.4.2)\n",
      "Requirement already satisfied: prompt_toolkit<4.0,>=2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from questionary->codecarbon) (3.0.43)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests->codecarbon) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests->codecarbon) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests->codecarbon) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests->codecarbon) (2025.4.26)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from rich->codecarbon) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from rich->codecarbon) (2.15.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from typer->codecarbon) (1.5.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (0.14.0)\n",
      "Requirement already satisfied: cryptography>=3.4 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (43.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->codecarbon) (0.1.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\engam\\anaconda3\\lib\\site-packages (from prompt_toolkit<4.0,>=2.0->questionary->codecarbon) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.16.0)\n",
      "Requirement already satisfied: termcolor<4.0,>=3.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from yaspin->fief-client[cli]->codecarbon) (3.1.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\engam\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index\n",
    "!pip install llama-index-llms-llama-cpp\n",
    "!pip install llama-index-embeddings-huggingface\n",
    "!pip install sentence-transformers\n",
    "!pip install pypdf\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install codecarbon\n",
    "!pip install langchain-community # Dependency for Ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Configuration\n",
    "\n",
    "Here we import all necessary modules and set up the file paths for your model and data. \n",
    "\n",
    "**Please double-check that `MODEL_PATH` and `DATA_PATH` are correct for your system.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\engam\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'validate_default' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'validate_default' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\engam\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Configuration loaded.\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "import time\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from codecarbon import OfflineEmissionsTracker\n",
    "from llama_index.core import PromptTemplate\n",
    "import textwrap\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "\n",
    "# Set the path to your downloaded GGUF model\n",
    "# IMPORTANT: Use a raw string (r\"...\") for Windows paths\n",
    "# MODEL_PATH =r\"D:\\\\Mistral7B\\\\mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
    "\n",
    "# Set the path to your data (PDFs, .txt, etc.)\n",
    "DATA_PATH = r\"D:\\Mistral7B\\data\"\n",
    "\n",
    "# Set your country's 3-letter ISO code for CodeCarbon\n",
    "# Find your code: https://en.wikipedia.org/wiki/List_of_ISO_3166-1_alpha-3_codes\n",
    "YOUR_COUNTRY_ISO_CODE = \"EGY\"\n",
    "\n",
    "print(\"Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Models and Index\n",
    "\n",
    "This cell loads the Mistral 7B model into your GPU VRAM, loads the embedding model, and then scans your `DATA_PATH` to build the searchable RAG index. This step may take a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3050 Laptop GPU, compute capability 8.6, VMM: yes\n",
      "llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3050 Laptop GPU) - 3302 MiB free\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 201 tensors from D:\\Mistral7B\\tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = tinyllama_tinyllama-1.1b-chat-v1.0\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 22\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5632\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,61249]   = [\" t\", \"e r\", \"i n\", \" a\", \"e n...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   45 tensors\n",
      "llama_model_loader: - type q4_K:  135 tensors\n",
      "llama_model_loader: - type q6_K:   21 tensors\n",
      "llm_load_vocab: control token:      2 '</s>' is not marked as EOG\n",
      "llm_load_vocab: control token:      1 '<s>' is not marked as EOG\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_layer          = 22\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 4\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 256\n",
      "llm_load_print_meta: n_embd_v_gqa     = 256\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 5632\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 1B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 1.10 B\n",
      "llm_load_print_meta: model size       = 636.18 MiB (4.85 BPW) \n",
      "llm_load_print_meta: general.name     = tinyllama_tinyllama-1.1b-chat-v1.0\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors: offloading 22 repeating layers to GPU\n",
      "llm_load_tensors: offloading output layer to GPU\n",
      "llm_load_tensors: offloaded 23/23 layers to GPU\n",
      "llm_load_tensors:        CUDA0 model buffer size =   601.02 MiB\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =    35.16 MiB\n",
      "....................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 3904\n",
      "llama_new_context_with_model: n_ctx_per_seq = 3904\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 10000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_pre_seq (3904) > n_ctx_train (2048) -- possible training context overflow\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    83.88 MiB\n",
      "llama_new_context_with_model: KV self size  =   83.88 MiB, K (f16):   41.94 MiB, V (f16):   41.94 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   267.63 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    11.63 MiB\n",
      "llama_new_context_with_model: graph nodes  = 710\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | FORCE_MMQ = 1 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'tinyllama_tinyllama-1.1b-chat-v1.0', 'general.architecture': 'llama', 'llama.context_length': '2048', 'llama.rope.dimension_count': '64', 'llama.embedding_length': '2048', 'llama.block_count': '22', 'llama.feed_forward_length': '5632', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '4', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.chat_template': \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% for message in messages %}\n",
      "{% if message['role'] == 'user' %}\n",
      "{{ '<|user|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'system' %}\n",
      "{{ '<|system|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'assistant' %}\n",
      "{{ '<|assistant|>\n",
      "'  + message['content'] + eos_token }}\n",
      "{% endif %}\n",
      "{% if loop.last and add_generation_prompt %}\n",
      "{{ '<|assistant|>' }}\n",
      "{% endif %}\n",
      "{% endfor %}\n",
      "Using chat eos_token: </s>\n",
      "Using chat bos_token: <s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading and indexing documents...\n",
      "Loaded 1 document(s).\n",
      "Indexing complete.\n",
      "Query engine is ready (with custom anti-leak prompt).\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing models...\")\n",
    "MODEL_PATH = r\"D:\\Mistral7B\\tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n",
    "\n",
    "# Load the local LLM (Mistral 7B) with GPU offloading\n",
    "llm = LlamaCPP(\n",
    "    model_path=MODEL_PATH,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=1024,\n",
    "    context_window=3900,\n",
    "    generate_kwargs={},\n",
    "    # Set n_gpu_layers to -1 to offload all layers to GPU\n",
    "    model_kwargs={\"n_gpu_layers\": -1},\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Load the local Embedding Model\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# Set up LlamaIndex global settings to use our local models\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "print(\"\\nLoading and indexing documents...\")\n",
    "documents = SimpleDirectoryReader(DATA_PATH).load_data()\n",
    "print(f\"Loaded {len(documents)} document(s).\")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "print(\"Indexing complete.\")\n",
    "\n",
    "\n",
    "# --- ADD THIS SECTION ---\n",
    "# Define the new, strict prompt template\n",
    "qa_template_str = (\n",
    "    \"You are an expert assistant. Answer the user's question based *only* on the \"\n",
    "    \"provided context.\\n\\n\"\n",
    "    \"Strict Rules:\\n\"\n",
    "    \"1. Do not mention the context, the source document, or 'the text'.\\n\"\n",
    "    \"2. Answer the question directly, as if you knew the information yourself.\\n\"\n",
    "    \"3. If the answer is not in the context, state that you do not have enough \"\n",
    "    \"information to answer.\\n\\n\"\n",
    "    \"--- Context ---\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"--- Question ---\\n\"\n",
    "    \"{query_str}\\n\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "qa_template_str2 = (\n",
    "    \"You are an expert assistant. Answer the user's question based *only* on the \"\n",
    "    \"provided context.\\n\\n\"\n",
    "    \"Strict Rules:\\n\"\n",
    "    \"1. Use the context as inspiration, but do not copy it.'.\\n\"\n",
    "    \"2. Expand or interpret the ideas creatively, producing a short paragraph.\\n\"\n",
    "    \"3. Keep the tone natural and imaginative, as if writing your own reflection\"\n",
    "    \"information to answer.\\n\\n\"\n",
    "    \"--- Context ---\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"--- Question ---\\n\"\n",
    "    \"{query_str}\\n\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "qa_template_str3 = (\n",
    "    \"You are an expert assistant. Answer the user's question based *only* on the \"\n",
    "    \"provided context.\\n\\n\"\n",
    "    \"Strict Rules:\\n\"\n",
    "    \"1. Rewrite the given information in your own words.'.\\n\"\n",
    "    \"2. Preserve meaning and tone without copying phrases directly..\\n\"\n",
    "    \"3. The output should read naturally like an original paragraph.\"\n",
    "    \"information to answer.\\n\\n\"\n",
    "    \"--- Context ---\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"--- Question ---\\n\"\n",
    "    \"{query_str}\\n\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "qa_template = PromptTemplate(qa_template_str2)\n",
    "# --- END SECTION ---\n",
    "\n",
    "\n",
    "# --- MODIFY THIS LINE ---\n",
    "# Create the query engine, passing in the new template\n",
    "query_engine = index.as_query_engine(\n",
    "    streaming=True,\n",
    "    text_qa_template=qa_template,  # <-- Pass the template here\n",
    "    similarity_top_k=3,\n",
    "    include_source_nodes=True,\n",
    ")\n",
    "# --- END MODIFICATION ---\n",
    "\n",
    "print(\"Query engine is ready (with custom anti-leak prompt).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Start Interactive RAG + Carbon Tracking\n",
    "\n",
    "Run this cell to start the interactive chat. You can ask questions about your documents.\n",
    "\n",
    "- Type your question and press Enter.\n",
    "- The model will stream its answer.\n",
    "- After the answer, `codecarbon` will print the latency and environmental cost for that specific query.\n",
    "- Type `exit` to stop the loop and see the total emissions for the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing CodeCarbon tracker for country: EGY\n",
      "\n",
      "--- Query Engine Ready (Recursive Editing: 3 cycles) ---\n",
      "Type 'exit' to quit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1173.22 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1925 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /  1023 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10866.38 ms /  2948 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 3044 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Your Question:  Imagine being one of the people in Mission Control. How would you feel while watching the landing?\n",
      "\n",
      "--- Initial Draft (from RAG) ---\n",
      " I would feel a mix of excitement, relief, and sadness. I would be excited to\n",
      "see the spacecraft touch down safely, but I would also be sad to see the mission\n",
      "come to an end. The thought of the people who worked so hard to make this\n",
      "mission a success would be a constant source of pride and motivation.  ---\n",
      "Context --- page_label: 1 file_path: D:\\Mistraal7B\\data\\Apolllo.pdf  As  the\n",
      "descent  began,  Armstrong  and  Aldrin  found  themselve  passing  landmarks\n",
      "on  the   surface   two   or   three   seconds   early,   and   reported   that\n",
      "they   weren't     long;   they   would   land   mile   south   of   their\n",
      "target   point.   Eagle   was   traveling   too   many   of   the   Moons\n",
      "crater   Moons   ,     and   only   ,         and   only the       Moons   Moons\n",
      "LagM   Landing    900      90        900  90  90  90Mos  9   900.   90 ions.\n",
      "9000Mo's. 900:s. 0,0s:s: *  *sense:Mosts to your:s to yourn00s and *notion to\n",
      "theat theirine to nots,0nions. sustment. nots.9s. *9s,sions,s9s.\n",
      "*9s,00s,s.0sine.0s.9s. 00: ands,s,00s,s,s,s,s,n,s,s,0s,s,0s,s,s,s\n",
      "ands,s,saint,s, andans,s,d, anddines,sence,s,sink,s,s,s, andsouts:siding:\n",
      "ands:ment'informations,s.sions.s.sentence.sence,sence,s,sence, and onwardsinfor\n",
      "mationsions,desions,designions,sentence&#sencepenionsenceumsencequest\n",
      "ions,questions,sentence:andandpenencethesumsideficulenceurutorsentinformati\n",
      "onsentinationunderward\\/wordsensepenficanficenceinformationinformationsward\n",
      "origninformationsionsponence\\/andssferorinformationspeninformationsumsphere\n",
      "nceutorinformationsurespheranorutorsignutororpenutorpenutorutorficencepenentu\n",
      "torutorquestionssinformations&#informationsinformationinformation:Ms:Re:Rei\n",
      "nformations\\/informationinformationorigns\\/or\\/orwords\\/or\\/information\\/infor\n",
      "mationinformationinformationinformationoralternsand\\/words\\/wordsand\\/and\n",
      "\\/orand\\.information\\/informationsinformationinformationor\\.or\\/or\\.or\\/info\n",
      "rmationinformations\\musts.s\\s\\m\\s\\sent\\/information\\/or\\/or\\sent\\/or\\.Ms\\.oro\n",
      "rs\\or\n",
      "ors\\sents.Msor\\or.~and\\or\\.and\\.prov\\prov\\provands:\\prov:\\prov\\.wordss.s.\n",
      "but\\buts.buts.but.but.buts.but.liter.M.butsbuts\\s\\M.Ms. and\\ Ms\\s\\s\\s.Ms. mquot\n",
      "the\\ s\\  the\\ Ms\\ and\\ ands. andsand\\ ands\\and\\ and\\ and and M M\n",
      "\n",
      "--- Refinement Cycle 1/3 ---\n",
      "--- Critic is thinking... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1173.22 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  3044 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   858 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9046.62 ms /  3902 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 2061 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cr.\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "Cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr:\n",
      "\n",
      "\n",
      "\n",
      "cr\n",
      "\n",
      "\n",
      "cr\n",
      "\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr.\n",
      "cr\n",
      "cr\n",
      "crcrcrcr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr.\n",
      "dra.\n",
      "cr.\n",
      "cr.\n",
      "cr.\n",
      "cr,the,dra,dradra,the,\n",
      "the, crs\n",
      "the,\n",
      "the\n",
      "the\n",
      "the,thedra\n",
      "the\n",
      "theecr\n",
      "the crs\n",
      "thedspes\n",
      "the:\n",
      "thee, anddra.\n",
      "cr crs.crdradradradradracr?the?cr:cr:cr:the:thee-the crscrcrgsgs\n",
      "the\n",
      "the-the\n",
      "cr.\n",
      "\n",
      "the.\n",
      "the the the\n",
      "the, thescr.the,the:the.cr.cr.crscrsedcr.cr.\n",
      "cr.\n",
      "cr.cr.cr.\n",
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "cr,the,\n",
      "the\n",
      "the crscr.cr.the\n",
      "the\n",
      "cr.cr.the\n",
      "cr.\n",
      "cr.\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "the\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr.cr.cr.cr.cr.cr.cr.cr.cr.cr.cr.cr.cr.cral.cralalideal\n",
      "cr.\n",
      "cride\n",
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "crdraws\n",
      "\n",
      "\n",
      "the\n",
      "the\n",
      "cr\n",
      "cr.\n",
      "cr\n",
      "cr\n",
      "cr.\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "\n",
      "cr\n",
      "\n",
      "cr\n",
      "\n",
      "\n",
      "\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "\n",
      "cr\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "thealtheu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "theo\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "to\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "the\n",
      "\n",
      "\n",
      "the\n",
      "\n",
      "the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "F\n",
      "F\n",
      "\n",
      "F\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Refiner is working... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1173.22 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  2061 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /  1023 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9688.05 ms /  3084 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 2989 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Intermediate Refined Draft (Cycle 1) ---\n",
      " The original draft was a high-quality piece of writing that provided a detailed\n",
      "explanation of the mission's objectives, the team's roles, and the critical\n",
      "components of the mission. The draft was well-structured, with a clear\n",
      "introduction, a well-written body, and a strong conclusion. The use of vivid and\n",
      "descriptive language helped to create a vivid picture of the mission's\n",
      "environment, and the team's interactions with the spacecraft. The draft also\n",
      "included a clear and concise explanation of the mission's objectives, the team's\n",
      "roles, and the critical components. The draft also included a well-structured\n",
      "introduction, a well-written body, and a strong conclusion, which helped to\n",
      "provide a well-rational and well-structured. The drafted.  The draft. The draft.\n",
      "The space. The draft.  The draft.  The draft, the draft, the draft. The drafted.\n",
      "The draft. The team. The team. The team. The team. The The team. The team. The\n",
      "team. The team. The team. The team, the team, and, the team, the team, The team,\n",
      "the team, the team, the team, The, The, The, The, The, The, The, The, The, The,\n",
      "The, The, The, The, The, The, The, The, The, The, The, The, The, The, The, The,\n",
      "The, The, The, The, The,suggout, The, The, and, The, The, The, and, and, The,\n",
      "The, The, The, The,t,s, The, The, The, The,t,s, The, The,d, The, The,d,s,t, The,\n",
      "The, The,t, The, The,t,s,s, The, The,s,n, The,nate, The,n,n,ding,or,n,n, The,\n",
      "the,nions,because,becauseledled,because,because,with,thewith\n",
      ",s,theums,and,the,and,mentmentandandandmentsand\n",
      "mentsorandspecifics.s,thekensionsums,your,andions,orders,person,org\n",
      "ed,underification,underences,hadtenled<hadorunderastersignsurd.<you\n",
      "rmenterorasterbingming,speasterbingbingbing,butasteraster,oraster,andmentsa\n",
      "ndplemiced or orasterutororastedastedormwardsignsaster,underastedkedkedcedmi\n",
      "ngmingcedledkedkedpping,and,knownledclarand andclar,thewritten ordracedandandd\n",
      "uzingsignsigns,literment,butdingming,Refbedorplementorword,refor,orbedswordund\n",
      "erbed,litertedwritten.MicedMandrefbedbedbingfullyRefkedRefignple andrefplem\n",
      "ingpleponming,refaster,refasted,reforastmingming,reforor,refref,ref,ref,ref,ref\n",
      ",refandandandandref,refferferpleminggingrefrefpenrefrefsrefbingref,ref,ref,re\n",
      "f,ref,ref,theRef,ref,refsref.ref\n",
      "refsRefcibleRefsref:Refsppingref.Refsrefref,ref,Refs Ref,ref,refastedref,ref.r\n",
      "ef.ref,ref,ref.ref.ref.ref.ref:refdrableandref.Ref.RefsRefsandandandrefs.Refsref\n",
      ",ref,ref,ref,ref,ref,ref,ref,ref,ref,refref,ref:refs\\refs.refRefsRefsrefrefrefs\\\n",
      "refs\\ref,refsRefsref,ref,ref,ref,ref,ref,ref,refRefsRefsref to therefrefrefrefre\n",
      "frefrefRefsRefsrefRefRefRefsRefsRefRefRefRefsRefsRefRefsrefsRefsrefsssRefRefRefs\n",
      "refsrefrefrefrefrefrefsrefsrefssssrefs\n",
      "\n",
      "--- Refinement Cycle 2/3 ---\n",
      "--- Critic is thinking... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1173.22 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  2989 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   907 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9679.70 ms /  3896 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 2058 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cr.Cr.Cr.Cr.\n",
      "cr.\n",
      "cr.\n",
      "cr.Cr.\n",
      "crsedcr\n",
      "cr\n",
      "crcrsedcrcrcr.crcrcrcr.cr.cr:cr.cr.\n",
      "cr.\n",
      "cr.crcrcr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr:\n",
      "Cr\n",
      "\n",
      "cr\n",
      "\n",
      "\n",
      "\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr.\n",
      "cr\n",
      "cr\n",
      "crcrcr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "crs\n",
      "cr\n",
      "dra.\n",
      "dra.\n",
      "cr.\n",
      "cr.\n",
      "cr.cr,crdra,cr,dradra,the,\n",
      "cr,\n",
      "cr,\n",
      "the,the the\n",
      "the,thedraging\n",
      "the\n",
      "thecrcr.\n",
      "the crsthedthe.\n",
      "the the the anddrageddra cr crs.crdradradradradradra?the?cr:dra:cr:the:thee-the the crscrgsgs\n",
      "theferging\n",
      "crcible\n",
      "cr.\n",
      "the.\n",
      "the the the\n",
      "the,thespes.the:the:cr.cr.cr.crscrsedcr.crs\n",
      "cr:\n",
      "cr.cr.\n",
      "\n",
      "cr\n",
      "the\n",
      "\n",
      "cr.\n",
      "cr,cr,the\n",
      "the\n",
      "the crsthe crscr.cr\n",
      "cr.cr.cr.\n",
      "cr.\n",
      "cr.\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr.cr.cr.cr.\n",
      "cr.cr.cr.cr.cr.cr.cr.cr.cral.cralalaldraws\n",
      "cr.\n",
      "crdradra\n",
      "the\n",
      "cr\n",
      "the\n",
      "craldraws\n",
      "\n",
      "the\n",
      "the\n",
      "cr\n",
      "cr.\n",
      "cr\n",
      "cr.\n",
      "cr.\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "\n",
      "cr\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cr\n",
      "the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "theal\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "to\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "the\n",
      "\n",
      "\n",
      "\n",
      "the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "F\n",
      "\n",
      "F\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Refiner is working... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1173.22 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  2058 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /  1023 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9800.57 ms /  3081 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 2987 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Intermediate Refined Draft (Cycle 2) ---\n",
      " The original draft was a high-quality piece of writing that provided a detailed\n",
      "explanation of the mission's objective, the team's roles, and the critical\n",
      "components of the mission. The draft was well-structured, with a clear\n",
      "introduction, a well-written body, and a strong conclusion. The use of vivid and\n",
      "descriptive languaire is a vital component of the mission's environment,\n",
      "mission's objective, the team's interaction with the spacecraft. The drafted.\n",
      "The draft.  The space. The draft.  The draft. The team's roles, the clear and\n",
      "concise, with a well-structured, with a clear introduction, a well-structed, a\n",
      "well-structure, a well-struct, the drafted. The team's roles, with a well-\n",
      "structs, the drafted, the team's roles, the team's introduction, the team's\n",
      "conc's conc'drafted, the team's conc'd's, the team's conc'draft's conc's conc's\n",
      "conc'd'd's the conc'd'd'd'd, the conc'd'd'd'd'd'd'd'd'd'd'd'd'd'd'd'd'd'd'draft'\n",
      "d'd'd'd'd'd'draft'd'd'd'd'draft'd'd'd'd'd'd'd'd'd'draft'd'd'd'd'd'd'd'd'd'd'd'd'\n",
      "s'd's'd'd'd'd'd'dd'd'd'dd'd'd,d,d'd,d,d'ddddd'd'd'd,d'd'd,ddingd'd'dding'ddingd'\n",
      "d,d,d'd's,d,d,d,d,d,d,withs,withs,materials,dents\n",
      ",mentmentanddingdingmentssmentss.s.\n",
      "withs,or,clarencekenseedurdence:orudders orcherdingdingurdancedumsurdencesories\n",
      "mondswersignedleddrabledledsastersastersastedorastersurdencebledledsastersasters\n",
      ",orastersasters,orbertgartmingmingmingmingmingbingbingbedsbedbingbingbingbedbed\n",
      ",clarencebedswords,andumsheetbedced or or <reviewkens orastedottedorasterswardsi\n",
      "gnscedblecedgedledgedgedaster,clarcedledced,and,andledledowedclarence\n",
      "andders,anddraceded and and\n",
      "andclarencectionssence.andbleblebledingdingdingdingding,andmermentderswers\n",
      "orward,or,orbeds.clarenceclarification orlinedlinedorbeds.clarenceworddingdinga\n",
      "ceignively.orcedsedpecandmentsandorastersasters,clarence,written,written,refbi\n",
      "ngmentence orments ordersace,or,mustments ors orref,reviewing,ref,ref,andmentger\n",
      "srefference.ref,refcible,refference,ref,ref,ref,ref,ref,ref,and,theference.ref,\n",
      "refcescesferenceference,refference,refferencegerbedsference,refwerscible,reffere\n",
      "ncerefgergerbedsrefsrefficref,refferenceref.ref.ref,ref,refference.ref.andand.r\n",
      "efference.ref,andref,refes,refsrefdrarefdrarefdrarefrefref,ref,ref,ref,ref,ref,r\n",
      "efs,ref,ref,ref,ref,ref,ref,ref,refs,ref,ref,ref,ref,ref,ref,ref,ref,ref,ref,ref\n",
      ",ref,ref,ref,ref,ref,ref,refrefrefrefdradrarefdradrarefref refsref,ref,refrefref\n",
      "drarefrefref,ref,ref,ref,ref,ref,ref,ref,ref,ref,refs,ref,ref,ref,refrefrefrefre\n",
      "frefref,refrefss\n",
      "\n",
      "--- Refinement Cycle 3/3 ---\n",
      "--- Critic is thinking... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1173.22 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  2987 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   909 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10181.23 ms /  3896 tokens\n",
      "Llama.generate: 7 prefix-match hit, remaining 2062 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cr.Cr.Cr.Cr.Cr.cr.\n",
      "cr.\n",
      "cr.Cr.\n",
      "crsedcrsedcrsedcr.Crsedcrcrcr.cr.crcr.cr.cr.cr.cr.\n",
      "cr.\n",
      "cr.cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "crcrcr\n",
      "crcrcr\n",
      "cr\n",
      "cr\n",
      "cr:\n",
      "\n",
      "\n",
      "cr\n",
      "\n",
      "\n",
      "\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr.\n",
      "cr\n",
      "cr\n",
      "crcrcrcr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "crs\n",
      "cr\n",
      "cr,\n",
      "dra.\n",
      "cr.\n",
      "cr.\n",
      "cr.cr,crdra,cr,dradradra,\n",
      "the,\n",
      "dra,\n",
      "the,\n",
      "the\n",
      "the,the,draging\n",
      "the\n",
      "thecrcr\n",
      "the crs\n",
      "thedthe.\n",
      "the \n",
      "the anddrageddcr\n",
      "the crsdradradradradradradra?the?cr:dra:the:the:thee-the the thedrags\n",
      "the\n",
      "theferging\n",
      "cr.\n",
      "cr.\n",
      "the\n",
      "the the the\n",
      "the,the,the,the:the:cr.cr.cr.crscrsedcr.cr\n",
      "cr.\n",
      "cr.cr.cr.\n",
      "cr\n",
      "the\n",
      "\n",
      "the\n",
      "cr.cr,the:\n",
      "the\n",
      "the crscrcr.cr.the\n",
      "the\n",
      "cr.cr.\n",
      "cr.\n",
      "cr.\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr.cr.cr.cr.cr.\n",
      "cr.cr.cr.cr.cr.cr.cr.cr.cralal,thealidealdraw\n",
      "cr.\n",
      "cr\n",
      "the\n",
      "Cr.\n",
      "the\n",
      "the\n",
      "cr\n",
      "the\n",
      "\n",
      "the\n",
      "the\n",
      "Cr\n",
      "cr.\n",
      "cr\n",
      "cr\n",
      "cr.\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "\n",
      "cr\n",
      "cr\n",
      "\n",
      "cr\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "cr\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "the al\n",
      "\n",
      "\n",
      "\n",
      "the,\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "theo\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "to\n",
      "to\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "the\n",
      "\n",
      "\n",
      "the\n",
      "the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "the\n",
      "\n",
      "\n",
      "\n",
      "the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "F\n",
      "\n",
      "F\n",
      "\n",
      "F\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Refiner is working... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1173.22 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  2062 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /  1023 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10586.15 ms /  3085 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Intermediate Refined Draft (Cycle 3) ---\n",
      " The original draft was a high-quality piece of writing that provided a detailed\n",
      "explanation of the mission's objective, the team's roles, and the critical\n",
      "components of the mission. The draft was well-structured, with a clear\n",
      "introduction, a well-written body, and a strong conclusion. The use of vivid and\n",
      "descriptive languaire is a vital component of the mission's environment,\n",
      "mission's objective, the team's interaction with the spacecraft. The drafted.\n",
      "The space. The draft.  The team's roles, the clear and conc's conc's conc's\n",
      "conc's conc's conc's conc's conc's's's's conc's conc's's\n",
      "conc's'd's's's's'd's's's's's's'd's'd's'd's'd's's'd's's'd's'd'd's'd's'd's's of's\n",
      "ofs's ofd'd'd'd's'd'd's ofdraft'd'd'd'd'd's ofd'd'd'd's'd's's'draft's's'd'd'd'dr\n",
      "aft's'd'd'd'd'd'd'd'd'd'd'd'd'd'd'd'd'd'd'd'd's'd'd'd'd'd'd's's'd's'd'd'd'd'd'd'\n",
      "s'd'd'd'd'd'd'd'd'd'd'd'd'd's'd's'd'd'd'd'd'dding'ddingding's'd'dtoddingd,\n",
      "dmaterials, andments\n",
      "andandmentsorsandmings.and,andutor,orkens orber\n",
      "tmingming,<<materialsories,orences,orasteddrabledmenttoastersorastersaster.\n",
      "ment,or,<<orasters,orwordbingment,orastersasters\n",
      "orbedbingaster,or,andasterswordss andtenor orclarutorutor.orastedutor.knowin\n",
      "gmentsasted.andwordastedgedcedmingment,clarcedledcedor,and,clarowedclar.and.and.\n",
      "buttuntil.andandandclarencesence.\n",
      "andment<<speencepenence,andmentmentmentss,or,ors.materials orclarence.clare\n",
      "nce.or.clarence.refference.andref.orced.andvilferenceand.orastersacection,orace\n",
      ",refived.refference.refence orments ors ors.refference.refments.ref,refferencer\n",
      "efferenceandmentger,refsferenceref.refrefcible,refference.ref,refums,ref,ref,a\n",
      "nd,ref,the,ref.ref,refsference,refference,refaking,refficastedref,refference,ref\n",
      "ence,refgerref,ref,refs.ref,ref,refref.ref.ref.ref.ref,ref.ref.and.refference.re\n",
      "f,andref,ref,ref,s.ref,refrefref,ref,ref,ref,ref,ref,ref,ref,ref,ref,ref,ref,ref\n",
      ",ref,ref,ref,refs,ref,ref,ref,ref,ref,ref,ref,ref,ref,ref,ref,ref,ref,ref,ref,re\n",
      "f,ref,ref,refref.ref.to,ref.ref.ref,ref,ref.ref,ref,ref,ref,ref,ref,ref,ref'ref\n",
      ",ref,ref,ref,refs,ref,ref,ref,refrefrefrefref,refrefrefrefsrefsref,ref\n",
      "\n",
      "--- Final Refined Answer (After All Cycles) ---\n",
      " The original draft was a high-quality piece of writing that provided a detailed\n",
      "explanation of the mission's objective, the team's roles, and the critical\n",
      "components of the mission. The draft was well-structured, with a clear\n",
      "introduction, a well-written body, and a strong conclusion. The use of vivid and\n",
      "descriptive languaire is a vital component of the mission's environment,\n",
      "mission's objective, the team's interaction with the spacecraft. The drafted.\n",
      "The space. The draft.  The team's roles, the clear and conc's conc's conc's\n",
      "conc's conc's conc's conc's conc's's's's conc's conc's's\n",
      "conc's'd's's's's'd's's's's's's'd's'd's'd's'd's's'd's's'd's'd'd's'd's'd's's of's\n",
      "ofs's ofd'd'd'd's'd'd's ofdraft'd'd'd'd'd's ofd'd'd'd's'd's's'draft's's'd'd'd'dr\n",
      "aft's'd'd'd'd'd'd'd'd'd'd'd'd'd'd'd'd'd'd'd'd's'd'd'd'd'd'd's's'd's'd'd'd'd'd'd'\n",
      "s'd'd'd'd'd'd'd'd'd'd'd'd'd's'd's'd'd'd'd'd'dding'ddingding's'd'dtoddingd,\n",
      "dmaterials, andments\n",
      "andandmentsorsandmings.and,andutor,orkens orber\n",
      "tmingming,<<materialsories,orences,orasteddrabledmenttoastersorastersaster.\n",
      "ment,or,<<orasters,orwordbingment,orastersasters\n",
      "orbedbingaster,or,andasterswordss andtenor orclarutorutor.orastedutor.knowin\n",
      "gmentsasted.andwordastedgedcedmingment,clarcedledcedor,and,clarowedclar.and.and.\n",
      "buttuntil.andandandclarencesence.\n",
      "andment<<speencepenence,andmentmentmentss,or,ors.materials orclarence.clare\n",
      "nce.or.clarence.refference.andref.orced.andvilferenceand.orastersacection,orace\n",
      ",refived.refference.refence orments ors ors.refference.refments.ref,refferencer\n",
      "efferenceandmentger,refsferenceref.refrefcible,refference.ref,refums,ref,ref,a\n",
      "nd,ref,the,ref.ref,refsference,refference,refaking,refficastedref,refference,ref\n",
      "ence,refgerref,ref,refs.ref,ref,refref.ref.ref.ref.ref,ref.ref.and.refference.re\n",
      "f,andref,ref,ref,s.ref,refrefref,ref,ref,ref,ref,ref,ref,ref,ref,ref,ref,ref,ref\n",
      ",ref,ref,ref,refs,ref,ref,ref,ref,ref,ref,ref,ref,ref,ref,ref,ref,ref,ref,ref,re\n",
      "f,ref,ref,refref.ref.to,ref.ref.ref,ref,ref.ref,ref,ref,ref,ref,ref,ref,ref'ref\n",
      ",ref,ref,ref,refs,ref,ref,ref,refrefrefrefref,refrefrefrefsrefsref,ref\n",
      "\n",
      "\n",
      "--- Query Metrics (Full Loop) ---\n",
      "Latency: 69.96 seconds\n",
      "Emissions: 1.240913 gCO2eq\n",
      "Energy: 2.175873 Wh\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "--- Total Emissions Summary (Session) ---\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'OfflineEmissionsTracker' object has no attribute 'emissions_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 150\u001b[0m\n\u001b[0;32m    148\u001b[0m total_emissions_kg \u001b[38;5;241m=\u001b[39m tracker\u001b[38;5;241m.\u001b[39mstop()\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Total Emissions Summary (Session) ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracker\u001b[38;5;241m.\u001b[39memissions_data:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Energy Consumed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtracker\u001b[38;5;241m.\u001b[39memissions_data\u001b[38;5;241m.\u001b[39menergy_consumed\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1000\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Wh\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal CO2 Emitted: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_emissions_kg\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1000\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m gCO2eq\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'OfflineEmissionsTracker' object has no attribute 'emissions_data'"
     ]
    }
   ],
   "source": [
    "# --- Define the new prompts for the Critic and Refiner ---\n",
    "\n",
    "CRITIC_PROMPT = \"\"\"\n",
    "You are a 'Critic' AI. Your job is to provide specific, actionable feedback on a\n",
    "draft answer. You will be given the original question, the source context, and the\n",
    "draft answer.\n",
    "\n",
    "Evaluate the 'Draft Answer' based *only* on the 'Source Context' for two criteria:\n",
    "1.  **Faithfulness:** Is all information in the answer supported by the context?\n",
    "2.  **Relevance:** Does the answer directly address the question?\n",
    "\n",
    "Provide your feedback as a list of bullet points. If the draft is perfect,\n",
    "simply respond with \"The draft is perfect.\"\n",
    "\n",
    "---\n",
    "**Source Context:**\n",
    "{context}\n",
    "---\n",
    "**Original Question:**\n",
    "{question}\n",
    "---\n",
    "**Draft Answer:**\n",
    "{draft}\n",
    "---\n",
    "**Your Feedback:**\n",
    "\"\"\"\n",
    "\n",
    "REFINER_PROMPT = \"\"\"\n",
    "You are a 'Refiner' AI. Your job is to rewrite a draft answer based on\n",
    "feedback from a critic.\n",
    "\n",
    "Your goal is to produce a final, high-quality answer that directly\n",
    "answers the question, is fully supported by the context (which you haven't seen),\n",
    "and incorporates the critic's feedback.\n",
    "\n",
    "**Strict Rules:**\n",
    "- Do not mention the context, the feedback, or the draft.\n",
    "- Do not add any new information.\n",
    "- Just provide the final, improved answer.\n",
    "\n",
    "---\n",
    "**Original Draft:**\n",
    "{draft}\n",
    "---\n",
    "**Critic's Feedback:**\n",
    "{feedback}\n",
    "---\n",
    "**Your Refined Answer:**\n",
    "\"\"\"\n",
    "\n",
    "# --- Define the number of refinement cycles ---\n",
    "REFINEMENT_CYCLES = 3\n",
    "\n",
    "print(f\"\\nInitializing CodeCarbon tracker for country: {YOUR_COUNTRY_ISO_CODE}\")\n",
    "tracker = OfflineEmissionsTracker(country_iso_code=YOUR_COUNTRY_ISO_CODE)\n",
    "tracker.start()\n",
    "\n",
    "print(f\"\\n--- Query Engine Ready (Recursive Editing: {REFINEMENT_CYCLES} cycles) ---\")\n",
    "print(\"Type 'exit' to quit.\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        query = input(\"Ask a question about your documents: \")\n",
    "        if query.lower() == \"exit\":\n",
    "            break\n",
    "\n",
    "        # --- Start tracking the entire multi-step process ---\n",
    "        tracker.start_task(\"Recursive RAG Query\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # --- Step 1: Get the Draft (and Context) ---\n",
    "        response_stream = query_engine.query(query)\n",
    "\n",
    "        # Collect the streamed draft text\n",
    "        draft_text = \"\"\n",
    "        for chunk_text in response_stream.response_gen:\n",
    "            draft_text += chunk_text\n",
    "\n",
    "        # Extract the source context\n",
    "        context_str = \"\\n---\\n\".join(\n",
    "            [node.get_content() for node in response_stream.source_nodes]\n",
    "        )\n",
    "\n",
    "        print(f\"\\n\\nYour Question: {query}\")\n",
    "        print(\"\\n--- Initial Draft (from RAG) ---\")\n",
    "        print(textwrap.fill(draft_text, width=80))\n",
    "\n",
    "        # --- MODIFICATION: Start Recursive Loop ---\n",
    "\n",
    "        current_draft = draft_text  # Initialize the loop with the first draft\n",
    "\n",
    "        for i in range(REFINEMENT_CYCLES):\n",
    "            print(f\"\\n--- Refinement Cycle {i + 1}/{REFINEMENT_CYCLES} ---\")\n",
    "\n",
    "            # --- Step 2: Run the Critic ---\n",
    "            print(\"--- Critic is thinking... ---\")\n",
    "            critic_prompt = CRITIC_PROMPT.format(\n",
    "                context=context_str,\n",
    "                question=query,\n",
    "                draft=current_draft,  # Use the *current* draft\n",
    "            )\n",
    "\n",
    "            feedback_response = llm.complete(critic_prompt)\n",
    "            feedback_text = feedback_response.text\n",
    "            print(feedback_text)\n",
    "\n",
    "            # --- Step 3: Check for Convergence ---\n",
    "            if \"The draft is perfect\" in feedback_text:\n",
    "                print(\"--- Critic approved. Stopping refinement loop. ---\")\n",
    "                break  # Exit the for loop early\n",
    "\n",
    "            # --- Step 4: Run the Refiner ---\n",
    "            print(\"--- Refiner is working... ---\")\n",
    "            refiner_prompt = REFINER_PROMPT.format(\n",
    "                draft=current_draft, feedback=feedback_text\n",
    "            )\n",
    "\n",
    "            refiner_response = llm.complete(refiner_prompt)\n",
    "\n",
    "            # --- Step 5: Update Draft for Next Loop ---\n",
    "            current_draft = (\n",
    "                refiner_response.text\n",
    "            )  # The refined answer becomes the new draft\n",
    "\n",
    "            print(f\"--- Intermediate Refined Draft (Cycle {i + 1}) ---\")\n",
    "            print(textwrap.fill(current_draft, width=80))\n",
    "\n",
    "        # --- END OF MODIFIED LOOP ---\n",
    "\n",
    "        # The loop is finished, 'current_draft' holds the final answer\n",
    "        final_answer = current_draft\n",
    "\n",
    "        # Print the final, refined answer\n",
    "        print(\"\\n--- Final Refined Answer (After All Cycles) ---\")\n",
    "        print(textwrap.fill(final_answer, width=80))\n",
    "\n",
    "        # --- Stop tracking and get emissions ---\n",
    "        end_time = time.time()\n",
    "        emissions_data = tracker.stop_task()\n",
    "\n",
    "        print(f\"\\n\\n--- Query Metrics (Full Loop) ---\")\n",
    "        print(f\"Latency: {end_time - start_time:.2f} seconds\")\n",
    "        print(f\"Emissions: {emissions_data.emissions * 1000:.6f} gCO2eq\")\n",
    "        print(f\"Energy: {emissions_data.energy_consumed * 1000:.6f} Wh\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "finally:\n",
    "    # This stops the main tracker\n",
    "    total_emissions_kg = tracker.stop()\n",
    "    print(\"\\n\\n--- Total Emissions Summary (Session) ---\")\n",
    "    if tracker.emissions_data:\n",
    "        print(\n",
    "            f\"Total Energy Consumed: {tracker.emissions_data.energy_consumed * 1000:.4f} Wh\"\n",
    "        )\n",
    "    print(f\"Total CO2 Emitted: {total_emissions_kg * 1000:.4f} gCO2eq\")\n",
    "    print(\"Full report saved to 'emissions.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
