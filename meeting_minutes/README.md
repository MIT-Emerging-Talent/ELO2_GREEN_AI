<!-- markdownlint-disable MD013 -->
<!-- Disabled MD013 (Line length) for better readability -->

# ğŸ—“ï¸ Meeting Minutes â€“ Environmental Impact of AI Models

This directory documents the weekly progress and decision-making process for the research project onÂ **the environmental and performance trade-offs between large proprietary and small open-source AI models**.

Each meeting entry outlines team discussions, feedback, experimental progress, and assigned tasks across project milestones.

## ğŸ§­ Milestone 1 â€“ Scoping & Research Question Refinement

**Timeline:**Â September 27 â€“ October 14, 2025

The first milestone focused on refining the research direction and defining a clear, measurable problem withinÂ **Green AI**. After exploring various AI-related topics, the team finalized the project title â€”Â **â€œGreen AI Benchmarking of Foundation Modelsâ€**Â â€” and the research question:

> Can open-source LLMs match the accuracy of commercial models while reducing environmental impact?
>

Key progress included reviewing literature on energy, carbon, and water use in AI systems, selecting benchmark tasks (**reasoning**Â andÂ **summarization**), and identifying evaluation metrics forÂ **accuracy**Â andÂ **environmental footprint**. The team also chose comparison models (**GPT-4**Â andÂ **Mistral-7B**), created shared documentation, and distributed responsibilities among members.

By the end of Milestone 1, the project established its scope, research framework, and collaborative infrastructure, setting the stage forÂ **Milestone 2**, focused on tool setup and metric calibration.

## âš™ï¸ Milestone 2 â€“ Tool Setup & Experiment Planning

**Timeline:**Â October 15 â€“ November 6, 2025

With the research framework and scope finalized in Milestone 1,Â **Milestone 2**Â focused on preparing the experimental environment and defining how sustainability metrics were be measured. This phase involved setting up tools such asÂ **CodeCarbon**,Â **CarbonTracker**, andÂ **Eco2AI**Â to monitor energy and carbon usage, and exploringÂ **Water Usage Effectiveness (WUE)**Â datasets from major cloud providers like AWS, Microsoft, and Google.

The team also planned to configure testing environments for small open-source models (e.g.,Â **Mistral**,Â **LLaMA-2**) usingÂ **Hugging Face Transformers**,Â **PyTorch**, and GPU-enabled platforms such asÂ **Colab**. Another core deliverable was theÂ **experimental design document**, which was outlining the metrics (energy, carbon, water, and accuracy), workflows, and methodology diagrams guiding the model evaluation process.

By the end of Milestone 2, the team completed the technical setup, finalized the measurement pipeline, and validated that all tracking tools operate consistently across model typesâ€”ensuring a smooth transition into Milestone 3, where full experiments will be executed.

## ğŸ“Š Milestone 3 â€“ Model Benchmarking & Data Collection

**Timeline:** November 7 â€“ November 18, 2025

Milestone 3 marked the beginning of the full experimental phase. Using the measurement pipeline and tooling established in Milestone 2, the team ran benchmark tasks on both proprietary and open-source models to collect data on **accuracy** and **environmental impact**. This included tracking **energy consumption and carbon emissions** for each testing model under consistent test conditions.

During this phase, the team also validated accuracy results on selected reasoning and summarization tasks, investigated irregular outputs, and updated evaluation scripts when needed. Additional observations such as **inference time, token throughput**, and **hardware utilization** were recorded to support later analysis.

By the end of Milestone 3, the project has produced a complete experimental dataset covering sustainability metrics and accuracy scores for all evaluated models, providing a strong foundation for **Milestone 4**, which focuses on human evaluation and qualitative assessment.

## ğŸ§ª Milestone 4 â€“ Human Evaluation & Survey Analysis

**Timeline:** November 19 â€“ December 3, 2025

Milestone 4 centered on incorporating **human judgment** into the benchmarking process and concluded successfully. The team prepared and published a Google Form survey to compare model outputs side-by-side, and participants evaluated **clarity, coherence, informativeness, factuality,** and **overall preference**.

To improve participation and focus, the survey scope was refined to eight questions across four categoriesâ€”**Reasoning, Summarization, Creative Writing,** and **Paraphrasing**â€”and the **Retrieval/RAG** category was excluded due to its emphasis on factual lookup rather than generative quality.

Once responses were collected, the team analyzed the results by aggregating scores, assessing agreement among reviewers, and comparing human preferences. Initial insights, including distributional patterns and respondent demographics, were reviewed via Google Forms visualizations, and notable alignments and divergences between human judgments and quantitative metrics were documented to guide interpretation in the final analysis.

By the end of Milestone 4, the project integrated the human evaluation results into the broader dataset, consolidated the confirmed question set and model pairings, and prepared materials for downstream reporting. This provided a more nuanced understanding of model performance, completing the human evaluation phase and setting up the transition into **Milestone 5**.

## ğŸ“£ Milestone 5 â€“ Communication of Results & Final Presentation

**Timeline:** December 4 â€“ ongoing

Milestone 5 focuses on packaging and communicating the projectâ€™s findings while completing the final presentation and releasing the full set of artifacts. The team is synthesizing human evaluation results to produce a coherent analysis narrative, drafting and editing the presentation and article for publication, and finalizing an infographic and visual summary that will be embedded in both the article and the presentation.

In parallel, the repository is being cleaned and organized to publish the code, data, and analysis notebooks with clear usage notes and data access instructions. Everything will be finalized on December 7.
