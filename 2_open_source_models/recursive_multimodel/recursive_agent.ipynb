{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd933b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub\n",
    "!pip install llama-index-core\n",
    "!pip install llama-index-embeddings-huggingface\n",
    "!pip install sentence-transformers\n",
    "!pip install pypdf\n",
    "!pip install codecarbon\n",
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f230080",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-llms-llama-cpp llama-index-embeddings-huggingface llama-index-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1869cdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YNA\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "from huggingface_hub import InferenceClient\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from codecarbon import OfflineEmissionsTracker\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "HF_API_KEY = (\n",
    "    \"hf_abcdefghijklmnopqrstuvwxyz\"  # Replace with your actual Hugging Face API key\n",
    ")\n",
    "CRITIC_MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "# Local Config (Refiner)\n",
    "LAMINI_MODEL_PATH = r\"C:\\Users\\user\\.cache\\huggingface\\hub\\models--MBZUAI--LaMini-Flan-T5-248M\\snapshots\\4e871ba5f20216feaa3b845fc782229cd64eba47\"\n",
    "DATA_PATH = (\n",
    "    r\"C:\\Users\\user\\ELO2_GREEN_AI\\2_open_source_models\\quantized_models\\mistral7b\\data\"\n",
    ")\n",
    "YOUR_COUNTRY_ISO_CODE = \"EGY\"\n",
    "\n",
    "# --- 2. Define Robust Prompts ---\n",
    "\n",
    "# Critic: Forced to start with a status tag\n",
    "CRITIC_SYSTEM_PROMPT = \"\"\"You are a strict Editor.\n",
    "Compare the 'Draft' to the 'Source Context'.\n",
    "\n",
    "Output format:\n",
    "- Start with \"[OK]\" if the draft is accurate and needs no changes.\n",
    "- Start with \"[REVISE]\" if there are errors or missing key facts.\n",
    "- Then provide a bulleted list of feedback.\n",
    "\n",
    "Rules:\n",
    "1. If the Draft contradicts the Context, mark it [REVISE].\n",
    "2. If the Draft is missing a CRITICAL fact, mark it [REVISE].\n",
    "3. Do NOT nitpick small details.\"\"\"\n",
    "\n",
    "CRITIC_USER_TEMPLATE = \"\"\"--- Source Context ---\n",
    "{context}\n",
    "--- User Question ---\n",
    "{query}\n",
    "--- Draft Answer ---\n",
    "{draft}\n",
    "\n",
    "Critique:\"\"\"\n",
    "\n",
    "REFINER_PROMPT_TEMPLATE = \"\"\"You are a professional Writer.\n",
    "Rewrite the 'Draft Answer' to incorporate the 'Editor's Feedback'.\n",
    "\n",
    "Rules:\n",
    "- Only fix what the Editor asked for.\n",
    "- Do NOT cut off the answer; write the complete response.\n",
    "- Do not add external info.\n",
    "\n",
    "--- Draft Answer ---\n",
    "{draft}\n",
    "\n",
    "--- Editor's Feedback ---\n",
    "{feedback}\n",
    "\n",
    "--- Rewritten Answer ---\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "159299b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Client (Critic) initialized.\n",
      "Loading LaMini-Flan-T5 (Refiner)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Mistral 7B (Refiner) loaded.\n",
      "Retriever ready.\n"
     ]
    }
   ],
   "source": [
    "if not HF_API_KEY:\n",
    "    raise ValueError(\"HF_TOKEN not set.\")\n",
    "\n",
    "# --- 1. Initialize API Client (Critic) ---\n",
    "client = InferenceClient(token=HF_API_KEY)\n",
    "print(\"API Client (Critic) initialized.\")\n",
    "\n",
    "# --- 2. Initialize Local Mistral (Refiner) ---\n",
    "\n",
    "\n",
    "print(\"Loading LaMini-Flan-T5 (Refiner)...\")\n",
    "lamini_tokenizer = AutoTokenizer.from_pretrained(LAMINI_MODEL_PATH)\n",
    "lamini_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    LAMINI_MODEL_PATH, torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "\n",
    "class LaMiniWrapper:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def complete(self, prompt):\n",
    "        inputs = self.tokenizer(\n",
    "            prompt, return_tensors=\"pt\", max_length=512, truncation=True\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            temperature=0.1,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=1.1,\n",
    "            no_repeat_ngram_size=3,\n",
    "        )\n",
    "\n",
    "        text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        class Response:\n",
    "            def __init__(self, text):\n",
    "                self.text = text\n",
    "\n",
    "        return Response(text)\n",
    "\n",
    "\n",
    "llm_local = LaMiniWrapper(lamini_model, lamini_tokenizer)\n",
    "print(\"Local Mistral 7B (Refiner) loaded.\")\n",
    "\n",
    "# --- 3. Initialize Local Retriever ---\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "documents = SimpleDirectoryReader(DATA_PATH).load_data()\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "retriever = index.as_retriever(similarity_top_k=3)\n",
    "print(\"Retriever ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "419abb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hybrid Studio (Robust Version) ---\n",
      "Type 'exit' to quit.\n",
      "\n",
      "--- Cycle 1 ---\n",
      "1. Cloud API (Critic) is evaluating...\n",
      "\n",
      "[Editor's Feedback]:\n",
      "[REVISE]\n",
      "\n",
      "- The draft does not accurately describe the computer alarms that appeared during the landing. The draft should include the specific computer alarms (1201 and 1202) and the crew's reaction to them.\n",
      "- The draft does not mention the specific actions taken by the crew in response to the computer alarms, such as Armstrong taking semi-automatic control.\n",
      "- The draft does not include the details about Mission Control's role in addressing the alarms and the reassurance given to the crew.\n",
      "- The draft does not mention the post-mission analysis showing the real fuel remaining was probably closer to 50 seconds, which is critical information.\n",
      "\n",
      "Feedback:\n",
      "- The draft should include the specific computer alarms (1201 and 1202) and the crew's reaction to them.\n",
      "- Include the details about Mission Control's role in addressing the alarms and the reassurance given to the crew.\n",
      "- Mention the post-mission analysis showing the real fuel remaining was probably closer to 50 seconds.\n",
      "\n",
      "2. Local GPU (Refiner) is rewriting...\n",
      "\n",
      "[Refined Draft]:\n",
      "Draft Answer: As the descent began, Armstrong and Aldrin found themselves\n",
      "passing landmarks on the surface two or three seconds early. The problem could\n",
      "have been mascons, concentrations of high mass in a region or regions of the\n",
      "Moon's crust that contains a gravitational anomaly, potentially altering Eagle's\n",
      "trajectory. Five minutes into the descent, the burn and 6,000 feet (1,800 m)\n",
      "above the surface of the moon, the LM guidance computer (LGC) distracted the\n",
      "crew with the first of several unexpected 1201 and 1202 program alarms. Inside\n",
      "Mission Control Center, computer engineer Jack Garman told Guidance Officer\n",
      "Steve Bales it was safe to continue the descent. The program alarm indicated\n",
      "\"executive overflows,\" meaning the guidance computer could not complete all its\n",
      "tasks in real-time and had to postpone some of them. Margaret Hamilton, the\n",
      "Director of Apollo Flight Computer Programming at the MIT Charles Stark Draper\n",
      "Laboratory later recalled: \"To blame the computer for the Apollo 11 problems is\n",
      "like blaming the person who spots a fire and calls the fire department.\n",
      "Actually, the computer was programmed to do more than recognize error\n",
      "conditions. A complete set of recovery programs was incorporated into the\n",
      "software. The software's action, in this case, was to eliminate lower priority\n",
      "tasks and re-establish the more important ones. The computer, rather than almost\n",
      "forcing an abort, prevented an abourt. If the computer hadn't recognized this\n",
      "problem and took recovery action, I doubt if Apollo 11 would have been the\n",
      "successful Moon landing it was.\n",
      "\n",
      "--- Cycle 2 ---\n",
      "1. Cloud API (Critic) is evaluating...\n",
      "\n",
      "[Editor's Feedback]:\n",
      "[REVISE]\n",
      "\n",
      "- The draft does not mention the specific computer alarms (1201 and 1202) that appeared during the landing.\n",
      "- The draft does not reference the exact timing of the alarms (five minutes into the descent, 6,000 feet above the surface).\n",
      "- The draft does not include the specific actions taken by Mission Control to address the alarms.\n",
      "- The draft does not mention the critical role of the guidance computer in safely continuing the descent despite the alarms.\n",
      "- The draft does not include the perspective of the astronauts experiencing the alarms.\n",
      "- The draft does not provide the exact quote from Jack Garman to Steve Bales.\n",
      "- The draft does not mention Margaret Hamilton's role or her specific recollection of the event.\n",
      "\n",
      "2. Local GPU (Refiner) is rewriting...\n",
      "\n",
      "[Refined Draft]:\n",
      "Draft Answer: The LM guidance computer (LGC) distracted the crew with the first\n",
      "of several unexpected 1201 and 1202 program alarms during the Apollo 11 mission.\n",
      "The program alarm indicated \"executive overflows,\" meaning the guidance computer\n",
      "could not complete all its tasks in real-time and had to postpone some of them.\n",
      "The recovery programs were incorporated into the software, eliminating lower\n",
      "priority tasks and re-establishing the more important ones. Margaret Hamilton,\n",
      "the Director of Apollo Flight Computer Programming at the MIT Charles Stark\n",
      "Draper Laboratory later recalled that to blame the computer for the Apollo 11,\n",
      "problems is like blaming the person who spots a fire and calls the fire\n",
      "department. Actually, the computer was programmed to do more than recognize\n",
      "error conditions. The software's action, in this case, was to eliminate lower\n",
      "priority jobs and regain the more significant ones. If the computer hadn't\n",
      "recognized this problem and took recovery action, I doubt if Apollo 11 would\n",
      "have been the successful Moon landing it was.\n",
      "\n",
      "==================================================\n",
      "FINAL RESULT:\n",
      "Draft Answer: The LM guidance computer (LGC) distracted the crew with the first\n",
      "of several unexpected 1201 and 1202 program alarms during the Apollo 11 mission.\n",
      "The program alarm indicated \"executive overflows,\" meaning the guidance computer\n",
      "could not complete all its tasks in real-time and had to postpone some of them.\n",
      "The recovery programs were incorporated into the software, eliminating lower\n",
      "priority tasks and re-establishing the more important ones. Margaret Hamilton,\n",
      "the Director of Apollo Flight Computer Programming at the MIT Charles Stark\n",
      "Draper Laboratory later recalled that to blame the computer for the Apollo 11,\n",
      "problems is like blaming the person who spots a fire and calls the fire\n",
      "department. Actually, the computer was programmed to do more than recognize\n",
      "error conditions. The software's action, in this case, was to eliminate lower\n",
      "priority jobs and regain the more significant ones. If the computer hadn't\n",
      "recognized this problem and took recovery action, I doubt if Apollo 11 would\n",
      "have been the successful Moon landing it was.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YNA\\AppData\\Roaming\\Python\\Python312\\site-packages\\codecarbon\\output_methods\\file.py:94: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, new_df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# --- Helper to call API (Increased Tokens) ---\n",
    "def call_critic_api(system_prompt, user_prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    try:\n",
    "        response = client.chat_completion(\n",
    "            messages=messages,\n",
    "            model=CRITIC_MODEL_ID,\n",
    "            max_tokens=2048,  # INCREASED from 512 to prevent cutting\n",
    "            temperature=0.1,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"API Error: {e}\"\n",
    "\n",
    "\n",
    "# --- Start Hybrid Loop ---\n",
    "REFINEMENT_CYCLES = 2\n",
    "tracker = OfflineEmissionsTracker(country_iso_code=YOUR_COUNTRY_ISO_CODE)\n",
    "tracker.start()\n",
    "\n",
    "print(\"\\n--- Hybrid Studio (Robust Version) ---\")\n",
    "print(\"Type 'exit' to quit.\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # 1. Inputs\n",
    "        query = input(\"\\n(1/3) Enter User Query: \")\n",
    "        if query.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "\n",
    "        # Retrieve Context locally\n",
    "        retrieved_nodes = retriever.retrieve(query)\n",
    "        context_str = \"\\n---\\n\".join([node.get_content() for node in retrieved_nodes])\n",
    "\n",
    "        draft_text = input(\"(2/3) Paste Draft Text: \")\n",
    "\n",
    "        tracker.start_task(\"Hybrid Refinement\")\n",
    "        current_draft = draft_text\n",
    "\n",
    "        for i in range(REFINEMENT_CYCLES):\n",
    "            print(f\"\\n--- Cycle {i + 1} ---\")\n",
    "\n",
    "            # --- A. CRITIC STEP (API) ---\n",
    "            print(\"1. Cloud API (Critic) is evaluating...\")\n",
    "            critic_input = CRITIC_USER_TEMPLATE.format(\n",
    "                context=context_str, query=query, draft=current_draft\n",
    "            )\n",
    "            critique = call_critic_api(CRITIC_SYSTEM_PROMPT, critic_input)\n",
    "\n",
    "            print(f\"\\n[Editor's Feedback]:\\n{critique}\\n\")\n",
    "\n",
    "            # --- NEW ROBUST CHECK ---\n",
    "            # Only stop if it explicitly starts with [OK]\n",
    "            if critique.startswith(\"[OK]\"):\n",
    "                print(\">> Critic is satisfied. Stopping early.\")\n",
    "                break\n",
    "            elif \"[OK]\" in critique[:20]:  # Fallback if it has a small prefix\n",
    "                print(\">> Critic is satisfied. Stopping early.\")\n",
    "                break\n",
    "\n",
    "            # --- B. REFINER STEP (Local) ---\n",
    "            print(\"2. Local GPU (Refiner) is rewriting...\")\n",
    "            refiner_input = REFINER_PROMPT_TEMPLATE.format(\n",
    "                draft=current_draft, feedback=critique\n",
    "            )\n",
    "\n",
    "            # Local generation with sufficient length\n",
    "            refined_response = llm_local.complete(refiner_input)\n",
    "            current_draft = refined_response.text\n",
    "\n",
    "            print(\"\\n[Refined Draft]:\")\n",
    "            print(textwrap.fill(current_draft, width=80))\n",
    "\n",
    "        tracker.stop_task()\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"FINAL RESULT:\")\n",
    "        print(textwrap.fill(current_draft, width=80))\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "finally:\n",
    "    tracker.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
