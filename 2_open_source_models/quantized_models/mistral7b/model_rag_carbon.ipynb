{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local RAG with Mistral 7B & CodeCarbon Evaluation\n",
    "\n",
    "This notebook runs a complete Retrieval-Augmented Generation (RAG) pipeline locally on your machine. It uses:\n",
    "\n",
    "- **`llama-index`**: To build the RAG pipeline.\n",
    "- **`llama-cpp-python`**: To run the quantized Mistral 7B GGUF model.\n",
    "- **GPU Acceleration**: The model is configured to run on your NVIDIA GPU (`n_gpu_layers=-1`).\n",
    "- **`codecarbon`**: To measure the energy consumption and CO2 emissions for each query you make in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installations\n",
    "\n",
    "This cell installs all the required Python libraries. \n",
    "\n",
    "**Note:** This assumes you have already installed `llama-cpp-python` with the correct CUDA (GPU) support. If not, you may need to run this command in your terminal first:\n",
    "\n",
    "`$env:CMAKE_ARGS = \"-DGGML_CUDA=on\"`\n",
    "`pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in c:\\users\\engam\\anaconda3\\lib\\site-packages (0.14.5)\n",
      "Requirement already satisfied: llama-index-cli<0.6,>=0.5.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index) (0.5.3)\n",
      "Requirement already satisfied: llama-index-core<0.15.0,>=0.14.5 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index) (0.14.5)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.6,>=0.5.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index) (0.5.1)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index) (0.9.4)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.7,>=0.6.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index) (0.6.5)\n",
      "Requirement already satisfied: llama-index-readers-file<0.6,>=0.5.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index) (0.5.4)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index) (0.5.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (3.10.5)\n",
      "Requirement already satisfied: aiosqlite in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (2.2.0)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (2024.6.1)\n",
      "Requirement already satisfied: httpx in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (0.27.0)\n",
      "Requirement already satisfied: llama-index-workflows<3,>=2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (2.8.3)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (10.4.0)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (4.5.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (2.12.3)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (2.32.5)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.5->llama-index) (2.0.34)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (0.12.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (4.15.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15.0,>=0.14.5->llama-index) (1.14.1)\n",
      "Requirement already satisfied: openai>=1.1.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.109.1)\n",
      "Requirement already satisfied: llama-cloud==0.1.35 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.35)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-cloud==0.1.35->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.4.26)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (4.12.3)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.7.1)\n",
      "Requirement already satisfied: pandas<2.3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.2.2)\n",
      "Requirement already satisfied: pypdf<7,>=5.1.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (6.1.2)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.54)\n",
      "Requirement already satisfied: click in c:\\users\\engam\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\engam\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index) (2024.9.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.5->llama-index) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.5->llama-index) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.5->llama-index) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.5->llama-index) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.5->llama-index) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.5->llama-index) (1.11.0)\n",
      "Requirement already satisfied: griffe in c:\\users\\engam\\anaconda3\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.5->llama-index) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.5->llama-index) (3.1.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.5)\n",
      "Requirement already satisfied: anyio in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.15.0,>=0.14.5->llama-index) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.15.0,>=0.14.5->llama-index) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.15.0,>=0.14.5->llama-index) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.15.0,>=0.14.5->llama-index) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.15.0,>=0.14.5->llama-index) (0.14.0)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-workflows<3,>=2->llama-index-core<0.15.0,>=0.14.5->llama-index) (0.4.2)\n",
      "Requirement already satisfied: llama-cloud-services>=0.6.54 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.54)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.5->llama-index) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.5->llama-index) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.5->llama-index) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.15.0,>=0.14.5->llama-index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.15.0,>=0.14.5->llama-index) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.5->llama-index) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\engam\\anaconda3\\lib\\site-packages (from tqdm<5,>=4.66.1->llama-index-core<0.15.0,>=0.14.5->llama-index) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.15.0,>=0.14.5->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from dataclasses-json->llama-index-core<0.15.0,>=0.14.5->llama-index) (3.26.1)\n",
      "Requirement already satisfied: python-dotenv<2,>=1.0.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-cloud-services>=0.6.54->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.1.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.15.0,>=0.14.5->llama-index) (24.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.5->llama-index) (2.1.3)\n",
      "Requirement already satisfied: llama-index-llms-llama-cpp in c:\\users\\engam\\anaconda3\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: llama-cpp-python<0.4,>=0.3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-llms-llama-cpp) (0.3.4)\n",
      "Requirement already satisfied: llama-index-core<0.15,>=0.13.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-llms-llama-cpp) (0.14.5)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-cpp-python<0.4,>=0.3.0->llama-index-llms-llama-cpp) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-cpp-python<0.4,>=0.3.0->llama-index-llms-llama-cpp) (1.26.4)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-cpp-python<0.4,>=0.3.0->llama-index-llms-llama-cpp) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-cpp-python<0.4,>=0.3.0->llama-index-llms-llama-cpp) (3.1.4)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.10.5)\n",
      "Requirement already satisfied: aiosqlite in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.2.0)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2024.6.1)\n",
      "Requirement already satisfied: httpx in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.27.0)\n",
      "Requirement already satisfied: llama-index-workflows<3,>=2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.8.3)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.3)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.9.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (10.4.0)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (4.5.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.12.3)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.32.5)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.0.34)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.12.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (4.66.5)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.14.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.11.0)\n",
      "Requirement already satisfied: griffe in c:\\users\\engam\\anaconda3\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python<0.4,>=0.3.0->llama-index-llms-llama-cpp) (2.1.3)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-workflows<3,>=2->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.4.2)\n",
      "Requirement already satisfied: click in c:\\users\\engam\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\engam\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2024.9.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2025.4.26)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\engam\\anaconda3\\lib\\site-packages (from tqdm<5,>=4.66.1->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.26.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.14.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (24.1)\n",
      "Requirement already satisfied: llama-index-embeddings-huggingface in c:\\users\\engam\\anaconda3\\lib\\site-packages (0.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.33.0)\n",
      "Requirement already satisfied: llama-index-core<0.15,>=0.13.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-embeddings-huggingface) (0.14.5)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-embeddings-huggingface) (4.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\engam\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\engam\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.15.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\engam\\anaconda3\\lib\\site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.10.5)\n",
      "Requirement already satisfied: aiosqlite in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.2.0)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.2.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.27.0)\n",
      "Requirement already satisfied: llama-index-workflows<3,>=2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.8.3)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.3)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (10.4.0)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (4.5.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.12.3)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.0.34)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.12.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.14.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (4.52.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.7.1+cu118)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.13.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.11.0)\n",
      "Requirement already satisfied: griffe in c:\\users\\engam\\anaconda3\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.1.4)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from llama-index-workflows<3,>=2->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.4.2)\n",
      "Requirement already satisfied: click in c:\\users\\engam\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\engam\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2024.9.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2025.4.26)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.0.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\engam\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.4.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.5.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (3.26.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (0.14.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-embeddings-huggingface) (2.1.3)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\engam\\anaconda3\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.52.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.7.1+cu118)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.33.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\engam\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\engam\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\engam\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\engam\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\engam\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n",
      "Requirement already satisfied: pypdf in c:\\users\\engam\\anaconda3\\lib\\site-packages (6.1.2)\n",
      "Requirement already satisfied: torch in c:\\users\\engam\\anaconda3\\lib\\site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\engam\\anaconda3\\lib\\site-packages (0.22.1+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\engam\\anaconda3\\lib\\site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\engam\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\engam\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\engam\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\engam\\anaconda3\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\engam\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: codecarbon in c:\\users\\engam\\anaconda3\\lib\\site-packages (3.0.6)\n",
      "Requirement already satisfied: arrow in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (1.2.3)\n",
      "Requirement already satisfied: click in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (8.2.1)\n",
      "Requirement already satisfied: fief-client[cli] in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (0.20.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (2.2.2)\n",
      "Requirement already satisfied: prometheus_client in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (0.14.1)\n",
      "Requirement already satisfied: psutil>=6.0.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (7.1.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (9.0.0)\n",
      "Requirement already satisfied: pydantic in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (2.12.3)\n",
      "Requirement already satisfied: nvidia-ml-py in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (13.580.82)\n",
      "Requirement already satisfied: rapidfuzz in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (3.14.1)\n",
      "Requirement already satisfied: requests in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (2.32.5)\n",
      "Requirement already satisfied: questionary in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (2.1.1)\n",
      "Requirement already satisfied: rich in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (13.7.1)\n",
      "Requirement already satisfied: typer in c:\\users\\engam\\anaconda3\\lib\\site-packages (from codecarbon) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from arrow->codecarbon) (2.9.0.post0)\n",
      "Requirement already satisfied: colorama in c:\\users\\engam\\anaconda3\\lib\\site-packages (from click->codecarbon) (0.4.6)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.21.3 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from fief-client[cli]->codecarbon) (0.27.0)\n",
      "Requirement already satisfied: jwcrypto<2.0.0,>=1.4 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from fief-client[cli]->codecarbon) (1.5.6)\n",
      "Requirement already satisfied: yaspin in c:\\users\\engam\\anaconda3\\lib\\site-packages (from fief-client[cli]->codecarbon) (3.2.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pandas->codecarbon) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pandas->codecarbon) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pandas->codecarbon) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pydantic->codecarbon) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pydantic->codecarbon) (2.41.4)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pydantic->codecarbon) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from pydantic->codecarbon) (0.4.2)\n",
      "Requirement already satisfied: prompt_toolkit<4.0,>=2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from questionary->codecarbon) (3.0.43)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests->codecarbon) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests->codecarbon) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests->codecarbon) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from requests->codecarbon) (2025.4.26)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from rich->codecarbon) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from rich->codecarbon) (2.15.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from typer->codecarbon) (1.5.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (0.14.0)\n",
      "Requirement already satisfied: cryptography>=3.4 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (43.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->codecarbon) (0.1.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\engam\\anaconda3\\lib\\site-packages (from prompt_toolkit<4.0,>=2.0->questionary->codecarbon) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.16.0)\n",
      "Requirement already satisfied: termcolor<4.0,>=3.1 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from yaspin->fief-client[cli]->codecarbon) (3.1.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\engam\\anaconda3\\lib\\site-packages (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\engam\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index\n",
    "!pip install llama-index-llms-llama-cpp\n",
    "!pip install llama-index-embeddings-huggingface\n",
    "!pip install sentence-transformers\n",
    "!pip install pypdf\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install codecarbon\n",
    "!pip install langchain-community # Dependency for Ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Configuration\n",
    "\n",
    "Here we import all necessary modules and set up the file paths for your model and data. \n",
    "\n",
    "**Please double-check that `MODEL_PATH` and `DATA_PATH` are correct for your system.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\engam\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'validate_default' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'validate_default' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\engam\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Configuration loaded.\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "import time\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from codecarbon import OfflineEmissionsTracker\n",
    "from llama_index.core import PromptTemplate\n",
    "import textwrap\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "\n",
    "# Set the path to your downloaded GGUF model\n",
    "# IMPORTANT: Use a raw string (r\"...\") for Windows paths\n",
    "# MODEL_PATH =r\"D:\\\\Mistral7B\\\\mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
    "\n",
    "# Set the path to your data (PDFs, .txt, etc.)\n",
    "DATA_PATH = r\"D:\\Mistral7B\\data\"\n",
    "\n",
    "# Set your country's 3-letter ISO code for CodeCarbon\n",
    "# Find your code: https://en.wikipedia.org/wiki/List_of_ISO_3166-1_alpha-3_codes\n",
    "YOUR_COUNTRY_ISO_CODE = \"EGY\"\n",
    "\n",
    "print(\"Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Models and Index\n",
    "\n",
    "This cell loads the Mistral 7B model into your GPU VRAM, loads the embedding model, and then scans your `DATA_PATH` to build the searchable RAG index. This step may take a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3050 Laptop GPU, compute capability 8.6, VMM: yes\n",
      "llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3050 Laptop GPU) - 3302 MiB free\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from D:/Mistral7B/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: control token:      2 '</s>' is not marked as EOG\n",
      "llm_load_vocab: control token:      1 '<s>' is not marked as EOG\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading output layer to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CUDA0 model buffer size =  4095.05 MiB\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =    70.31 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 3904\n",
      "llama_new_context_with_model: n_ctx_per_seq = 3904\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 1000000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (3904) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   488.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  488.00 MiB, K (f16):  244.00 MiB, V (f16):  244.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   283.63 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    15.63 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | FORCE_MMQ = 1 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading and indexing documents...\n",
      "Loaded 1 document(s).\n",
      "Indexing complete.\n",
      "Query engine is ready (with custom anti-leak prompt).\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing models...\")\n",
    "MODEL_PATH = \"D:/Mistral7B/mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
    "\n",
    "# Load the local LLM (Mistral 7B) with GPU offloading\n",
    "llm = LlamaCPP(\n",
    "    model_path=MODEL_PATH,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=1024,\n",
    "    context_window=3900,\n",
    "    generate_kwargs={},\n",
    "    # Set n_gpu_layers to -1 to offload all layers to GPU\n",
    "    model_kwargs={\"n_gpu_layers\": -1},\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Load the local Embedding Model\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# Set up LlamaIndex global settings to use our local models\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "print(\"\\nLoading and indexing documents...\")\n",
    "documents = SimpleDirectoryReader(DATA_PATH).load_data()\n",
    "print(f\"Loaded {len(documents)} document(s).\")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "print(\"Indexing complete.\")\n",
    "\n",
    "\n",
    "# --- ADD THIS SECTION ---\n",
    "# Define the new, strict prompt template\n",
    "qa_template_str = (\n",
    "    \"You are an expert assistant. Answer the user's question based *only* on the \"\n",
    "    \"provided context.\\n\\n\"\n",
    "    \"Strict Rules:\\n\"\n",
    "    \"1. Do not mention the context, the source document, or 'the text'.\\n\"\n",
    "    \"2. Answer the question directly, as if you knew the information yourself.\\n\"\n",
    "    \"3. If the answer is not in the context, state that you do not have enough \"\n",
    "    \"information to answer.\\n\\n\"\n",
    "    \"--- Context ---\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"--- Question ---\\n\"\n",
    "    \"{query_str}\\n\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "qa_template_str2 = (\n",
    "    \"You are an expert assistant. Answer the user's question based *only* on the \"\n",
    "    \"provided context.\\n\\n\"\n",
    "    \"Strict Rules:\\n\"\n",
    "    \"1. Use the context as inspiration, but do not copy it.'.\\n\"\n",
    "    \"2. Expand or interpret the ideas creatively, producing a short paragraph.\\n\"\n",
    "    \"3. Keep the tone natural and imaginative, as if writing your own reflection\"\n",
    "    \"information to answer.\\n\\n\"\n",
    "    \"--- Context ---\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"--- Question ---\\n\"\n",
    "    \"{query_str}\\n\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "qa_template_str3 = (\n",
    "    \"You are an expert assistant. Answer the user's question based *only* on the \"\n",
    "    \"provided context.\\n\\n\"\n",
    "    \"Strict Rules:\\n\"\n",
    "    \"1. Rewrite the given information in your own words.'.\\n\"\n",
    "    \"2. Preserve meaning and tone without copying phrases directly..\\n\"\n",
    "    \"3. The output should read naturally like an original paragraph.\"\n",
    "    \"information to answer.\\n\\n\"\n",
    "    \"--- Context ---\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"--- Question ---\\n\"\n",
    "    \"{query_str}\\n\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "qa_template_str4 = (\n",
    "    \"You are an expert assistant. Answer the user's question based *only* on the \"\n",
    "    \"provided context.\\n\\n\"\n",
    "    \"Strict Rules:\\n\"\n",
    "    \"1. Provide short, well-structured answers (25 sentences)..'.\\n\"\n",
    "    \"2. Use only logical reasoning\\n\"\n",
    "    \"3. Do not add assumptions or outside facts\"\n",
    "    \"information to answer.\\n\\n\"\n",
    "    \"--- Context ---\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"--- Question ---\\n\"\n",
    "    \"{query_str}\\n\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "qa_template = PromptTemplate(qa_template_str)\n",
    "# --- END SECTION ---\n",
    "\n",
    "\n",
    "# --- MODIFY THIS LINE ---\n",
    "# Create the query engine, passing in the new template\n",
    "query_engine = index.as_query_engine(\n",
    "    streaming=True,\n",
    "    text_qa_template=qa_template,  # <-- Pass the template here\n",
    ")\n",
    "# --- END MODIFICATION ---\n",
    "\n",
    "print(\"Query engine is ready (with custom anti-leak prompt).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Start Interactive RAG + Carbon Tracking\n",
    "\n",
    "Run this cell to start the interactive chat. You can ask questions about your documents.\n",
    "\n",
    "- Type your question and press Enter.\n",
    "- The model will stream its answer.\n",
    "- After the answer, `codecarbon` will print the latency and environmental cost for that specific query.\n",
    "- Type `exit` to stop the loop and see the total emissions for the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing CodeCarbon tracker for country: EGY\n",
      "\n",
      "--- Query Engine Ready (Tracking Emissions) ---\n",
      "Type 'exit' to quit.\n",
      "\n",
      "\n",
      "Your Question: Summarize the main events during the Apollo 11 lunar landing in 3 sentences.\n",
      "\n",
      "Assistant: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   70428.16 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1885 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   102 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  119472.65 ms /  1987 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Apollo 11 lunar landing was marked by Armstrong and Aldrin reporting they\n",
      "were passing landmarks too early due to Eagle traveling too fast, and\n",
      "encountering unexpected 1201 and 1202 program alarms. The guidance computer,\n",
      "rather than forcing an abort, took recovery action and prevented an abort,\n",
      "allowing Armstrong to take semi-automatic control and land the spacecraft in a\n",
      "clear patch of ground, despite having limited propellant remaining.\n",
      "\n",
      "\n",
      "--- Query Metrics ---\n",
      "Latency: 119.76 seconds\n",
      "Emissions: 1.457414 gCO2eq\n",
      "Energy: 2.555495 Wh\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Your Question: What were the main challenges Armstrong faced while landing the Eagle?\n",
      "\n",
      "Assistant: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1858 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   70428.16 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   140 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   68114.01 ms /   159 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The main challenges Armstrong faced while landing the Eagle were passing\n",
      "landmarks earlier than expected due to Eagle traveling too fast, encountering\n",
      "unexpected 1201 and 1202 program alarms, and dealing with a gravitational\n",
      "anomaly caused by mascons in the Moon's crust. Additionally, Armstrong had to\n",
      "take semi-automatic control when the computer's landing target was in a boulder-\n",
      "strewn area and had to land at the first possible site due to dwindling\n",
      "propellant supply. Lunar dust kicked up by the LM's engine also impaired his\n",
      "ability to determine the spacecraft's motion.\n",
      "\n",
      "\n",
      "--- Query Metrics ---\n",
      "Latency: 68.81 seconds\n",
      "Emissions: 0.855907 gCO2eq\n",
      "Energy: 1.500786 Wh\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Your Question: Describe the activities the astronauts performed on the lunar surface.\n",
      "\n",
      "Assistant: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1858 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   70428.16 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   274 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  137524.22 ms /   294 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The astronauts, Armstrong and Aldrin, found themselves passing landmarks on the\n",
      "lunar surface earlier than expected due to Eagle traveling too fast. They\n",
      "reported this to Mission Control and experienced several unexpected 1201 and\n",
      "1202 program alarms. Mission Control assured them it was safe to continue the\n",
      "descent. The alarms indicated 'executive overflows', meaning the guidance\n",
      "computer could not complete all its tasks in real-time and had to postpone some.\n",
      "Margaret Hamilton, the Director of Apollo Flight Computer Programming, later\n",
      "recalled that the computer was programmed to do more than just recognize error\n",
      "conditions and had a complete set of recovery programs incorporated into the\n",
      "software. The computer's action was to eliminate lower priority tasks and re-\n",
      "establish the more important ones, preventing an abort. Armstrong took semi-\n",
      "automatic control when he saw the computer's landing target was in a boulder-\n",
      "strewn area. Throughout the descent, Aldrin called out navigation data to\n",
      "Armstrong. With only 90 seconds of propellant remaining, Armstrong found a clear\n",
      "patch of ground and maneuvered the spacecraft towards it. However, lunar dust\n",
      "kicked up by the LM's engine began to impair his ability to determine the\n",
      "spacecraft's motion.\n",
      "\n",
      "\n",
      "--- Query Metrics ---\n",
      "Latency: 137.77 seconds\n",
      "Emissions: 1.730770 gCO2eq\n",
      "Energy: 3.034809 Wh\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Your Question: Compare the planned timeline for the lunar surface operations with what actually happened.\n",
      "\n",
      "Assistant: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1858 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   70428.16 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   211 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  103056.42 ms /   232 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the context, Armstrong and Aldrin found themselves passing landmarks\n",
      "earlier than expected during their descent, indicating they were traveling too\n",
      "fast. They reported being \"long\" and miles west of their target point. The LM\n",
      "guidance computer (LGC) experienced unexpected 1201 and 1202 program alarms,\n",
      "which were later determined to be \"executive overflows,\" meaning the computer\n",
      "could not complete all its tasks in real-time and had to postpone some of them.\n",
      "The computer's recovery programs prevented an abort and allowed the successful\n",
      "Moon landing. Armstrong took semi-automatic control when the computer's landing\n",
      "target was in a boulder-strewn area. He was determined to land at the first\n",
      "possible site due to dwindling propellant. The actual landing site was not\n",
      "explicitly stated in the context, so it's unclear how it compares to the planned\n",
      "site. However, it appears that the descent and landing deviated from the planned\n",
      "timeline.\n",
      "\n",
      "\n",
      "--- Query Metrics ---\n",
      "Latency: 103.24 seconds\n",
      "Emissions: 1.311431 gCO2eq\n",
      "Energy: 2.299522 Wh\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "--- Total Emissions Summary (Session) ---\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'OfflineEmissionsTracker' object has no attribute 'final_emissions_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Total Emissions Summary (Session) ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Access total energy from the tracker object itself\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracker\u001b[38;5;241m.\u001b[39mfinal_emissions_data:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Energy Consumed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtracker\u001b[38;5;241m.\u001b[39mfinal_emissions_data\u001b[38;5;241m.\u001b[39menergy_consumed\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1000\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Wh\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     52\u001b[0m     )\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal CO2 Emitted: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_emissions_kg\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1000\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m gCO2eq\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'OfflineEmissionsTracker' object has no attribute 'final_emissions_data'"
     ]
    }
   ],
   "source": [
    "print(f\"\\nInitializing CodeCarbon tracker for country: {YOUR_COUNTRY_ISO_CODE}\")\n",
    "tracker = OfflineEmissionsTracker(country_iso_code=YOUR_COUNTRY_ISO_CODE)\n",
    "tracker.start()\n",
    "\n",
    "print(\"\\n--- Query Engine Ready (Tracking Emissions) ---\")\n",
    "print(\"Type 'exit' to quit.\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        query = input(\"Ask a question about your documents: \")\n",
    "        if query.lower() == \"exit\":\n",
    "            break\n",
    "\n",
    "        # --- Start tracking just for the query ---\n",
    "        tracker.start_task(\"RAG Query\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        response_stream = query_engine.query(query)\n",
    "\n",
    "        # Print the user's question\n",
    "        print(f\"\\n\\nYour Question: {query}\")\n",
    "        print(\"\\nAssistant: \")\n",
    "\n",
    "        # --- Iterate stream and wrap text ---\n",
    "        full_answer_text = \"\"\n",
    "        for chunk_text in response_stream.response_gen:\n",
    "            full_answer_text += chunk_text\n",
    "\n",
    "        # Wrap the complete answer to a width of 80 characters\n",
    "        wrapped_answer = textwrap.fill(full_answer_text, width=80)\n",
    "        print(wrapped_answer)\n",
    "        # --- END MODIFICATION --\n",
    "\n",
    "        # --- Stop tracking and get emissions for this single query ---\n",
    "        end_time = time.time()\n",
    "        emissions_data = tracker.stop_task()\n",
    "\n",
    "        print(\"\\n\\n--- Query Metrics ---\")\n",
    "        print(f\"Latency: {end_time - start_time:.2f} seconds\")\n",
    "        print(f\"Emissions: {emissions_data.emissions * 1000:.6f} gCO2eq\")\n",
    "        print(f\"Energy: {emissions_data.energy_consumed * 1000:.6f} Wh\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "finally:\n",
    "    # This stops the main tracker and saves the total emissions.csv file\n",
    "    total_emissions_kg = tracker.stop()\n",
    "    print(\"\\n\\n--- Total Emissions Summary (Session) ---\")\n",
    "    # Access total energy from the tracker object itself\n",
    "    if tracker.final_emissions_data:\n",
    "        print(\n",
    "            f\"Total Energy Consumed: {tracker.final_emissions_data.energy_consumed * 1000:.4f} Wh\"\n",
    "        )\n",
    "    print(f\"Total CO2 Emitted: {total_emissions_kg * 1000:.4f} gCO2eq\")\n",
    "    print(\"Full report saved to 'emissions.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
