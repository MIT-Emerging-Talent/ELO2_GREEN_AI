# Local RAG Implementation and Evaluation with Mistral 7B

## 1. ðŸŽ¯ Project Goal

This project implements and evaluates a **Retrieval-Augmented Generation (RAG)**
 pipeline using **Mistral 7B**, a powerful, open-source Large Language Model (LLM).

The primary goal was to test the feasibility, accuracy, and environmental cost
of running an open-source model locally on consumer hardware (a laptop) as
 a viable alternative to paid, commercial API-based models.

---

## 2. ðŸ¤– About the Model: Mistral 7B

### Why Mistral 7B Was Chosen

Mistral 7B was selected as the core model for this project for several key reasons:

* **Performance vs. Size:** It is famous for outperforming much larger models
(like Llama 2 13B)
* on a wide range of benchmarks, offering state-of-the-art
  performance in a small package.
* **Local Feasibility:** Its 7-billion-parameter size, especially when **quantized**
* (shrunk) into a format like `.gguf`, is small enough to run effectively on
  consumer-grade hardware.
  This was essential for the project's goal of local, laptop-based execution.
* **Open-Source & Privacy:** As a fully open-source model, it can be run 100%
  offline. This ensures complete data privacy and eliminates API fees.
* **Excellent Community Support:** Mistral 7B is well-supported by key RAG frameworks
  , including LlamaIndex and LangChain, and can be run efficiently using `llama-cpp-python`.

### Development & Key Facts

Mistral 7B was developed by **Mistral AI**, a Paris-based AI startup, and released
 in September 2023.

* **Core Goal:** Mistral AI's goal was to prove that **efficiency** was a more
  important frontier than just model size. They demonstrated that a smaller,
  exceptionally well-trained model could outperform larger competitors.
* **Key Innovation (GQA):** It uses **Grouped-Query Attention (GQA)** for faster
  inference. This allows the model to process information and generate responses
  much more quickly and with a smaller memory footprint (KV cache) than models
  using standard Multi-Head Attention.
* **Key Innovation (SWA):** It also employs **Sliding Window Attention (SWA)**.
* This mechanism allows the model to handle very long sequences
  (a 32k token context window) without the typical massive computation and memory
 costs.
* **License:** It was released under the **Apache 2.0 license**, a very
  permissive open-source license that allows for almost unrestricted use and
  modification by the community.

---

## 3. ðŸ› ï¸ Methodology: Retrieval-Augmented Generation (RAG)

To answer questions about specific documents (like PDFs or text files) that the
 model wasn't trained on, we must augment its knowledge. We implemented
 a RAG pipeline, which works in two main stages:

1. **Retrieval:** When a user asks a question, the pipeline first converts the
question into a numerical representation (an embedding). It uses this to search
a pre-built index of the source documents and "retrieves" the most relevant
chunks of text.
2. **Generation:** The retrieved text chunks (the "context") are then combined
with the original question into a single, comprehensive prompt. This prompt
is fed to Mistral 7B, which generates a final answer based *only* on the
provided context.

### Implementation Details

* **Framework:** `llama-index`
* **Model Loader:** `llama-cpp-python` (To run the GGUF model)
* **Embedding Model:** `BAAI/bge-small-en-v1.5` (To create the vector index of documents)
* **LLM:** `mistral-7b-instruct-v0.2.Q4_K_M.gguf` (A 4-bit quantized version of
* the model, ideal for balancing performance and resource use)

---

## 4. ðŸ“Š Project Results: Prompts & Responses

Below is a sample of the prompts given to the RAG pipeline and the corresponding
 answers generated by the local Mistral 7B model.

### Example 1: [Your Question Here]

* **Prompt:**
    > [**PASTE YOUR QUESTION HERE**]

* **Generated Response:**
    > [**PASTE THE MODEL'S RESPONSE HERE**]

* **My Analysis:**
    > [Add your notes]

---

### Example 2: [Your Question Here]

* **Prompt:**
    > [**PASTE YOUR QUESTION HERE**]

* **Generated Response:**
    > [**PASTE THE MODEL'S RESPONSE HERE**]

* **My Analysis:**
    > [Add your notes here...]

---

### Example 3: [Your Question Here]

* **Prompt:**
    > [**PASTE YOUR QUESTION HERE**]

* **Generated Response:**
    > [**PASTE THE MODEL'S RESPONSE HERE**]

* **My Analysis:**
    > [Add your notes here...]

---
